{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets using example\n",
    "\n",
    "### This notebook will show an example of using our custom dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:02.655110500Z",
     "start_time": "2025-03-20T08:37:49.400845400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 11:37:59 __init__.py:190] Automatically detected platform cuda.\n",
      "WARNING 03-20 11:37:59 cuda.py:336] Detected different devices in the system: \r\n",
      "WARNING 03-20 11:37:59 cuda.py:336] NVIDIA GeForce RTX 2080 Ti\r\n",
      "WARNING 03-20 11:37:59 cuda.py:336] NVIDIA TITAN RTX\r\n",
      "WARNING 03-20 11:37:59 cuda.py:336] Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f0a88348410>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# This code enables using of \"src.data\" imports in vs code (when you're launching it directly from notebooks directory)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from src.data.classification import SST2Dataset\n",
    "from src.data.generation import SamsumDataset\n",
    "from src.data.multi_task import BBHDataset\n",
    "from src.evaluation.evaluator import TextClassificationEvaluator, GenerationEvaluator\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:13.678949400Z",
     "start_time": "2025-03-20T08:38:02.653111900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e4b3a967da344619a53f12a07f0e6c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading model weights\n",
    "\n",
    "model_name = \"AnatoliiPotapov/T-lite-instruct-0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=device,\n",
    "    torch_dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:13.920620Z",
     "start_time": "2025-03-20T08:38:13.678477600Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializing dataset\n",
    "\n",
    "sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:13.922821500Z",
     "start_time": "2025-03-20T08:38:13.920681300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821\n"
     ]
    }
   ],
   "source": [
    "# data length\n",
    "\n",
    "print(len(sst2_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:13.923318Z",
     "start_time": "2025-03-20T08:38:13.920681300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Please perform Sentiment Classification task.\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\n<INPUT>\\n\\nResponse:\\n'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get your prompt like that\n",
    "\n",
    "sst2_ds.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:14.044920800Z",
     "start_time": "2025-03-20T08:38:13.921251700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99]) torch.Size([99]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# getting first data sample\n",
    "\n",
    "input_ids, attention_mask, label = next(iter(sst2_ds))\n",
    "print(input_ids.shape, attention_mask.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:14.839454400Z",
     "start_time": "2025-03-20T08:38:13.973780400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# terminators were taken from hf model page (t-lite 0.1)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# generating answer for our sample \n",
    "# unsqueeze(0) - to make to necessary shape (when using DataLoader it'll be done automatically)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids.unsqueeze(0),\n",
    "    attention_mask = attention_mask.unsqueeze(0),\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=terminators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:14.841457500Z",
     "start_time": "2025-03-20T08:38:14.839454400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Please perform Sentiment Classification task.\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\nno movement, no yuks, not much of anything.\\n\\nResponse:\\n<ans>negative</ans>'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding the answer\n",
    "\n",
    "ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'Response:\\n<ans>negative</ans>'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ans.find(\"Response:\\n\")\n",
    "ans[pos:]   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:38:14.931824600Z",
     "start_time": "2025-03-20T08:38:14.839454400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Huggingface evaluation*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:39:02.968307600Z",
     "start_time": "2025-03-20T08:38:14.887509600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]/nfs/home/edyagin/.virtualenvs/prompt_optimization/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 1/29 [00:01<00:52,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 2/29 [00:03<00:46,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 3/29 [00:05<00:43,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 4/29 [00:06<00:41,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 5/29 [00:08<00:39,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 6/29 [00:09<00:37,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 7/29 [00:11<00:35,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 8/29 [00:13<00:33,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 9/29 [00:14<00:32,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 10/29 [00:16<00:30,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 11/29 [00:17<00:28,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 12/29 [00:19<00:27,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 13/29 [00:21<00:25,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 14/29 [00:22<00:24,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 15/29 [00:24<00:22,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 16/29 [00:25<00:20,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 17/29 [00:27<00:19,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 18/29 [00:29<00:17,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 19/29 [00:30<00:16,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 20/29 [00:32<00:14,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 21/29 [00:34<00:12,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 22/29 [00:35<00:11,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 23/29 [00:37<00:09,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 24/29 [00:38<00:08,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 25/29 [00:40<00:06,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 26/29 [00:42<00:04,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 27/29 [00:43<00:03,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 28/29 [00:45<00:01,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 29/29 [00:46<00:00,  1.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'f1': 0.6061193982765608, 'accuracy': 0.9093904448105437}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generate_params = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"eos_token_id\": terminators\n",
    "}\n",
    "\n",
    "\n",
    "evaluator = TextClassificationEvaluator()\n",
    "evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=sst2_ds,\n",
    "    batch_size=64,\n",
    "    model_generate_args = model_generate_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]/nfs/home/edyagin/.virtualenvs/prompt_optimization/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 1/29 [00:01<00:53,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 2/29 [00:03<00:51,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 3/29 [00:05<00:49,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 4/29 [00:07<00:46,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 5/29 [00:09<00:43,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 6/29 [00:10<00:41,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 7/29 [00:12<00:39,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 8/29 [00:14<00:38,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 9/29 [00:16<00:36,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 10/29 [00:18<00:34,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 11/29 [00:20<00:32,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 12/29 [00:22<00:31,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 13/29 [00:23<00:29,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 14/29 [00:25<00:27,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 15/29 [00:27<00:25,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 16/29 [00:29<00:23,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 17/29 [00:31<00:21,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 18/29 [00:32<00:19,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 19/29 [00:34<00:17,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 20/29 [00:36<00:16,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 21/29 [00:38<00:14,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 22/29 [00:40<00:12,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 23/29 [00:42<00:11,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 24/29 [00:43<00:09,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 25/29 [00:45<00:07,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 26/29 [00:47<00:05,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 27/29 [00:49<00:03,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 28/29 [00:51<00:01,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 29/29 [00:52<00:00,  1.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'f1': 0.9609998488752062, 'accuracy': 0.9610104338275672}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use your prompt instead of basic one\n",
    "\n",
    "my_prompt = \"You will be given movie reviews. Determine if the given review has negative or positive sentiment.\"\n",
    "\n",
    "prompted_sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=my_prompt,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "prompted_metrics_hf = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=prompted_sst2_ds,\n",
    "    batch_size=64,\n",
    "    model_generate_args = model_generate_params\n",
    ")\n",
    "prompted_metrics_hf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:39:55.393214100Z",
     "start_time": "2025-03-20T08:39:02.964232700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*vllm evaluation*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "174"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:39:55.916070700Z",
     "start_time": "2025-03-20T08:39:55.443085200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-20 11:39:56 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-20 11:40:08 config.py:542] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-20 11:40:08 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='AnatoliiPotapov/T-lite-instruct-0.1', speculative_config=None, tokenizer='AnatoliiPotapov/T-lite-instruct-0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=AnatoliiPotapov/T-lite-instruct-0.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-20 11:40:09 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-20 11:40:09 cuda.py:227] Using XFormers backend.\n",
      "INFO 03-20 11:40:10 model_runner.py:1110] Starting to load model AnatoliiPotapov/T-lite-instruct-0.1...\n",
      "INFO 03-20 11:40:10 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98dfea4de85b49cc9d818b9d281ee626"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 11:40:19 model_runner.py:1115] Loading model weights took 14.9605 GB\n",
      "INFO 03-20 11:40:21 worker.py:267] Memory profiling takes 1.64 seconds\r\n",
      "INFO 03-20 11:40:21 worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.27GiB\r\n",
      "INFO 03-20 11:40:21 worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.22GiB; the rest of the memory reserved for KV Cache is 5.07GiB.\n",
      "INFO 03-20 11:40:22 executor_base.py:110] # CUDA blocks: 2593, # CPU blocks: 2048\n",
      "INFO 03-20 11:40:22 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 5.06x\n",
      "INFO 03-20 11:40:25 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 11:40:43 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.24 GiB\n",
      "INFO 03-20 11:40:43 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "model = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:40:44.554604400Z",
     "start_time": "2025-03-20T08:39:55.911955200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:41:17.114433Z",
     "start_time": "2025-03-20T08:40:44.551259400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:32<00:00,  1.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'f1': 0.9642991254620392, 'accuracy': 0.9643053267435475}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_generate_args =  {\n",
    "    \"max_tokens\": 50,\n",
    "    \"stop_token_ids\": terminators\n",
    "}\n",
    "\n",
    "prompted_metrics_vllm = evaluator.evaluate_vllm(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=prompted_sst2_ds,\n",
    "    batch_size=64,\n",
    "    model_generate_args = vllm_generate_args\n",
    ")\n",
    "prompted_metrics_vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:41:17.412097Z",
     "start_time": "2025-03-20T08:41:17.184487700Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can also use generation dataset\n",
    "\n",
    "sds = SamsumDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:41:17.415097500Z",
     "start_time": "2025-03-20T08:41:17.412097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n"
     ]
    }
   ],
   "source": [
    "print(len(sds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:41:17.416094Z",
     "start_time": "2025-03-20T08:41:17.412097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([712]) torch.Size([712]) torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, label = next(iter(sds))\n",
    "print(input_ids.shape, attention_mask.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:43:33.094909Z",
     "start_time": "2025-03-20T08:41:17.412097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /nfs/home/edyagin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /nfs/home/edyagin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /nfs/home/edyagin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "100%|██████████| 26/26 [02:04<00:00,  4.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'bleu': 0.0884306294872872,\n 'rouge': 0.3109028862801102,\n 'meteor': 0.4495479453172058}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generate_params = {\n",
    "    \"max_tokens\": 256,\n",
    "    \"stop_token_ids\": terminators\n",
    "}\n",
    "\n",
    "evaluator = GenerationEvaluator()\n",
    "metrics = evaluator.evaluate_vllm(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=sds,\n",
    "    batch_size=32,\n",
    "    model_generate_args = model_generate_params\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:43:39.851417900Z",
     "start_time": "2025-03-20T08:43:36.919368800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Multi-task dataset example\n",
    "\n",
    "ds = BBHDataset(\n",
    "    tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "ds = ds.task('boolean_expressions')\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model_generate_params = {\n",
    "    \"max_tokens\": 50,\n",
    "    \"stop_token_ids\": terminators\n",
    "}\n",
    "\n",
    "evaluator = TextClassificationEvaluator()\n",
    "metrics = evaluator.evaluate_vllm(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=ds,\n",
    "    batch_size=128,\n",
    "    model_generate_args = model_generate_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:43:41.172232400Z",
     "start_time": "2025-03-20T08:43:41.124175100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'f1': 0.5625, 'accuracy': 0.6825396825396826}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "*C использованием vllm сервера*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import ray\n",
    "import contextlib\n",
    "\n",
    "from vllm.distributed.parallel_state import (\n",
    "    destroy_model_parallel,\n",
    "    destroy_distributed_environment,\n",
    ")\n",
    "\n",
    "# Delete the llm object and free the memory\n",
    "destroy_model_parallel()\n",
    "destroy_distributed_environment()\n",
    "del model.llm_engine.model_executor\n",
    "del model\n",
    "with contextlib.suppress(AssertionError):\n",
    "    torch.distributed.destroy_process_group()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "ray.shutdown()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:43:44.563889600Z",
     "start_time": "2025-03-20T08:43:43.688264100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 29/29 [01:11<00:00,  2.47s/it]=(true | false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'f1': 0.6167765560167768, 'accuracy': 0.8934651290499726}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сервер запущен такой командой\n",
    "# vllm serve \"AnatoliiPotapov/T-lite-instruct-0.1\" --dtype half\n",
    "\n",
    "metrics_vllm_server = evaluator.evaluate_vllm_server(\n",
    "    model_name=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=prompted_sst2_ds,\n",
    "    batch_size=64,\n",
    "    model_generate_args = vllm_generate_args\n",
    ")\n",
    "metrics_vllm_server"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:47:05.083797500Z",
     "start_time": "2025-03-20T08:45:53.082633200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
