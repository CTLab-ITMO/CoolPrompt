{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e61688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-15 15:30:30,345] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-11-15 15:30:30,347] [INFO] [assistant.__init__] - Validating the system model\n",
      "[2025-11-15 15:30:30,348] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-15 15:30:30,347] [INFO] [assistant.__init__] - Validating the system model\n",
      "[2025-11-15 15:30:30,348] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-15 15:30:33,142] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-15 15:30:33,143] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-15 15:30:33,142] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-15 15:30:33,143] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-15 15:30:43,336] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-15 15:30:43,337] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-15 15:30:43,338] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-15 15:30:43,339] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-15 15:30:43,340] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-15 15:30:43,341] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-15 15:30:43,342] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-15 15:30:43,336] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-15 15:30:43,337] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-15 15:30:43,338] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-15 15:30:43,339] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-15 15:30:43,340] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-15 15:30:43,341] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-15 15:30:43,342] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-15 15:30:49,024] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-15 15:30:49,026] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Summarize the following text:[PROMPT_END]\n",
      "[2025-11-15 15:30:49,027] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-15 15:30:49,024] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-15 15:30:49,026] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Summarize the following text:[PROMPT_END]\n",
      "[2025-11-15 15:30:49,027] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-15 15:30:57,966] [DEBUG] [assistant.run] - Final prompt:\n",
      "Summarize the following text:\n",
      "[2025-11-15 15:30:57,968] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-15 15:30:57,969] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:30:57,970] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-15 15:30:57,966] [DEBUG] [assistant.run] - Final prompt:\n",
      "Summarize the following text:\n",
      "[2025-11-15 15:30:57,968] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-15 15:30:57,969] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:30:57,970] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-15 15:31:33,768] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:31:33,768] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:31:33,769] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the following text:\n",
      "[2025-11-15 15:31:33,769] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the following text:\n",
      "[2025-11-15 15:32:06,393] [INFO] [assistant.run] - Initial llm_as_judge score: 0.58, final llm_as_judge score: 0.33999999999999997\n",
      "[2025-11-15 15:32:06,394] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-15 15:32:06,393] [INFO] [assistant.run] - Initial llm_as_judge score: 0.58, final llm_as_judge score: 0.33999999999999997\n",
      "[2025-11-15 15:32:06,394] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-15 15:32:20,983] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-15 15:32:20,984] [INFO] [assistant.run] - Your initial prompt was open-ended. We improved it by asking for a specific text to summarize, which reduces ambiguity and ensures relevance. The added context makes it clear what you want from the summary, focusing on providing an accurate and concise overview of the provided text.\n",
      "[2025-11-15 15:32:20,985] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-15 15:32:20,986] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-15 15:32:20,983] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-15 15:32:20,984] [INFO] [assistant.run] - Your initial prompt was open-ended. We improved it by asking for a specific text to summarize, which reduces ambiguity and ensures relevance. The added context makes it clear what you want from the summary, focusing on providing an accurate and concise overview of the provided text.\n",
      "[2025-11-15 15:32:20,985] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-15 15:32:20,986] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-15 15:32:23,252] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-15 15:32:23,253] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-15 15:32:23,253] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-15 15:32:23,254] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-15 15:32:23,255] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-15 15:32:23,258] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-15 15:32:23,259] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-15 15:32:23,252] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-15 15:32:23,253] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-15 15:32:23,253] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-15 15:32:23,254] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-15 15:32:23,255] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-15 15:32:23,258] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-15 15:32:23,259] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-15 15:32:27,361] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-15 15:32:27,362] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Generate a summary of the following text:[PROMPT_END]\n",
      "[2025-11-15 15:32:27,363] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-15 15:32:27,361] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-15 15:32:27,362] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Generate a summary of the following text:[PROMPT_END]\n",
      "[2025-11-15 15:32:27,363] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-15 15:32:35,248] [DEBUG] [assistant.run] - Final prompt:\n",
      "Generate a summary of the following text:\n",
      "[2025-11-15 15:32:35,249] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-15 15:32:35,250] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:32:35,250] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-15 15:32:35,248] [DEBUG] [assistant.run] - Final prompt:\n",
      "Generate a summary of the following text:\n",
      "[2025-11-15 15:32:35,249] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-15 15:32:35,250] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:32:35,250] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-15 15:33:21,108] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:33:21,109] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Generate a summary of the following text:\n",
      "[2025-11-15 15:33:21,108] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:33:21,109] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Generate a summary of the following text:\n",
      "[2025-11-15 15:34:11,011] [INFO] [assistant.run] - Initial llm_as_judge score: 0.395, final llm_as_judge score: 0.49\n",
      "[2025-11-15 15:34:11,011] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-15 15:34:11,011] [INFO] [assistant.run] - Initial llm_as_judge score: 0.395, final llm_as_judge score: 0.49\n",
      "[2025-11-15 15:34:11,011] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-15 15:34:24,869] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-15 15:34:24,870] [INFO] [assistant.run] - You should follow these three simple rules:\n",
      "1. Structured 2. Detalized 3. Instructive\n",
      "[2025-11-15 15:34:24,871] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-15 15:34:24,872] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-15 15:34:24,869] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-15 15:34:24,870] [INFO] [assistant.run] - You should follow these three simple rules:\n",
      "1. Structured 2. Detalized 3. Instructive\n",
      "[2025-11-15 15:34:24,871] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-15 15:34:24,872] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-15 15:34:28,342] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-15 15:34:28,343] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-15 15:34:28,344] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-15 15:34:28,344] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-15 15:34:28,345] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-15 15:34:28,346] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-15 15:34:28,347] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "\n",
      "[2025-11-15 15:34:28,342] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-15 15:34:28,343] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-15 15:34:28,344] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-15 15:34:28,344] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-15 15:34:28,345] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-15 15:34:28,346] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-15 15:34:28,347] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "\n",
      "[2025-11-15 15:34:34,160] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-15 15:34:34,161] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "**PROMPT_START**\n",
      "Instructively guide the user on how to effectively generate a detailed response to a complex question or problem, emphasizing the importance of thorough understanding and comprehensive explanation. Emphasize the need for multi-step reasoning, information retrieval from various sources if necessary, and provide strategies for breaking down complex concepts into simpler terms suitable for an educational audience.\n",
      "\n",
      "**PROMPT_END**\n",
      "\n",
      "This hypothetical instructive prompt is designed to effectively guide users in creating detailed responses to complex questions or problems by emphasizing thorough understanding, comprehensive explanation, multiple step reasoning, and the importance of information retrieval from various sources.\n",
      "[2025-11-15 15:34:34,162] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-15 15:34:34,160] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-15 15:34:34,161] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "**PROMPT_START**\n",
      "Instructively guide the user on how to effectively generate a detailed response to a complex question or problem, emphasizing the importance of thorough understanding and comprehensive explanation. Emphasize the need for multi-step reasoning, information retrieval from various sources if necessary, and provide strategies for breaking down complex concepts into simpler terms suitable for an educational audience.\n",
      "\n",
      "**PROMPT_END**\n",
      "\n",
      "This hypothetical instructive prompt is designed to effectively guide users in creating detailed responses to complex questions or problems by emphasizing thorough understanding, comprehensive explanation, multiple step reasoning, and the importance of information retrieval from various sources.\n",
      "[2025-11-15 15:34:34,162] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-15 15:34:42,707] [DEBUG] [assistant.run] - Final prompt:\n",
      "**PROMPT_START**\n",
      "Instructively guide the user on how to effectively generate a detailed response to a complex question or problem, emphasizing the importance of thorough understanding and comprehensive explanation. Emphasize the need for multi-step reasoning, information retrieval from various sources if necessary, and provide strategies for breaking down complex concepts into simpler terms suitable for an educational audience.\n",
      "\n",
      "**PROMPT_END**\n",
      "\n",
      "This hypothetical instructive prompt is designed to effectively guide users in creating detailed responses to complex questions or problems by emphasizing thorough understanding, comprehensive explanation, multiple step reasoning, and the importance of information retrieval from various sources.\n",
      "[2025-11-15 15:34:42,707] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-15 15:34:42,708] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:34:42,709] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "\n",
      "[2025-11-15 15:34:42,707] [DEBUG] [assistant.run] - Final prompt:\n",
      "**PROMPT_START**\n",
      "Instructively guide the user on how to effectively generate a detailed response to a complex question or problem, emphasizing the importance of thorough understanding and comprehensive explanation. Emphasize the need for multi-step reasoning, information retrieval from various sources if necessary, and provide strategies for breaking down complex concepts into simpler terms suitable for an educational audience.\n",
      "\n",
      "**PROMPT_END**\n",
      "\n",
      "This hypothetical instructive prompt is designed to effectively guide users in creating detailed responses to complex questions or problems by emphasizing thorough understanding, comprehensive explanation, multiple step reasoning, and the importance of information retrieval from various sources.\n",
      "[2025-11-15 15:34:42,707] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-15 15:34:42,708] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:34:42,709] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "\n",
      "[2025-11-15 15:35:10,701] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:35:10,702] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "**PROMPT_START**\n",
      "Instructively guide the user on how to effectively generate a detailed response to a complex question or problem, emphasizing the importance of thorough understanding and comprehensive explanation. Emphasize the need for multi-step reasoning, information retrieval from various sources if necessary, and provide strategies for breaking down complex concepts into simpler terms suitable for an educational audience.\n",
      "\n",
      "**PROMPT_END**\n",
      "\n",
      "This hypothetical instructive prompt is designed to effectively guide users in creating detailed responses to complex questions or problems by emphasizing thorough understanding, comprehensive explanation, multiple step reasoning, and the importance of information retrieval from various sources.\n",
      "[2025-11-15 15:35:10,701] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-15 15:35:10,702] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "**PROMPT_START**\n",
      "Instructively guide the user on how to effectively generate a detailed response to a complex question or problem, emphasizing the importance of thorough understanding and comprehensive explanation. Emphasize the need for multi-step reasoning, information retrieval from various sources if necessary, and provide strategies for breaking down complex concepts into simpler terms suitable for an educational audience.\n",
      "\n",
      "**PROMPT_END**\n",
      "\n",
      "This hypothetical instructive prompt is designed to effectively guide users in creating detailed responses to complex questions or problems by emphasizing thorough understanding, comprehensive explanation, multiple step reasoning, and the importance of information retrieval from various sources.\n",
      "[2025-11-15 15:37:24,471] [INFO] [assistant.run] - Initial llm_as_judge score: 0.1, final llm_as_judge score: 0.24\n",
      "[2025-11-15 15:37:24,472] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-15 15:37:24,471] [INFO] [assistant.run] - Initial llm_as_judge score: 0.1, final llm_as_judge score: 0.24\n",
      "[2025-11-15 15:37:24,472] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-15 15:37:41,239] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-15 15:37:41,242] [INFO] [assistant.run] - Your initial prompt was open-ended. We improved it by providing a clear instruction on how to create detailed responses effectively. This new version emphasizes the need for thorough understanding, comprehensive explanation, multiple step reasoning, and strategies for breaking down complex concepts into simpler terms suitable for an educational audience. It also includes specific instructions regarding the use of various sources if necessary.\n",
      "[2025-11-15 15:37:41,239] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-15 15:37:41,242] [INFO] [assistant.run] - Your initial prompt was open-ended. We improved it by providing a clear instruction on how to create detailed responses effectively. This new version emphasizes the need for thorough understanding, comprehensive explanation, multiple step reasoning, and strategies for breaking down complex concepts into simpler terms suitable for an educational audience. It also includes specific instructions regarding the use of various sources if necessary.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from coolprompt import PromptTuner\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "target_model = ChatOllama(model=\"qwen2.5:1.5b-instruct\")\n",
    "system_model = ChatOllama(model=\"qwen2.5:3b-instruct\")\n",
    "\n",
    "tuner = PromptTuner(target_model=target_model, system_model=system_model)\n",
    "\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:40]\n",
    "targets = samsum[\"train\"][\"summary\"][:40]\n",
    "\n",
    "# There are 4 available templates for llm as judge metric:\n",
    "\n",
    "# Accuracy: The answer is factually correct and contains no mistakes. \n",
    "# It uses the right terms and numbers and does not introduce wrong information.\n",
    "\n",
    "# Coherence: The answer is well structured and easy to follow. \n",
    "# Ideas are in a logical order and there are no contradictions.\n",
    "\n",
    "# Fluency: The answer is written in natural, correct language. \n",
    "# Grammar, vocabulary and phrasing are clear and do not make the text hard to read.\n",
    "\n",
    "# Relevance: The answer directly responds to the request and stays on topic. \n",
    "# It does not add unnecessary or unrelated details.\n",
    "\n",
    "result = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"llm_as_judge\",\n",
    "    llm_as_judge_criteria=\"relevance\",\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# You can use several criteria, and the final score will be the mean of them.\n",
    "\n",
    "result2 = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"llm_as_judge\",\n",
    "    llm_as_judge_criteria=[\"relevance\", \"coherence\"],\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# You can use custom criteria in llm as judge\n",
    "\n",
    "custom = {\n",
    "    \"creativity\": \"\"\"You will be given a response to a question.\n",
    "    Rate the creativity on a scale from 1 to {metric_ceil}.\n",
    "    Return ONLY a single number.\n",
    "\n",
    "    Source: {request}\n",
    "    Response: {response}\n",
    "\n",
    "    Creativity score (number only):\"\"\"\n",
    "}\n",
    "\n",
    "result3 = tuner.run(\n",
    "    start_prompt=\"\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"llm_as_judge\",\n",
    "    llm_as_judge_criteria=[\"creativity\"],\n",
    "    llm_as_judge_custom_templates=custom,\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a072c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
