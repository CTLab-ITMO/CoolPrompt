{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Тестирование на NER датасете с использованием rubert**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43b976697114afd5"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"unimelb-nlp/wikiann\", 'ru')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:20.161165100Z",
     "start_time": "2025-02-27T13:27:14.427332500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Илизаров', ',', 'Гавриил', 'Абрамович']\n",
      "[1, 2, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_datasets['train'][0][\"tokens\"])\n",
    "print(raw_datasets['train'][0][\"ner_tags\"])\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature # уже список тегов"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:20.230270800Z",
     "start_time": "2025-02-27T13:27:20.229269900Z"
    }
   },
   "id": "2078128b9933eaf9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Все лейблы, нужны будут для evaluate\n",
    "all_labels = ner_feature.feature.names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:20.230270800Z",
     "start_time": "2025-02-27T13:27:20.229269900Z"
    }
   },
   "id": "87cc78d2d88db125"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Подготовка датасета"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01b08cbd5b24937"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Нельзя просто rubert-base-cased, нужно, чтобы\n",
    "# hf.co/model_name резолвилось.\n",
    "model_checkpoint = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:23.602957800Z",
     "start_time": "2025-02-27T13:27:20.230270800Z"
    }
   },
   "id": "b55373b74abefe0d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN_IDX = -100 # игнорируется функцией потерь\n",
    "\n",
    "# Суть в том, что после токенизации берта слова разбились на части\n",
    "# типа [\"Гав\", \"##рил\", \"##ов\"], а у них должна быть одинаковая метка\n",
    "# поэтому нужен алайнмент\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    cur_wid = None\n",
    "    for wid in word_ids:\n",
    "        # Начало нового\n",
    "        if wid != cur_wid:\n",
    "            cur_wid = wid\n",
    "            label = -100 if (wid is None) else labels[wid]\n",
    "            \n",
    "            new_labels.append(label)\n",
    "        elif wid is None:\n",
    "            new_labels.append(-100)\n",
    "        \n",
    "        # продолжение текущего\n",
    "        else:\n",
    "            label = labels[cur_wid]\n",
    "            if label % 2 == 1:\n",
    "                label += 1 # случай смены с I-, на B-\n",
    "                            \n",
    "            new_labels.append(label)\n",
    "            \n",
    "    return new_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:23.614953200Z",
     "start_time": "2025-02-27T13:27:23.613952Z"
    }
   },
   "id": "79069636e0398f40"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': [[101, 35377, 31332, 1388, 128, 56031, 41439, 102], [101, 118, 118, 118, 21919, 30310, 5679, 118, 118, 118, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 1, 2, 2, 2, 2, 2, -100], [-100, 0, 0, 0, 5, 6, 6, 0, 0, 0, -100]]}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Батчевая версия для токенизатора\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Пока нельзя return_tensors, так как пэддинг будет неправильный\n",
    "    # Это настроим в коллаторе\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    new_labels = []\n",
    "    \n",
    "    for i, cur_labels in enumerate(examples['ner_tags']):\n",
    "        cur_word_ids = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels = align_labels_with_tokens(cur_labels, cur_word_ids)\n",
    "        new_labels.append(aligned_labels)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenize_and_align_labels(raw_datasets['train'][0, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:23.644574400Z",
     "start_time": "2025-02-27T13:27:23.613952Z"
    }
   },
   "id": "345ea647c5e0564a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Инициализируем модель, указываем,\n",
    "# сколько выходов будет у новой головы\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(all_labels)) \n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.config.num_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:26.013169600Z",
     "start_time": "2025-02-27T13:27:23.642385600Z"
    }
   },
   "id": "80f67054eb2ac76f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    # выкинем ner_tags, spans, lan и тд\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:26.113071600Z",
     "start_time": "2025-02-27T13:27:26.039773300Z"
    }
   },
   "id": "838c44ca761686e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Инференс модели + тестирование eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3679eb6ef879b8bf"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Для начала нужно завести DataCollator\n",
    "# есть как раз готовый, который -100 будет заполнять лейблы\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], batch_size=256, shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:26.114002600Z",
     "start_time": "2025-02-27T13:27:26.113071600Z"
    }
   },
   "id": "b04defd29bbd0b1d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator import TokenClassificationEvaluator\n",
    "\n",
    "model.eval()\n",
    "evaluator = TokenClassificationEvaluator(\n",
    "    all_labels, label_ids_to_ignore=[SPECIAL_TOKEN_IDX]\n",
    ")\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    # Позволяет накапливать по батчам, а потом считать сразу за все\n",
    "    evaluator.add_batch(model_outputs=outputs, references=batch['labels'])\n",
    "    \n",
    "f1_score = evaluator.compute()['overall_f1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:36.855614100Z",
     "start_time": "2025-02-27T13:27:26.114002600Z"
    }
   },
   "id": "e1d0b1d9d4e6a254"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0.01415979889903586"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:36.889328600Z",
     "start_time": "2025-02-27T13:27:36.884231200Z"
    }
   },
   "id": "cbb6cefbbdf5864b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Скор получился низким, так как у модели голова инициализирована рандомно**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20c08d8314f569e0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T13:27:36.889328600Z",
     "start_time": "2025-02-27T13:27:36.884231200Z"
    }
   },
   "id": "fcc44e0b6ae61f59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
