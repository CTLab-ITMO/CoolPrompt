{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets using example\n",
    "\n",
    "### This notebook will show an example of using our custmom dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/vzhuravlev/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f635819e950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# This code enables using of \"src.data\" imports in vs code (when you're launching it directly from notebooks directory)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from src.data.classification import SST2Dataset, SamsumDataset\n",
    "from src.data.base.datasets import BaseClassificationDataset\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/vzhuravlev/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.48s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at t-bank-ai/T-lite-instruct-0.1 and are newly initialized: ['model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/nfs/home/vzhuravlev/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading model weights\n",
    "qconf = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_name = \"t-bank-ai/T-lite-instruct-0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=qconf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing dataset\n",
    "\n",
    "sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=\"../data/sst-2/test-00000-of-00001.parquet\",\n",
    "    config_path=\"../data/\",\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821\n"
     ]
    }
   ],
   "source": [
    "# data length\n",
    "\n",
    "print(len(sst2_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please perform Sentiment Classification task\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\n<INPUT>\\n\\nResponse:\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get your prompt like that\n",
    "\n",
    "sst2_ds.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99]) torch.Size([99]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# getting first data sample\n",
    "\n",
    "input_ids, attention_mask, label = next(iter(sst2_ds))\n",
    "print(input_ids.shape, attention_mask.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/vzhuravlev/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/nfs/home/vzhuravlev/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "# terminators were taken from hf model page (t-lite 0.1)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# generating answer for our sample \n",
    "# unsqueeze(0) - to make to necessary shape (when using DataLoader it'll be done automatically)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids.unsqueeze(0),\n",
    "    attention_mask = attention_mask.unsqueeze(0),\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=terminators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please perform Sentiment Classification task\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\nno movement, no yuks, not much of anything.\\n\\nResponse:\\n<ans>negative</ans>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding the answer\n",
    "\n",
    "ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response:\\n<ans>negative</ans>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ans.find(\"Response:\\n\")\n",
    "ans[pos:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_second_position(input: str, substr: str) -> int:\n",
    "    first_position = input.find(substr)\n",
    "    return input.find(substr, first_position + 1)\n",
    "\n",
    "def parse_answer(model_output: str) -> str:\n",
    "    left_bracket_pos = find_second_position(model_output, \"<ans>\")\n",
    "    right_bracket_pos = find_second_position(model_output, \"</ans>\")\n",
    "    \n",
    "    if left_bracket_pos == -1 or right_bracket_pos == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    return model_output[left_bracket_pos + len(\"<ans>\"):right_bracket_pos]\n",
    "\n",
    "def evaluate_accuracy(ds: BaseClassificationDataset, batch_size: int = 64) -> float:\n",
    "    # creating data loadet\n",
    "    dl = DataLoader(sst2_ds, batch_size=batch_size)\n",
    "\n",
    "    # keeping history of answers and labels\n",
    "    results = []\n",
    "    all_labels = []\n",
    "    \n",
    "    label_mapping = ds.get_labels_mapping()\n",
    "\n",
    "    for input_ids, attention_mask, labels in tqdm(dl):\n",
    "        # generating batch output\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            eos_token_id=terminators,\n",
    "        )\n",
    "        \n",
    "        # decoding answers\n",
    "        answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "        # parsing answers        \n",
    "        answers = [parse_answer(answer) for answer in answers]\n",
    "        # convert string answers to labels\n",
    "        labeled_answers = [label_mapping.get(answer, -1) for answer in answers]\n",
    "\n",
    "        res = torch.Tensor(labeled_answers).type(torch.long)\n",
    "        results.append(res)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "    # concatenating all steps together\n",
    "    results = torch.cat(results).cpu()\n",
    "    all_labels = torch.cat(all_labels).cpu()\n",
    "    \n",
    "    accuracy = torch.mean((results == all_labels).type(torch.float))\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [17:52<00:00, 71.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6990664601325989"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(sst2_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use your prompt instead of basic one\n",
    "\n",
    "my_prompt = \"You will be given reviews. Determine if the given review has negative or positive sentiment.\"\n",
    "\n",
    "prompted_sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=\"../data/sst-2/test-00000-of-00001.parquet\",\n",
    "    config_path=\"../data/\",\n",
    "    prompt=my_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/nfs/home/vzhuravlev/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [17:51<00:00, 71.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6990664601325989"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(prompted_sst2_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use generation dataset\n",
    "\n",
    "sds = SamsumDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=\"../data/samsum/test-00000-of-00001.parquet\",\n",
    "    config_path='../data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n"
     ]
    }
   ],
   "source": [
    "print(len(sds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([715]) torch.Size([715]) torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, label = next(iter(sds))\n",
    "print(input_ids.shape, attention_mask.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=input_ids.unsqueeze(0),\n",
    "    attention_mask = attention_mask.unsqueeze(0),\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"INSTRUCTION:\\nSummarize the following text\\n\\nINPUT:\\nHannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n\\nRESPONSE:\\nHannah asked Amanda for Betty's number. Amanda couldn't find it, but suggested asking Larry, who had called Betty recently. Hannah was hesitant to ask Larry, but Amanda reassured her that Larry was a nice person. Hannah finally agreed to ask Larry for Betty's number.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
