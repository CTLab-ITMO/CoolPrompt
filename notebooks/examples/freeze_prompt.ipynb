{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ee7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asd480/anaconda3/envs/coolprompt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import load_dataset\n",
    "from coolprompt.assistant import PromptTuner\n",
    "from coolprompt.utils.prompt_freezer import split_prompt, merge_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24df0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:5]\n",
    "targets = samsum[\"train\"][\"summary\"][:5]\n",
    "\n",
    "geval_steps = [\n",
    "    \"1. Compare the assistant's summary with the reference and list the key facts each one mentions.\",\n",
    "    \"2. Check whether the assistant introduces any unsupported information or misses essential events.\",\n",
    "    \"3. Decide if the assistant's summary is concise and coherent while covering all required details.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0acf86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:34:05,994] [DEBUG] [prompt_freezer.split_prompt] - Found 1 frozen part(s). Frozen content: in exactly 4 sentences....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizable: Summarize the text\n",
      "Frozen: in exactly 4 sentences.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"Summarize the text <freeze>in exactly 4 sentences.</freeze>\"\n",
    "optimizable, frozen = split_prompt(start_prompt)\n",
    "print(f\"Optimizable: {optimizable}\")\n",
    "print(f\"Frozen: {frozen}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7c19c",
   "metadata": {},
   "source": [
    "## HyPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d33ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:34:06,019] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-12-20 15:34:06,020] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-12-20 15:34:06,021] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-12-20 15:34:06,022] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:34:09,050] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-12-20 15:34:09,052] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-12-20 15:34:09,053] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-12-20 15:34:09,054] [INFO] [assistant.run] - Dataset: 5 samples\n",
      "[2025-12-20 15:34:09,055] [INFO] [assistant.run] - Target: 5 samples\n",
      "[2025-12-20 15:34:09,056] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-12-20 15:34:09,056] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text <freeze>in exactly 4 sentences.</freeze>\n",
      "[2025-12-20 15:34:09,056] [INFO] [hype.hype_optimizer] - Found 1 frozen part(s) in prompt\n",
      "[2025-12-20 15:34:12,615] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-12-20 15:34:12,616] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Please provide a concise summary of the given text, ensuring that the response is clear and focused. The summary must capture the main ideas without unnecessary elaboration and should be structured effectively. Importantly, the summary should be composed <freeze>in exactly 4 sentences.</freeze>[PROMPT_END]\n",
      "[2025-12-20 15:34:12,618] [DEBUG] [hype.hype_optimizer] - Final prompt with frozen parts integrated:\n",
      "Please provide a concise summary of the given text, ensuring that the response is clear and focused. The summary must capture the main ideas without unnecessary elaboration and should be structured effectively. Importantly, the summary should be composed in exactly 4 sentences.\n",
      "[2025-12-20 15:34:12,619] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-12-20 15:34:15,235] [DEBUG] [assistant.run] - Final prompt:\n",
      "Please provide a concise summary of the given text, ensuring that the response is clear and focused. The summary must capture the main ideas without unnecessary elaboration and should be structured effectively. Importantly, the summary should be composed in exactly 4 sentences.\n",
      "[2025-12-20 15:34:15,236] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-12-20 15:34:15,237] [DEBUG] [prompt_freezer.split_prompt] - Found 1 frozen part(s). Frozen content: in exactly 4 sentences....\n",
      "[2025-12-20 15:34:15,237] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:34:15,238] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text in exactly 4 sentences.\n",
      "[2025-12-20 15:34:23,255] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:34:23,256] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that the response is clear and focused. The summary must capture the main ideas without unnecessary elaboration and should be structured effectively. Importantly, the summary should be composed in exactly 4 sentences.\n",
      "[2025-12-20 15:34:31,313] [INFO] [assistant.run] - Initial geval score: 0.6, final geval score: 0.6499999999999999\n",
      "[2025-12-20 15:34:31,314] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-12-20 15:34:36,633] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-12-20 15:34:36,634] [INFO] [assistant.run] - Your initial prompt was straightforward but lacked clarity and specificity. The improved version begins with a polite request ('Please provide'), which sets a more respectful tone. It emphasizes the need for a 'concise summary' and specifies that the response should be 'clear and focused', guiding the AI to prioritize clarity. Additionally, the phrase 'capture the main ideas without unnecessary elaboration' instructs the AI to avoid extraneous details, which enhances the quality of the summary. The final prompt retains your original intent of summarizing in 4 sentences but adds context and structure, making it more effective. A key lesson here is to include clear instructions on the desired qualities of the response to ensure it meets your expectations.\n"
     ]
    }
   ],
   "source": [
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_evaluation_steps=geval_steps,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba7d73",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3571a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Summarize the text <freeze>in exactly 4 sentences.</freeze>\n",
      "Final: Please provide a concise summary of the given text, ensuring that the response is clear and focused. The summary must capture the main ideas without unnecessary elaboration and should be structured effectively. Importantly, the summary should be composed in exactly 4 sentences.\n",
      "Initial score: 0.6000\n",
      "Final score: 0.6500\n",
      "Frozen in final_prompt True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial: {start_prompt}\")\n",
    "print(f\"Final: {final_prompt}\")\n",
    "print(f\"Initial score: {tuner.init_metric:.4f}\")\n",
    "print(f\"Final score: {tuner.final_metric:.4f}\")\n",
    "print(f\"Frozen in final_prompt {frozen in final_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d910ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:34:36,672] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-12-20 15:34:36,672] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-12-20 15:34:36,673] [INFO] [assistant.run] - Validating args for PromptTuner running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-12-20 15:34:38,107] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-12-20 15:34:41,391] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-12-20 15:34:41,395] [INFO] [assistant.run] - Method: reflective, Task: generation\n",
      "[2025-12-20 15:34:41,399] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-12-20 15:34:41,400] [INFO] [assistant.run] - Dataset: 5 samples\n",
      "[2025-12-20 15:34:41,401] [INFO] [assistant.run] - Target: 5 samples\n",
      "[2025-12-20 15:34:41,402] [INFO] [evoluter.__init__] - Found frozen parts in initial prompt. LLM will preserve them in natural position.\n",
      "[2025-12-20 15:34:41,402] [INFO] [run.reflectiveprompt] - Starting ReflectivePrompt optimization...\n",
      "[2025-12-20 15:34:41,403] [DEBUG] [run.reflectiveprompt] - Start prompt:\n",
      "Summarize the text <freeze>in exactly 4 sentences.</freeze>\n",
      "[2025-12-20 15:34:41,404] [DEBUG] [run.reflectiveprompt] - Problem description:\n",
      "The user is seeking a concise summary of a given text, specifically requesting that the summary be limited to exactly four sentences. This indicates that the user values brevity and clarity, and wants to ensure that the most important points of the text are captured without unnecessary elaboration. The task requires the ability to distill information effectively, identifying key themes and ideas while maintaining coherence in the summary. The user may be looking to save time or to facilitate understanding of the text for themselves or others.\n",
      "[2025-12-20 15:34:41,405] [INFO] [evoluter._init_pop] - Initializing population...\n",
      "[2025-12-20 15:34:44,998] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:34:44,999] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:34:45,000] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Provide a summary of the text in exactly 4 sentences.\n",
      "[2025-12-20 15:34:53,225] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:34:53,225] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Condense the text in exactly 4 sentences.\n",
      "[2025-12-20 15:34:56,652] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:34:56,652] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Give a brief overview of the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:01,226] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:01,226] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Outline the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:04,372] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:04,373] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Rephrase the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:08,635] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:08,636] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:35:11,269] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:11,269] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Capture the essence of the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:14,967] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:14,968] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Rewrite the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:18,307] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:18,308] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Interpret the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:22,389] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:22,390] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:33,114] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:35:33,115] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:33,116] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:37,951] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:37,951] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:41,301] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:41,302] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:35:43,105] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:43,106] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:47,065] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:47,066] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:50,675] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:50,676] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:35:55,011] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:55,012] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the text, capturing the essence and key themes in exactly 4 sentences.\n",
      "[2025-12-20 15:35:57,265] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:35:57,265] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:01,701] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:01,701] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:36:04,403] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:04,404] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:08,582] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.5415562993173771\n",
      "[2025-12-20 15:36:08,583] [DEBUG] [evoluter._update_elitist] - Elitist text:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:11,861] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:36:11,861] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:11,862] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, distilling the key themes and ideas, in exactly 4 sentences.\n",
      "[2025-12-20 15:36:14,368] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:14,368] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:17,849] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:17,850] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:22,383] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:22,384] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:27,749] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:27,749] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, distilling the key themes and ideas, in exactly 4 sentences.\n",
      "[2025-12-20 15:36:30,043] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:30,044] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:32,284] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:32,284] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:37,236] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:37,237] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, distilling the key themes and ideas, in exactly 4 sentences.\n",
      "[2025-12-20 15:36:39,435] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:39,435] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:41,730] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:41,731] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, distilling the key themes and ideas, in exactly 4 sentences.\n",
      "[2025-12-20 15:36:44,186] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.6031870802819591\n",
      "[2025-12-20 15:36:44,187] [DEBUG] [evoluter._update_elitist] - Elitist text:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:36:44,187] [INFO] [evoluter._update_iter] - Iteration 0 finished...\n",
      "[2025-12-20 15:36:44,188] [INFO] [evoluter._update_iter] - Best score: 0.6031870802819591\n",
      "[2025-12-20 15:36:46,927] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:36:46,927] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:46,928] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:49,728] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:49,729] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:53,214] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:53,214] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:36:56,413] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:56,413] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:36:58,259] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:36:58,260] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:00,100] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:00,100] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:04,475] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:04,476] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:09,012] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:09,013] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:13,091] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:13,092] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:16,236] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:16,236] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:21,122] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:37:21,123] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:21,123] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:24,954] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:24,955] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:28,540] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:28,541] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:33,342] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:33,342] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:34,916] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:34,916] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:38,203] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:38,204] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:42,262] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:42,263] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:45,852] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:45,853] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:48,537] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:48,538] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:52,530] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:52,530] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:37:56,607] [INFO] [evoluter._update_elitist] - Iteration 1\n",
      "                Elitist score: 0.6031870802819591\n",
      "[2025-12-20 15:37:56,608] [DEBUG] [evoluter._update_elitist] - Elitist text:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:37:56,608] [DEBUG] [evoluter.evolution] - Elitist should always live\n",
      "[2025-12-20 15:37:56,609] [INFO] [evoluter._update_iter] - Iteration 1 finished...\n",
      "[2025-12-20 15:37:56,609] [INFO] [evoluter._update_iter] - Best score: 0.6031870802819591\n",
      "[2025-12-20 15:37:59,555] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:37:59,556] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:37:59,556] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:01,446] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:01,447] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:04,996] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:04,996] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:08,089] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:08,089] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:10,834] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:10,835] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:14,591] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:14,592] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:18,451] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:18,452] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:20,437] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:20,440] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:23,632] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:23,633] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:27,016] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:27,016] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:35,658] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:38:35,659] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:35,659] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:37,765] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:37,766] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:41,551] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:41,551] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:44,727] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:44,728] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:46,491] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:46,491] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:50,256] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:50,257] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:54,863] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:54,864] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:56,708] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:56,708] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:38:58,359] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:38:58,360] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:01,135] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:01,136] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:02,607] [INFO] [evoluter._update_elitist] - Iteration 2\n",
      "                Elitist score: 0.6031870802819591\n",
      "[2025-12-20 15:39:02,608] [DEBUG] [evoluter._update_elitist] - Elitist text:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:39:02,612] [INFO] [evoluter._update_iter] - Iteration 2 finished...\n",
      "[2025-12-20 15:39:02,619] [INFO] [evoluter._update_iter] - Best score: 0.6031870802819591\n",
      "[2025-12-20 15:39:05,228] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:39:05,229] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:05,229] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:08,726] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:08,727] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:11,789] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:11,790] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:14,627] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:14,628] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:17,550] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:17,550] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:19,107] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:19,107] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:22,615] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:22,616] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:26,200] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:26,200] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:29,726] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:29,727] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:31,575] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:31,575] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:39:38,157] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:39:38,157] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:38,157] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for myself or others.\n",
      "[2025-12-20 15:39:40,230] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:40,230] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for yourself or others.\n",
      "[2025-12-20 15:39:42,054] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:42,055] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for yourself or others.\n",
      "[2025-12-20 15:39:43,940] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:43,941] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for yourself or others.\n",
      "[2025-12-20 15:39:46,354] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:46,354] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for myself or others.\n",
      "[2025-12-20 15:39:48,075] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:48,076] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for yourself or others.\n",
      "[2025-12-20 15:39:50,326] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:50,328] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text without unnecessary elaboration.\n",
      "[2025-12-20 15:39:52,211] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:52,212] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for myself or others.\n",
      "[2025-12-20 15:39:53,655] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:53,656] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for yourself or others.\n",
      "[2025-12-20 15:39:56,204] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:39:56,205] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text, ensuring that it captures the key themes and ideas while maintaining coherence. The summary should be limited to exactly 4 sentences. This will help facilitate understanding of the text for myself or others.\n",
      "[2025-12-20 15:39:58,778] [INFO] [evoluter._update_elitist] - Iteration 3\n",
      "                Elitist score: 0.6031870802819591\n",
      "[2025-12-20 15:39:58,778] [DEBUG] [evoluter._update_elitist] - Elitist text:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:39:58,779] [INFO] [evoluter._update_iter] - Iteration 3 finished...\n",
      "[2025-12-20 15:39:58,780] [INFO] [evoluter._update_iter] - Best score: 0.6031870802819591\n",
      "[2025-12-20 15:40:02,478] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:40:02,480] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:02,481] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:05,186] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:05,187] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:08,720] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:08,720] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:14,023] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:14,023] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please distill the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:16,071] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:16,071] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:18,227] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:18,227] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:20,436] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:20,437] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide a concise summary of the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:24,670] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:24,671] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:29,469] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:29,470] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:32,144] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:32,144] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:40:39,237] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:40:39,237] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:39,237] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the most important points clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:41,385] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:41,386] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the most important points clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:44,128] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:44,129] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the key themes and ideas clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:46,072] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:46,073] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the most important points clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:48,163] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:48,164] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the most important points clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:50,466] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:50,467] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the key themes and ideas clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:52,118] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:52,119] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the most important points clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:56,620] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:56,620] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the key themes and ideas clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:58,073] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:58,074] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the most important points clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:40:59,757] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:40:59,758] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text, ensuring that the summary captures the key themes and ideas clearly and concisely, in exactly 4 sentences.\n",
      "[2025-12-20 15:41:01,521] [INFO] [evoluter._update_elitist] - Iteration 4\n",
      "                Elitist score: 0.6031870802819591\n",
      "[2025-12-20 15:41:01,521] [DEBUG] [evoluter._update_elitist] - Elitist text:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:41:01,521] [DEBUG] [evoluter.evolution] - Elitist should always live\n",
      "[2025-12-20 15:41:01,522] [INFO] [evoluter._update_iter] - Iteration 4 finished...\n",
      "[2025-12-20 15:41:01,522] [INFO] [evoluter._update_iter] - Best score: 0.6031870802819591\n",
      "[2025-12-20 15:41:01,525] [INFO] [evoluter.evolution] - BEST TRAIN SCORE: 0.6031870802819591\n",
      "[2025-12-20 15:41:01,525] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-20 15:41:01,526] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:41:01,526] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:41:03,863] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:41:03,864] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:06,490] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:41:06,491] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:10,416] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:41:10,417] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Distill the text into exactly 4 sentences.\n",
      "[2025-12-20 15:41:12,198] [INFO] [evoluter.evolution] - BEST VALIDATION SCORE: 0.38959539455002823\n",
      "[2025-12-20 15:41:12,199] [DEBUG] [evoluter.evolution] - BEST PROMPT:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:12,200] [DEBUG] [evoluter.evolution] - Final prompt with frozen parts integrated:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:12,200] [INFO] [run.reflectiveprompt] - ReflectivePrompt optimization completed\n",
      "[2025-12-20 15:41:12,201] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-12-20 15:41:13,610] [DEBUG] [assistant.run] - Final prompt:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:13,610] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-12-20 15:41:13,610] [DEBUG] [prompt_freezer.split_prompt] - Found 1 frozen part(s). Frozen content: in exactly 4 sentences....\n",
      "[2025-12-20 15:41:13,611] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:41:13,611] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:15,257] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:41:15,257] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please summarize the given text in exactly 4 sentences.\n",
      "[2025-12-20 15:41:18,431] [INFO] [assistant.run] - Initial meteor score: 0.3595822723192118, final meteor score: 0.3842779020993511\n",
      "[2025-12-20 15:41:18,431] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-12-20 15:41:22,724] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-12-20 15:41:22,724] [INFO] [assistant.run] - Your initial prompt included the phrase '<freeze>' which may have been intended to indicate a specific text or context but was unclear in its purpose. The final prompt simplifies the request by removing this ambiguity and directly asking for a summary of the 'given text'. This change enhances clarity and ensures that the AI understands the task without confusion. Additionally, the final prompt maintains the requirement for a summary in exactly 4 sentences, preserving your original intent. A key lesson here is to avoid unnecessary or unclear formatting that could distract from the main request.\n"
     ]
    }
   ],
   "source": [
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"reflective\",\n",
    "    metric=\"meteor\",\n",
    "    geval_evaluation_steps=geval_steps,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654127e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Summarize the text <freeze>in exactly 4 sentences.</freeze>\n",
      "Final: Please summarize the given text in exactly 4 sentences.\n",
      "Initial score: 0.3596\n",
      "Final score: 0.3843\n",
      "Frozen preserved: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial: {start_prompt}\")\n",
    "print(f\"Final: {final_prompt}\")\n",
    "print(f\"Initial score: {tuner.init_metric:.4f}\")\n",
    "print(f\"Final score: {tuner.final_metric:.4f}\")\n",
    "print(f\"Frozen preserved: {frozen in final_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e5901",
   "metadata": {},
   "source": [
    "## DistillPrompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c184eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:41:22,740] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-12-20 15:41:22,741] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-12-20 15:41:22,742] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-12-20 15:41:23,917] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-12-20 15:41:27,129] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-12-20 15:41:27,130] [INFO] [assistant.run] - Method: distill, Task: generation\n",
      "[2025-12-20 15:41:27,130] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-12-20 15:41:27,131] [INFO] [assistant.run] - Dataset: 5 samples\n",
      "[2025-12-20 15:41:27,131] [INFO] [assistant.run] - Target: 5 samples\n",
      "[2025-12-20 15:41:27,131] [INFO] [distiller.__init__] - Found frozen parts in prompt. LLM will preserve them in natural position.\n",
      "[2025-12-20 15:41:27,134] [INFO] [distiller.distillation] - Starting DistillPrompt optimization...\n",
      "[2025-12-20 15:41:27,134] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "  0%|          | 0/3 [00:00<?, ?it/s][2025-12-20 15:41:30,542] [INFO] [distiller.distillation] - Starting round 0\n",
      "[2025-12-20 15:41:32,374] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:34,967] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:37,478] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:39,630] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:43,608] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:45,309] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:47,784] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:50,303] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:54,879] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:57,243] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:41:59,813] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:02,976] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:05,834] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:08,907] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:10,865] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:13,247] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:16,822] [INFO] [distiller.distillation] - Best candidate score in round 0: 0.4306917446600142\n",
      " 33%|      | 1/3 [00:46<01:32, 46.28s/it][2025-12-20 15:42:16,825] [INFO] [distiller.distillation] - Starting round 1\n",
      "[2025-12-20 15:42:17,663] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:20,485] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:23,110] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:25,707] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:29,286] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:32,671] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:35,745] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:39,639] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:44,307] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:46,396] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:48,457] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:50,433] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:53,150] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:55,909] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:42:58,476] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:00,218] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:01,855] [INFO] [distiller.distillation] - Best candidate score in round 1: 0.4624687955387759\n",
      " 67%|   | 2/3 [01:31<00:45, 45.55s/it][2025-12-20 15:43:01,858] [INFO] [distiller.distillation] - Starting round 2\n",
      "[2025-12-20 15:43:05,125] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:07,172] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:08,791] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:10,679] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:14,023] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:15,551] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:18,445] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:20,143] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:23,396] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:24,690] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:26,151] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:27,865] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:31,272] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:34,207] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:35,963] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:38,004] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:43:39,592] [INFO] [distiller.distillation] - Best candidate score in round 2: 0.5107728030825858\n",
      "100%|| 3/3 [02:09<00:00, 43.02s/it]\n",
      "[2025-12-20 15:43:39,595] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:43:42,101] [INFO] [distiller.distillation] - Final best prompt score on validation: 0.3954717839805826\n",
      "[2025-12-20 15:43:42,102] [INFO] [distiller.distillation] - DistillPrompt optimization completed\n",
      "[2025-12-20 15:43:42,103] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-12-20 15:43:43,152] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-12-20 15:43:43,153] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:43:44,702] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:43:47,346] [INFO] [assistant.run] - Initial meteor score: 0.5205144652655944, final meteor score: 0.3963827151546716\n",
      "[2025-12-20 15:43:47,346] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-12-20 15:43:51,617] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-12-20 15:43:51,618] [INFO] [assistant.run] - Your initial prompt was straightforward but lacked clarity on how to handle specific elements of the text. The final prompt improves upon this by using the term 'condense' instead of 'summarize,' which emphasizes the need for brevity while retaining essential details. Additionally, it explicitly states to 'preserve essential details and organization,' guiding the AI to focus on maintaining the text's structure. The instruction to 'not alter any frozen fragments' adds a crucial layer of specificity, ensuring that important parts of the text are treated with care. Finally, the emphasis on 'brevity and coherence without unnecessary elaboration' helps to clarify the desired style of the summary. Key takeaway: Be specific about the requirements and constraints to guide the AI effectively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Summarize the text <freeze>in exactly 4 sentences.</freeze>\n",
      "Final: Condense the text while preserving essential details and organization in exactly 4 sentences. Ensure all key information is maintained and the structure remains clear, focusing on brevity and coherence without unnecessary elaboration. Do not alter any frozen fragments and include each one only once, exactly as written.\n",
      "Initial score: 0.5205\n",
      "Final score: 0.3964\n",
      "Frozen preserved: True\n"
     ]
    }
   ],
   "source": [
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "final_prompt_distill = tuner.run(\n",
    "    start_prompt=start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"distill\",\n",
    "    metric=\"meteor\",\n",
    "    num_epochs=3,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "_, frozen_check = split_prompt(start_prompt)\n",
    "\n",
    "print(f\"Initial: {start_prompt}\")\n",
    "print(f\"Final: {final_prompt_distill}\")\n",
    "print(f\"Initial score: {tuner.init_metric:.4f}\")\n",
    "print(f\"Final score: {tuner.final_metric:.4f}\")\n",
    "print(f\"Frozen preserved: {frozen_check in final_prompt_distill}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa11319",
   "metadata": {},
   "source": [
    "## Muliple freeze tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472031fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizable: Summarize the text .\n",
      "Frozen: in exactly 4 sentences Start your response with 'Summary:' prefix.\n"
     ]
    }
   ],
   "source": [
    "multi_freeze_prompt = \"Summarize the text <freeze>in exactly 4 sentences</freeze>. <freeze>Start your response with 'Summary:' prefix.</freeze>\"\n",
    "optimizable_multi, frozen_multi = split_prompt(multi_freeze_prompt)\n",
    "print(f\"Optimizable: {optimizable_multi}\")\n",
    "print(f\"Frozen: {frozen_multi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670fd61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:43:51,639] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-12-20 15:43:51,640] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-12-20 15:43:51,640] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-12-20 15:43:51,641] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-12-20 15:43:54,177] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-12-20 15:43:54,178] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-12-20 15:43:54,178] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-12-20 15:43:54,179] [INFO] [assistant.run] - Dataset: 5 samples\n",
      "[2025-12-20 15:43:54,179] [INFO] [assistant.run] - Target: 5 samples\n",
      "[2025-12-20 15:43:54,180] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-12-20 15:43:54,180] [INFO] [hype.hype_optimizer] - Found 2 frozen part(s) in prompt\n",
      "[2025-12-20 15:43:55,485] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-12-20 15:43:55,486] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-12-20 15:43:57,043] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-12-20 15:43:57,043] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:44:04,621] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:44:11,585] [INFO] [assistant.run] - Initial geval score: 0.6, final geval score: 0.6\n",
      "[2025-12-20 15:44:11,586] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-12-20 15:44:15,577] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-12-20 15:44:15,578] [INFO] [assistant.run] - Your initial prompt was clear in its request for a summary, but the final prompt enhances clarity and structure. The phrase 'Please provide a concise summary of the text' is more polite and formal, which can encourage a more thoughtful response. The instruction to ensure the response is structured in exactly 4 sentences is retained, but the wording is slightly refined for better flow. The directive to start with 'Summary:' is also preserved, ensuring consistency in the output format. Overall, the final prompt maintains your original intent while improving the tone and clarity, which can lead to a more effective response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Summarize the text <freeze>in exactly 4 sentences</freeze>. <freeze>Start your response with 'Summary:' prefix.</freeze>\n",
      "Final: Please provide a concise summary of the text, ensuring that the response is structured in exactly 4 sentences. Additionally, Start your response with 'Summary:' prefix.\n"
     ]
    }
   ],
   "source": [
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "final_prompt_multi = tuner.run(\n",
    "    start_prompt=multi_freeze_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_evaluation_steps=geval_steps,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(f\"Initial: {multi_freeze_prompt}\")\n",
    "print(f\"Final: {final_prompt_multi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bea796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-20 15:44:15,587] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-12-20 15:44:15,587] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-12-20 15:44:15,588] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-12-20 15:44:16,577] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-12-20 15:44:18,604] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-12-20 15:44:18,604] [INFO] [assistant.run] - Method: distill, Task: generation\n",
      "[2025-12-20 15:44:18,605] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-12-20 15:44:18,605] [INFO] [assistant.run] - Dataset: 5 samples\n",
      "[2025-12-20 15:44:18,605] [INFO] [assistant.run] - Target: 5 samples\n",
      "[2025-12-20 15:44:18,606] [INFO] [distiller.__init__] - Found frozen parts in prompt. LLM will preserve them in natural position.\n",
      "[2025-12-20 15:44:18,607] [INFO] [distiller.distillation] - Starting DistillPrompt optimization...\n",
      "[2025-12-20 15:44:18,607] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "  0%|          | 0/5 [00:00<?, ?it/s][2025-12-20 15:44:21,806] [INFO] [distiller.distillation] - Starting round 0\n",
      "[2025-12-20 15:44:28,176] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:32,182] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:34,837] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:37,603] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:43,308] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:48,173] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:51,324] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:54,808] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:44:59,204] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:02,181] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:05,278] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:08,919] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:13,130] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:17,006] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:19,388] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:22,834] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:26,590] [INFO] [distiller.distillation] - Best candidate score in round 0: 0.41274057524839486\n",
      " 20%|        | 1/5 [01:04<04:19, 64.79s/it][2025-12-20 15:45:26,594] [INFO] [distiller.distillation] - Starting round 1\n",
      "[2025-12-20 15:45:28,694] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:30,926] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:34,792] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:37,940] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:42,086] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:44,276] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:48,471] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:51,500] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:55,211] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:45:57,686] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:00,661] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:06,694] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:10,574] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:14,809] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:17,248] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:19,391] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:21,463] [INFO] [distiller.distillation] - Best candidate score in round 1: 0.41274057524839486\n",
      " 40%|      | 2/5 [01:59<02:56, 58.95s/it][2025-12-20 15:46:21,465] [INFO] [distiller.distillation] - Starting round 2\n",
      "[2025-12-20 15:46:23,039] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:25,927] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:28,913] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:32,485] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:36,994] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:40,313] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:43,257] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:47,141] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:52,559] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:46:56,890] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:00,351] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:04,650] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:09,083] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:13,134] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:16,673] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:20,627] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:25,648] [INFO] [distiller.distillation] - Best candidate score in round 2: 0.41274057524839486\n",
      " 60%|    | 3/5 [03:03<02:02, 61.34s/it][2025-12-20 15:47:25,651] [INFO] [distiller.distillation] - Starting round 3\n",
      "[2025-12-20 15:47:26,663] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:30,261] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:33,247] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:36,029] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:41,261] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:45,478] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:49,096] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:51,627] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:47:57,073] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:00,368] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:03,348] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:06,136] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:11,201] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:15,303] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:18,389] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:22,377] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:26,165] [INFO] [distiller.distillation] - Best candidate score in round 3: 0.41274057524839486\n",
      " 80%|  | 4/5 [04:04<01:01, 61.02s/it][2025-12-20 15:48:26,169] [INFO] [distiller.distillation] - Starting round 4\n",
      "[2025-12-20 15:48:27,589] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:31,022] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:34,051] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:36,365] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:42,594] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:46,032] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:49,614] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:51,538] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:48:57,082] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:00,690] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:03,848] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:06,610] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:12,851] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:17,457] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:19,719] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:22,249] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-20 15:49:24,839] [INFO] [distiller.distillation] - Best candidate score in round 4: 0.41274057524839486\n",
      "100%|| 5/5 [05:03<00:00, 60.61s/it]\n",
      "[2025-12-20 15:49:24,842] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:49:27,138] [INFO] [distiller.distillation] - Final best prompt score on validation: 0.4753576685900418\n",
      "[2025-12-20 15:49:27,140] [INFO] [distiller.distillation] - DistillPrompt optimization completed\n",
      "[2025-12-20 15:49:27,140] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-12-20 15:49:28,219] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-12-20 15:49:28,220] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:49:30,940] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 2 samples\n",
      "[2025-12-20 15:49:34,361] [INFO] [assistant.run] - Initial meteor score: 0.45068817359433067, final meteor score: 0.4496434022525419\n",
      "[2025-12-20 15:49:34,361] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-12-20 15:49:38,244] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-12-20 15:49:38,245] [INFO] [assistant.run] - Your initial prompt included the use of <freeze> tags, which are unnecessary in this context. The final prompt effectively removes these tags while retaining the core instructions, making it cleaner and more straightforward. By simplifying the structure, the prompt becomes easier for the AI to interpret and respond to without any potential confusion caused by formatting. The essential requirementssummarizing in 4 sentences and starting with 'Summary:'are preserved, ensuring that the intent remains intact. A key lesson here is to avoid extraneous formatting unless it serves a specific purpose, as clarity is crucial in prompt engineering.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Summarize the text <freeze>in exactly 4 sentences</freeze>. <freeze>Start your response with 'Summary:' prefix.</freeze>\n",
      "Final: Summarize the text in exactly 4 sentences. Start your response with 'Summary:' prefix.\n"
     ]
    }
   ],
   "source": [
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "final_prompt_multi = tuner.run(\n",
    "    start_prompt=multi_freeze_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"distill\",\n",
    "    metric=\"meteor\",\n",
    "    geval_evaluation_steps=geval_steps,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(f\"Initial: {multi_freeze_prompt}\")\n",
    "print(f\"Final: {final_prompt_multi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4d789",
   "metadata": {},
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f64d714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: Summarize the text <freeze>in exactly 3 sentences</freeze>\n",
      "PASS: Summarize the text <freeze>in english</freeze> and make it <freeze>concise</freeze>\n",
      "PASS: Summarize the text <freeze>in 3 sentences\n",
      "PASS: Summarize the text <freeze>in english</freeze></freeze>\n",
      "PASS: Summarize the text <freeze>in english</freeze> <freeze>concise\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.utils.prompt_freezer import validate_freeze_tags\n",
    "\n",
    "test1 = \"Summarize the text <freeze>in exactly 3 sentences</freeze>\"\n",
    "test2 = \"Summarize the text <freeze>in english</freeze> and make it <freeze>concise</freeze>\"\n",
    "test3 = \"Summarize the text <freeze>in 3 sentences\"\n",
    "test4 = \"Summarize the text <freeze>in english</freeze></freeze>\"\n",
    "test5 = \"Summarize the text <freeze>in english</freeze> <freeze>concise\"\n",
    "\n",
    "tests = [\n",
    "    (test1, True),\n",
    "    (test2, True),\n",
    "    (test3, False),\n",
    "    (test4, False),\n",
    "    (test5, False),\n",
    "]\n",
    "\n",
    "for test, should_pass in tests:\n",
    "    try:\n",
    "        validate_freeze_tags(test)\n",
    "        result = \"PASS\" if should_pass else \"FAIL\"\n",
    "    except ValueError:\n",
    "        result = \"PASS\" if not should_pass else \"FAIL\"\n",
    "    print(f\"{result}: {test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790a9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: Found 1 opening tags <freeze> but 0 closing tags </freeze>. Each opening tag must have closing tag.\n"
     ]
    }
   ],
   "source": [
    "bad_prompt = \"Text <freeze>frozen\"\n",
    "try:\n",
    "    split_prompt(bad_prompt)\n",
    "    print(\"FAIL: split_prompt should raise ValueError\")\n",
    "except ValueError as e:\n",
    "    print(f\"PASS: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
