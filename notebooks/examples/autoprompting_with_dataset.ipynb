{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f56aae-ee54-4085-a36e-4240b7a90931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dd4df-d2fd-4c63-bc0b-27f1a1f46ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration stanfordnlp--sst2-c614fb49d6bf6d65\n",
      "Reusing dataset parquet (/home/sitkina-alena/.cache/huggingface/datasets/stanfordnlp___parquet/stanfordnlp--sst2-c614fb49d6bf6d65/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b38078c5a04fe8a8202d6e806c9338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "class_dataset = sst2['train']['sentence'][:100]\n",
    "class_targets = sst2['train']['label'][:100]\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "gen_dataset = samsum['train']['dialogue'][:100]\n",
    "gen_targets = samsum['train']['summary'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85cc60",
   "metadata": {},
   "source": [
    "Starting with PromptTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56671cb8-25c0-4eee-b469-c3cf45c6e2e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to infer device type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Define an initial prompt\u001b[39;00m\n\u001b[32m      4\u001b[39m start_prompt = \u001b[33m'\u001b[39m\u001b[33mPerform Sentiment Classification task.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tuner = \u001b[43mPromptTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoolPrompt/coolprompt/assistant.py:43\u001b[39m, in \u001b[36mPromptTuner.__init__\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: BaseLanguageModel = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initializes the tuner with a LangChain-compatible language model.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m \u001b[33;03m            Will use DefaultLLM if not provided.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28mself\u001b[39m._model = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mDefaultLLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m.init_metric = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m.init_prompt = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoolPrompt/coolprompt/language_model/llm.py:43\u001b[39m, in \u001b[36mDefaultLLM.init\u001b[39m\u001b[34m(langchain_config, vllm_engine_config)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m langchain_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     41\u001b[39m     generation_and_model_config.update(langchain_config)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEFAULT_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfloat16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_engine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_and_model_config\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/pydantic/_internal/_decorators_v1.py:148\u001b[39m, in \u001b[36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[39m\u001b[34m(values, _)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/langchain_core/utils/pydantic.py:168\u001b[39m, in \u001b[36mpre_init.<locals>.wrapper\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    165\u001b[39m             values[name] = field_info.default\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/langchain_community/llms/vllm.py:89\u001b[39m, in \u001b[36mVLLM.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import vllm python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install vllm`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m values[\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mVLLModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensor_parallel_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrust_remote_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdownload_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvllm_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/vllm/utils.py:1028\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1021\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m         warnings.warn(\n\u001b[32m   1024\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1025\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1026\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/vllm/entrypoints/llm.py:210\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mself\u001b[39m.get_engine_class()\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/vllm/engine/llm_engine.py:582\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m engine_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m executor_class = \u001b[38;5;28mcls\u001b[39m._get_executor_cls(engine_config)\n\u001b[32m    584\u001b[39m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/vllm/engine/arg_utils.py:958\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    951\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBitsAndBytes load format and QLoRA adapter only support \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    952\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m quantization, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.quantization\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpu_offload_gb >= \u001b[32m0\u001b[39m, (\n\u001b[32m    955\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCPU offload space must be non-negative\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    956\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.cpu_offload_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m device_config = \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m model_config = \u001b[38;5;28mself\u001b[39m.create_model_config()\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_config.is_multimodal_model:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/name/lib/python3.11/site-packages/vllm/config.py:1208\u001b[39m, in \u001b[36mDeviceConfig.__init__\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m   1206\u001b[39m         \u001b[38;5;28mself\u001b[39m.device_type = \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFailed to infer device type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1210\u001b[39m     \u001b[38;5;66;03m# Device type is assigned explicitly\u001b[39;00m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_type = device\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to infer device type"
     ]
    }
   ],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "\n",
    "# Define an initial prompt\n",
    "class_start_prompt = 'Perform Sentiment Classification task.'\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = PromptTuner()\n",
    "\n",
    "# Call prompt optimization with dataset and target\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df7ab2",
   "metadata": {},
   "source": [
    "You can now get initial and final prompt metrics from tuner fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc088b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final prompt:\", final_prompt)\n",
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df89a78",
   "metadata": {},
   "source": [
    "You can do the same with generation task\n",
    "\n",
    "Also you can reuse previous tuner binded with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_start_prompt = \"Summarize the text\"\n",
    "\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=gen_start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"meteor\"\n",
    ")\n",
    "\n",
    "print(\"Final prompt:\", final_prompt)\n",
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea40cb",
   "metadata": {},
   "source": [
    "Currently supported metrics are\n",
    "- accuracy and f1 for classification\n",
    "- meteor, bleu and rouge for generation\n",
    "\n",
    "Also, task type must correspond the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"rouge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd599b2d",
   "metadata": {},
   "source": [
    "There are two ways to initialize tuner with your custom LLM\n",
    "\n",
    "To init a model by yourself and pass it to the tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "my_model = VLLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    dtype='float16',\n",
    ")\n",
    "\n",
    "tuner_with_custom_llm = PromptTuner(model=my_model)\n",
    "tuner_with_custom_llm.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf05ca",
   "metadata": {},
   "source": [
    "Or to change config of our default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0088a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coolprompt.language_model.llm import DefaultLLM\n",
    "\n",
    "changed_model = DefaultLLM.init(langchain_config={\n",
    "    'max_new_tokens': 1000,\n",
    "    \"temperature\": 0.0,\n",
    "})\n",
    "\n",
    "tuner_with_changed_llm = PromptTuner(model=changed_model)\n",
    "tuner_with_changed_llm.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396516",
   "metadata": {},
   "source": [
    "You can access prompts and their metrics via tuner fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(tuner):\n",
    "    print(\"Start prompt:\", tuner.start_prompt)\n",
    "    print(\"Final prompt:\", tuner.final_prompt)\n",
    "    print(\"Start prompt metric: \", tuner.init_metric)\n",
    "    print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1822",
   "metadata": {},
   "source": [
    "There are 3 currently implemented optimizers:\n",
    "- HyPE (optimizing with predefined system instruction)\n",
    "- DistillPrompt\n",
    "- ReflectivePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task=\"classification\",\n",
    "    method=\"hype\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"accuracy\"\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task='classification',\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    method='distill',\n",
    "    use_cache=True,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f677f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task='classification',\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    method='reflective',\n",
    "    problem_description='sentiment classification',\n",
    "    use_cache=False,\n",
    "    population_size=4,\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ebe58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
