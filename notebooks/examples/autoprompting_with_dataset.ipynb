{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdecc8f3",
   "metadata": {},
   "source": [
    "# AutoPrompting with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dd4df-d2fd-4c63-bc0b-27f1a1f46ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "class_dataset = sst2['train']['sentence'][:100]\n",
    "class_targets = sst2['train']['label'][:100]\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "gen_dataset = samsum['train']['dialogue'][:100]\n",
    "gen_targets = samsum['train']['summary'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85cc60",
   "metadata": {},
   "source": [
    "Starting with PromptTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56671cb8-25c0-4eee-b469-c3cf45c6e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "\n",
    "# Define an initial prompt\n",
    "class_start_prompt = 'Perform Sentiment Classification task.'\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = PromptTuner()\n",
    "\n",
    "# Call prompt optimization with dataset and target\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df7ab2",
   "metadata": {},
   "source": [
    "You can now get initial and final prompt metrics from tuner fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc088b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final prompt:\", final_prompt)\n",
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df89a78",
   "metadata": {},
   "source": [
    "You can do the same with generation task\n",
    "\n",
    "Also you can reuse previous tuner binded with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_start_prompt = \"Summarize the text\"\n",
    "\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=gen_start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"meteor\"\n",
    ")\n",
    "\n",
    "print(\"Final prompt:\", final_prompt)\n",
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea40cb",
   "metadata": {},
   "source": [
    "Currently supported metrics are\n",
    "- accuracy and f1 for classification\n",
    "- meteor, bleu and rouge for generation\n",
    "\n",
    "Also, task type must correspond the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"rouge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd599b2d",
   "metadata": {},
   "source": [
    "There are two ways to initialize tuner with your custom LLM\n",
    "\n",
    "To init a model by yourself and pass it to the tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "my_model = VLLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    dtype='float16',\n",
    ")\n",
    "\n",
    "tuner_with_custom_llm = PromptTuner(model=my_model)\n",
    "tuner_with_custom_llm.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf05ca",
   "metadata": {},
   "source": [
    "Or to change config of our default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0088a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coolprompt.language_model.llm import DefaultLLM\n",
    "\n",
    "changed_model = DefaultLLM.init(langchain_config={\n",
    "    'max_new_tokens': 1000,\n",
    "    \"temperature\": 0.0,\n",
    "})\n",
    "\n",
    "tuner_with_changed_llm = PromptTuner(model=changed_model)\n",
    "tuner_with_changed_llm.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396516",
   "metadata": {},
   "source": [
    "You can access prompts and their metrics via tuner fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(tuner):\n",
    "    print(\"Start prompt:\", tuner.start_prompt)\n",
    "    print(\"Final prompt:\", tuner.final_prompt)\n",
    "    print(\"Start prompt metric: \", tuner.init_metric)\n",
    "    print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1822",
   "metadata": {},
   "source": [
    "There are 3 currently implemented optimizers:\n",
    "- HyPE (optimizing with predefined system instruction)\n",
    "- DistillPrompt\n",
    "- ReflectivePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task=\"classification\",\n",
    "    method=\"hype\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"accuracy\"\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task='classification',\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    method='distill',\n",
    "    use_cache=True,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f677f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task='classification',\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    method='reflective',\n",
    "    problem_description='sentiment classification',\n",
    "    use_cache=False,\n",
    "    population_size=4,\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ebe58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
