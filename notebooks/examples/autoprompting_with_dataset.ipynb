{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdecc8f3",
   "metadata": {},
   "source": [
    "# AutoPrompting with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86dd4df-d2fd-4c63-bc0b-27f1a1f46ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "class_dataset = sst2['train']['sentence'][:100]\n",
    "class_targets = sst2['train']['label'][:100]\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "gen_dataset = samsum['train']['dialogue'][:100]\n",
    "gen_targets = samsum['train']['summary'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85cc60",
   "metadata": {},
   "source": [
    "Starting with PromptTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56671cb8-25c0-4eee-b469-c3cf45c6e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 00:56:30,551] [INFO] [llm.init] - Initializing default model\n",
      "[2025-07-09 00:56:30,551] [DEBUG] [llm.init] - Updating default model params with langchain config: None and vllm_engine_config: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 00:56:32 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 07-09 00:56:32 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-09 00:56:40 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-09 00:56:40 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='t-tech/T-lite-it-1.0', speculative_config=None, tokenizer='t-tech/T-lite-it-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=t-tech/T-lite-it-1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 07-09 00:56:42 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-09 00:56:42 cuda.py:226] Using XFormers backend.\n",
      "INFO 07-09 00:56:43 model_runner.py:1110] Starting to load model t-tech/T-lite-it-1.0...\n",
      "INFO 07-09 00:56:43 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc6b5ec3f6d44c4b0adeaa1f21fcb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 00:58:48 model_runner.py:1115] Loading model weights took 14.2426 GB\n",
      "INFO 07-09 00:58:53 worker.py:267] Memory profiling takes 5.28 seconds\n",
      "INFO 07-09 00:58:53 worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.27GiB\n",
      "INFO 07-09 00:58:53 worker.py:267] model weights take 14.24GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 2.62GiB.\n",
      "INFO 07-09 00:58:54 executor_base.py:111] # cuda blocks: 3069, # CPU blocks: 4681\n",
      "INFO 07-09 00:58:54 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 1.50x\n",
      "INFO 07-09 00:59:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████████████████████| 35/35 [00:16<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 00:59:18 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.21 GiB\n",
      "INFO 07-09 00:59:18 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 30.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-07-09 00:59:18,449] [INFO] [assistant.__init__] - Validating the model: VLLM\n",
      "[2025-07-09 00:59:18,451] [INFO] [assistant.__init__] - PromptTuner successfully initialized with model: VLLM\n",
      "[2025-07-09 00:59:18,452] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-07-09 00:59:19,205] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with f1 metric\n",
      "[2025-07-09 00:59:19,206] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 00:59:19,206] [INFO] [assistant.run] - Method: hype, Task: classification\n",
      "[2025-07-09 00:59:19,207] [INFO] [assistant.run] - Metric: f1, Validation size: 0.25\n",
      "[2025-07-09 00:59:19,208] [INFO] [assistant.run] - Dataset: 100 samples\n",
      "[2025-07-09 00:59:19,208] [INFO] [assistant.run] - Target: 100 samples\n",
      "[2025-07-09 00:59:19,209] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 328.07 toks/s, out\n",
      "[2025-07-09 00:59:19,993] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-07-09 00:59:19,993] [INFO] [assistant.run] - Evaluating on given dataset for classification task...\n",
      "[2025-07-09 00:59:19,994] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 100 samples\n",
      "Processed prompts: 100%|█| 100/100 [01:48<00:00,  1.08s/it, est. speed input: 276.26 toks/s,\n",
      "[2025-07-09 01:01:08,489] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 100 samples\n",
      "Processed prompts: 100%|█| 100/100 [00:05<00:00, 16.94it/s, est. speed input: 5260.67 toks/s\n",
      "[2025-07-09 01:01:14,480] [INFO] [assistant.run] - Initial f1 score: 0.6364983164983165, final f1 score: 0.8899889988998899\n",
      "[2025-07-09 01:01:14,481] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "\n",
    "# Define an initial prompt\n",
    "class_start_prompt = \"Classify sentence sentiment\"\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = PromptTuner()\n",
    "\n",
    "# Call prompt optimization with dataset and target\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc088b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Please provide a sentence and classify its sentiment as positive, negative, or neutral.\n",
      "Start prompt metric:  0.6364983164983165\n",
      "Final prompt metric:  0.8899889988998899\n"
     ]
    }
   ],
   "source": [
    "print(\"Final prompt:\", final_prompt)\n",
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df89a78",
   "metadata": {},
   "source": [
    "You can do the same with generation task\n",
    "\n",
    "Also you can reuse previous tuner binded to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3132db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 01:09:57,414] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-07-09 01:09:59,450] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with meteor metric\n",
      "[2025-07-09 01:09:59,450] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 01:09:59,451] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-07-09 01:09:59,451] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-07-09 01:09:59,452] [INFO] [assistant.run] - Dataset: 100 samples\n",
      "[2025-07-09 01:09:59,453] [INFO] [assistant.run] - Target: 100 samples\n",
      "[2025-07-09 01:09:59,453] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 275.80 toks/s, out\n",
      "[2025-07-09 01:10:00,386] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-07-09 01:10:00,387] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-07-09 01:10:00,387] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 100 samples\n",
      "Processed prompts: 100%|█| 100/100 [00:10<00:00,  9.87it/s, est. speed input: 3863.51 toks/s\n",
      "[2025-07-09 01:10:13,235] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 100 samples\n",
      "Processed prompts: 100%|█| 100/100 [00:11<00:00,  9.00it/s, est. speed input: 3658.18 toks/s\n",
      "[2025-07-09 01:10:24,551] [INFO] [assistant.run] - Initial meteor score: 0.2889385547946554, final meteor score: 0.30479966549868787\n",
      "[2025-07-09 01:10:24,551] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Please provide a concise summary of the text, focusing on the main points and omitting any unnecessary details.\n",
      "Start prompt metric:  0.2889385547946554\n",
      "Final prompt metric:  0.30479966549868787\n"
     ]
    }
   ],
   "source": [
    "gen_start_prompt = \"Summarize this text\"\n",
    "\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=gen_start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"meteor\"\n",
    ")\n",
    "\n",
    "print(\"Final prompt:\", final_prompt)\n",
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea40cb",
   "metadata": {},
   "source": [
    "Currently supported metrics are\n",
    "- accuracy and f1 for classification\n",
    "- meteor, bleu and rouge for generation\n",
    "\n",
    "Also, task type must correspond the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd599b2d",
   "metadata": {},
   "source": [
    "There are two ways to initialize tuner with your custom LLM\n",
    "\n",
    "To init a model by yourself and pass it to the tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc6647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 13:17:00 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 07-09 13:17:00 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-09 13:17:08 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 07-09 13:17:08 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 07-09 13:17:08 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 07-09 13:17:08 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 07-09 13:17:10 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-09 13:17:10 cuda.py:226] Using XFormers backend.\n",
      "INFO 07-09 13:17:11 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 07-09 13:17:11 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 07-09 13:17:11 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664a732b46ee443499fc06bd33c7430c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 13:17:32 model_runner.py:1115] Loading model weights took 3.3460 GB\n",
      "INFO 07-09 13:17:33 worker.py:267] Memory profiling takes 0.51 seconds\n",
      "INFO 07-09 13:17:33 worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.27GiB\n",
      "INFO 07-09 13:17:33 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 16.48GiB.\n",
      "INFO 07-09 13:17:33 executor_base.py:111] # cuda blocks: 38573, # CPU blocks: 9362\n",
      "INFO 07-09 13:17:33 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 4.71x\n",
      "INFO 07-09 13:17:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████████████████████| 35/35 [00:14<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 13:17:53 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.20 GiB\n",
      "INFO 07-09 13:17:53 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-07-09 13:17:54,325] [INFO] [assistant.__init__] - Validating the model: VLLM\n",
      "[2025-07-09 13:17:54,326] [INFO] [assistant.__init__] - PromptTuner successfully initialized with model: VLLM\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "my_model = VLLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    dtype='float16',\n",
    ")\n",
    "\n",
    "tuner_with_custom_llm = PromptTuner(model=my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b9805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 13:18:43,972] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-07-09 13:18:44,730] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with meteor metric\n",
      "[2025-07-09 13:18:44,730] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 13:18:44,731] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-07-09 13:18:44,732] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-07-09 13:18:44,733] [INFO] [assistant.run] - No dataset provided\n",
      "[2025-07-09 13:18:44,733] [INFO] [assistant.run] - No target provided\n",
      "[2025-07-09 13:18:44,734] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.94it/s, est. speed input: 496.47 toks/s, out\n",
      "[2025-07-09 13:18:45,252] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-07-09 13:18:45,253] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An instructive prompt to explain the difference between a cat and a kitten [...] '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_with_custom_llm.run(start_prompt=\"Explain the difference between a cat and a kitten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf05ca",
   "metadata": {},
   "source": [
    "Or to change config of our default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0088a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 13:22:45,700] [INFO] [llm.init] - Initializing default model\n",
      "[2025-07-09 13:22:45,701] [DEBUG] [llm.init] - Updating default model params with langchain config: {'max_new_tokens': 1000, 'temperature': 0.5} and vllm_engine_config: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 13:22:47 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 07-09 13:22:48 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-09 13:22:56 config.py:549] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-09 13:22:56 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='t-tech/T-lite-it-1.0', speculative_config=None, tokenizer='t-tech/T-lite-it-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=t-tech/T-lite-it-1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 07-09 13:22:58 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-09 13:22:58 cuda.py:226] Using XFormers backend.\n",
      "INFO 07-09 13:22:59 model_runner.py:1110] Starting to load model t-tech/T-lite-it-1.0...\n",
      "INFO 07-09 13:22:59 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974f1219ae2c4fc4ab4f3f22ef939b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 13:24:51 model_runner.py:1115] Loading model weights took 14.2426 GB\n",
      "INFO 07-09 13:24:57 worker.py:267] Memory profiling takes 5.31 seconds\n",
      "INFO 07-09 13:24:57 worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.27GiB\n",
      "INFO 07-09 13:24:57 worker.py:267] model weights take 14.24GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 2.62GiB.\n",
      "INFO 07-09 13:24:57 executor_base.py:111] # cuda blocks: 3069, # CPU blocks: 4681\n",
      "INFO 07-09 13:24:57 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 1.50x\n",
      "INFO 07-09 13:25:05 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████████████████████| 35/35 [00:16<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 13:25:22 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.21 GiB\n",
      "INFO 07-09 13:25:22 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 30.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-07-09 13:25:22,233] [INFO] [assistant.__init__] - Validating the model: VLLM\n",
      "[2025-07-09 13:25:22,233] [INFO] [assistant.__init__] - PromptTuner successfully initialized with model: VLLM\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.language_model.llm import DefaultLLM\n",
    "\n",
    "changed_model = DefaultLLM.init(langchain_config={\n",
    "    'max_new_tokens': 1000,\n",
    "    \"temperature\": 0.5,\n",
    "})\n",
    "\n",
    "tuner_with_changed_llm = PromptTuner(model=changed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79abfa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 13:25:35,352] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /nfs/home/asitkina/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-07-09 13:25:37,042] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with meteor metric\n",
      "[2025-07-09 13:25:37,043] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 13:25:37,043] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-07-09 13:25:37,043] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-07-09 13:25:37,044] [INFO] [assistant.run] - No dataset provided\n",
      "[2025-07-09 13:25:37,044] [INFO] [assistant.run] - No target provided\n",
      "[2025-07-09 13:25:37,044] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 261.55 toks/s, out\n",
      "[2025-07-09 13:25:38,028] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-07-09 13:25:38,028] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Please provide a detailed comparison between a cat and a kitten, focusing on their physical characteristics, behaviors, and life stages.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_with_changed_llm.run(start_prompt=\"Explain the difference between a cat and a kitten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396516",
   "metadata": {},
   "source": [
    "You can access prompts and their metrics via tuner fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedc8a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prompt: Summarize this text\n",
      "Final prompt: Please provide a concise summary of the text, focusing on the main points and omitting any unnecessary details.\n",
      "Start prompt metric:  0.2889385547946554\n",
      "Final prompt metric:  0.30479966549868787\n"
     ]
    }
   ],
   "source": [
    "def print_results(tuner):\n",
    "    print(\"Start prompt:\", tuner.init_prompt)\n",
    "    print(\"Final prompt:\", tuner.final_prompt)\n",
    "    print(\"Start prompt metric: \", tuner.init_metric)\n",
    "    print(\"Final prompt metric: \", tuner.final_metric)\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1822",
   "metadata": {},
   "source": [
    "There are 3 currently implemented optimizers:\n",
    "- HyPE (optimizing with predefined system instruction)\n",
    "- DistillPrompt\n",
    "- ReflectivePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77cc66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 01:11:35,596] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-07-09 01:11:36,329] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with f1 metric\n",
      "[2025-07-09 01:11:36,330] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 01:11:36,330] [INFO] [assistant.run] - Method: hype, Task: classification\n",
      "[2025-07-09 01:11:36,331] [INFO] [assistant.run] - Metric: f1, Validation size: 0.25\n",
      "[2025-07-09 01:11:36,332] [INFO] [assistant.run] - Dataset: 100 samples\n",
      "[2025-07-09 01:11:36,332] [INFO] [assistant.run] - Target: 100 samples\n",
      "[2025-07-09 01:11:36,333] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 320.47 toks/s, out\n",
      "[2025-07-09 01:11:37,114] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-07-09 01:11:37,115] [INFO] [assistant.run] - Evaluating on given dataset for classification task...\n",
      "[2025-07-09 01:11:37,115] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 100 samples\n",
      "Processed prompts: 100%|█| 100/100 [01:48<00:00,  1.09s/it, est. speed input: 275.90 toks/s,\n",
      "[2025-07-09 01:13:25,746] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 100 samples\n",
      "Processed prompts: 100%|█| 100/100 [00:05<00:00, 17.36it/s, est. speed input: 5388.44 toks/s\n",
      "[2025-07-09 01:13:31,896] [INFO] [assistant.run] - Initial f1 score: 0.6364983164983165, final f1 score: 0.8899889988998899\n",
      "[2025-07-09 01:13:31,897] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prompt: Classify sentence sentiment\n",
      "Final prompt: Please provide a sentence and classify its sentiment as positive, negative, or neutral.\n",
      "Start prompt metric:  0.6364983164983165\n",
      "Final prompt metric:  0.8899889988998899\n"
     ]
    }
   ],
   "source": [
    "tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    method=\"hype\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"f1\"\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62164ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 14:09:40,871] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-07-09 14:09:41,621] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with f1 metric\n",
      "[2025-07-09 14:09:41,622] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 14:09:41,623] [INFO] [assistant.run] - Method: distill, Task: classification\n",
      "[2025-07-09 14:09:41,624] [INFO] [assistant.run] - Metric: f1, Validation size: 0.25\n",
      "[2025-07-09 14:09:41,624] [INFO] [assistant.run] - Dataset: 20 samples\n",
      "[2025-07-09 14:09:41,625] [INFO] [assistant.run] - Target: 20 samples\n",
      "[2025-07-09 14:09:41,637] [INFO] [distiller.distillation] - Starting DistillPrompt optimization...\n",
      "[2025-07-09 14:09:41,638] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:57<00:00,  7.84s/it, est. speed input: 6.04 toks/s, out\n",
      "  0%|                                                                 | 0/1 [00:00<?, ?it/s][2025-07-09 14:11:39,285] [INFO] [distiller.distillation] - Starting round 0\n",
      "Processed prompts: 100%|█| 4/4 [00:01<00:00,  3.97it/s, est. speed input: 444.53 toks/s, out\n",
      "[2025-07-09 14:11:40,298] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:00<00:00, 24.58it/s, est. speed input: 1532.09 toks/s, \n",
      "[2025-07-09 14:11:40,929] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:48<00:00,  7.26s/it, est. speed input: 9.55 toks/s, out\n",
      "[2025-07-09 14:13:29,798] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [02:01<00:00,  8.09s/it, est. speed input: 7.70 toks/s, out\n",
      "[2025-07-09 14:15:31,236] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [02:07<00:00,  8.50s/it, est. speed input: 7.80 toks/s, out\n",
      "Processed prompts: 100%|█| 4/4 [01:44<00:00, 26.05s/it, est. speed input: 9.62 toks/s, outpu\n",
      "[2025-07-09 14:19:22,990] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:00<00:00, 24.43it/s, est. speed input: 1522.98 toks/s, \n",
      "[2025-07-09 14:19:23,625] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:57<00:00,  7.83s/it, est. speed input: 31.06 toks/s, ou\n",
      "[2025-07-09 14:21:21,158] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [02:01<00:00,  8.11s/it, est. speed input: 7.69 toks/s, out\n",
      "[2025-07-09 14:23:22,755] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-09 14:25:24 scheduler.py:1754] Sequence group 457 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 15/15 [02:21<00:00,  9.42s/it, est. speed input: 7.04 toks/s, out\n",
      "Processed prompts: 100%|█| 4/4 [01:50<00:00, 27.51s/it, est. speed input: 5.82 toks/s, outpu\n",
      "[2025-07-09 14:27:34,202] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:00<00:00, 24.19it/s, est. speed input: 1508.09 toks/s, \n",
      "[2025-07-09 14:27:34,845] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:01<00:00,  8.11it/s, est. speed input: 813.79 toks/s, o\n",
      "[2025-07-09 14:27:36,716] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [02:01<00:00,  8.13s/it, est. speed input: 7.66 toks/s, out\n",
      "[2025-07-09 14:29:38,725] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [02:11<00:00,  8.76s/it, est. speed input: 7.57 toks/s, out\n",
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.80s/it, est. speed input: 122.00 toks/s, out\n",
      "[2025-07-09 14:31:51,927] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:01<00:00, 10.45it/s, est. speed input: 1069.30 toks/s, \n",
      "Processed prompts: 100%|█| 3/3 [00:07<00:00,  2.64s/it, est. speed input: 29.13 toks/s, outp\n",
      "[2025-07-09 14:32:01,317] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:00<00:00, 20.94it/s, est. speed input: 3713.82 toks/s, \n",
      "[2025-07-09 14:32:02,057] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [02:09<00:00,  8.61s/it, est. speed input: 20.37 toks/s, ou\n",
      "[2025-07-09 14:34:11,181] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:03<00:00,  4.58it/s, est. speed input: 1562.68 toks/s, \n",
      "[2025-07-09 14:34:14,486] [INFO] [distiller.distillation] - Best candidate score in round 0: 0.784688995215311\n",
      "100%|███████████████████████████████████████████████████████| 1/1 [22:35<00:00, 1355.20s/it]\n",
      "[2025-07-09 14:34:14,488] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 5 samples\n",
      "Processed prompts: 100%|█| 5/5 [01:55<00:00, 23.16s/it, est. speed input: 2.65 toks/s, outpu\n",
      "[2025-07-09 14:36:10,314] [INFO] [distiller.distillation] - Final best prompt score on validation: 0.5833333333333333\n",
      "[2025-07-09 14:36:10,315] [INFO] [distiller.distillation] - DistillPrompt optimization completed\n",
      "[2025-07-09 14:36:10,316] [INFO] [assistant.run] - Evaluating on given dataset for classification task...\n",
      "[2025-07-09 14:36:10,316] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 20 samples\n",
      "Processed prompts: 100%|█| 20/20 [02:11<00:00,  6.55s/it, est. speed input: 7.03 toks/s, out\n",
      "[2025-07-09 14:38:21,429] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 20 samples\n",
      "Processed prompts: 100%|█| 20/20 [03:34<00:00, 10.70s/it, est. speed input: 6.08 toks/s, out\n",
      "[2025-07-09 14:41:55,537] [INFO] [assistant.run] - Initial f1 score: 0.21031746031746032, final f1 score: 0.7916666666666667\n",
      "[2025-07-09 14:41:55,537] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prompt: Classify sentence sentiment\n",
      "Final prompt: Identify the sentiment of each sentence in the provided text. Indicate the sentiment as positive, negative, or neutral.\n",
      "Start prompt metric:  0.21031746031746032\n",
      "Final prompt metric:  0.7916666666666667\n"
     ]
    }
   ],
   "source": [
    "tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset[:20],\n",
    "    target=class_targets[:20],\n",
    "    method=\"distill\",\n",
    "    use_cache=False,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f677f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 14:41:55,676] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-07-09 14:41:59,027] [INFO] [evaluator.__init__] - Evaluator sucessfully initialized with f1 metric\n",
      "[2025-07-09 14:41:59,027] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-07-09 14:41:59,028] [INFO] [assistant.run] - Method: reflective, Task: classification\n",
      "[2025-07-09 14:41:59,028] [INFO] [assistant.run] - Metric: f1, Validation size: 0.25\n",
      "[2025-07-09 14:41:59,028] [INFO] [assistant.run] - Dataset: 20 samples\n",
      "[2025-07-09 14:41:59,029] [INFO] [assistant.run] - Target: 20 samples\n",
      "[2025-07-09 14:41:59,030] [INFO] [run.reflectiveprompt] - Starting ReflectivePrompt optimization...\n",
      "[2025-07-09 14:41:59,031] [INFO] [evoluter._init_pop] - Initializing population...\n",
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.77s/it, est. speed input: 44.66 toks/s, outp\n",
      "[2025-07-09 14:42:00,805] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-07-09 14:42:00,807] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:56<00:00,  7.76s/it, est. speed input: 6.48 toks/s, out\n",
      "[2025-07-09 14:43:57,296] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:59<00:00,  7.99s/it, est. speed input: 6.42 toks/s, out\n",
      "[2025-07-09 14:45:57,236] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:58<00:00,  7.90s/it, est. speed input: 6.50 toks/s, out\n",
      "[2025-07-09 14:47:55,708] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:59<00:00,  7.99s/it, est. speed input: 6.42 toks/s, out\n",
      "[2025-07-09 14:49:55,594] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [01:49<00:00,  7.30s/it, est. speed input: 7.31 toks/s, out\n",
      "Processed prompts: 100%|█| 4/4 [00:00<00:00,  6.58it/s, est. speed input: 978.26 toks/s, out\n",
      "Processed prompts: 100%|█| 4/4 [00:01<00:00,  2.42it/s, est. speed input: 270.56 toks/s, out\n",
      "[2025-07-09 14:51:47,391] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-07-09 14:51:47,392] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:03<00:00,  4.38it/s, est. speed input: 312.48 toks/s, o\n",
      "[2025-07-09 14:51:50,836] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:00<00:00, 18.15it/s, est. speed input: 1621.76 toks/s, \n",
      "[2025-07-09 14:51:51,683] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:03<00:00,  3.99it/s, est. speed input: 264.39 toks/s, o\n",
      "[2025-07-09 14:51:55,468] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:03<00:00,  3.98it/s, est. speed input: 264.02 toks/s, o\n",
      "[2025-07-09 14:51:59,297] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.8611111111111112\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 284.31 toks/s, out\n",
      "Processed prompts: 100%|█| 4/4 [00:01<00:00,  2.39it/s, est. speed input: 274.55 toks/s, out\n",
      "[2025-07-09 14:52:01,554] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-07-09 14:52:01,555] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:07<00:00,  2.14it/s, est. speed input: 204.18 toks/s, o\n",
      "[2025-07-09 14:52:08,580] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:07<00:00,  2.14it/s, est. speed input: 204.11 toks/s, o\n",
      "[2025-07-09 14:52:15,608] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:07<00:00,  2.14it/s, est. speed input: 204.11 toks/s, o\n",
      "[2025-07-09 14:52:22,636] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 15 samples\n",
      "Processed prompts: 100%|█| 15/15 [00:07<00:00,  2.14it/s, est. speed input: 204.11 toks/s, o\n",
      "[2025-07-09 14:52:29,664] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.8611111111111112\n",
      "[2025-07-09 14:52:29,664] [INFO] [evoluter._update_iter] - Iteration 0 finished...\n",
      "[2025-07-09 14:52:29,665] [INFO] [evoluter._update_iter] - Best score: 0.8611111111111112\n",
      "[2025-07-09 14:52:29,665] [INFO] [evoluter.evolution] - BEST SCORE: 0.8611111111111112\n",
      "[2025-07-09 14:52:29,665] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-07-09 14:52:29,666] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 5 samples\n",
      "Processed prompts: 100%|█| 5/5 [00:02<00:00,  1.93it/s, est. speed input: 118.42 toks/s, out\n",
      "[2025-07-09 14:52:32,275] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 5 samples\n",
      "Processed prompts: 100%|█| 5/5 [00:03<00:00,  1.66it/s, est. speed input: 110.32 toks/s, out\n",
      "[2025-07-09 14:52:35,302] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 5 samples\n",
      "Processed prompts: 100%|█| 5/5 [00:03<00:00,  1.48it/s, est. speed input: 133.95 toks/s, out\n",
      "[2025-07-09 14:52:38,694] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 5 samples\n",
      "Processed prompts: 100%|█| 5/5 [00:02<00:00,  1.93it/s, est. speed input: 118.54 toks/s, out\n",
      "[2025-07-09 14:52:41,301] [INFO] [evoluter._update_elitist] - Iteration 1\n",
      "                Elitist score: 1.0\n",
      "[2025-07-09 14:52:41,301] [INFO] [run.reflectiveprompt] - ReflectivePrompt optimization completed\n",
      "[2025-07-09 14:52:41,302] [INFO] [assistant.run] - Evaluating on given dataset for classification task...\n",
      "[2025-07-09 14:52:41,302] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 20 samples\n",
      "Processed prompts: 100%|█| 20/20 [02:11<00:00,  6.56s/it, est. speed input: 7.03 toks/s, out\n",
      "[2025-07-09 14:54:52,477] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 20 samples\n",
      "Processed prompts: 100%|█| 20/20 [00:04<00:00,  4.93it/s, est. speed input: 345.90 toks/s, o\n",
      "[2025-07-09 14:54:56,554] [INFO] [assistant.run] - Initial f1 score: 0.21031746031746032, final f1 score: 0.8465473145780051\n",
      "[2025-07-09 14:54:56,555] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prompt: Classify sentence sentiment\n",
      "Final prompt: Classify the sentiment of the sentence into one of the following categories: positive, negative, or neutral. Provide a detailed explanation for your classification.\n",
      "Start prompt metric:  0.21031746031746032\n",
      "Final prompt metric:  0.8465473145780051\n"
     ]
    }
   ],
   "source": [
    "tuner.run(\n",
    "    start_prompt=class_start_prompt,\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset[:20],\n",
    "    target=class_targets[:20],\n",
    "    method=\"reflective\",\n",
    "    problem_description=\"sentiment classification\",\n",
    "    use_cache=False,\n",
    "    population_size=4,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ebe58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
