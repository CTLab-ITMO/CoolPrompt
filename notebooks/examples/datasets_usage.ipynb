{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets using example\n",
    "\n",
    "### This notebook will show an example of using our custmom dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/vzhuravlev/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7f3e6ba7f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# This code enables using of \"src.data\" imports in vs code (when you're launching it directly from notebooks directory)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from src.data.classification import SST2Dataset\n",
    "from src.data.generation import SamsumDataset\n",
    "from src.data.multi_task import BBHDataset\n",
    "from src.evaluation.evaluator import TextClassificationEvaluator, GenerationEvaluator\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loading model weights\n",
    "qconf = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_name = \"AnatoliiPotapov/T-lite-instruct-0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=qconf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing dataset\n",
    "\n",
    "sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821\n"
     ]
    }
   ],
   "source": [
    "# data length\n",
    "\n",
    "print(len(sst2_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please perform Sentiment Classification task.\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\n<INPUT>\\n\\nResponse:\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get your prompt like that\n",
    "\n",
    "sst2_ds.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99]) torch.Size([99]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# getting first data sample\n",
    "\n",
    "input_ids, attention_mask, label = next(iter(sst2_ds))\n",
    "print(input_ids.shape, attention_mask.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/nfs/home/vzhuravlev/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "# terminators were taken from hf model page (t-lite 0.1)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# generating answer for our sample \n",
    "# unsqueeze(0) - to make to necessary shape (when using DataLoader it'll be done automatically)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids.unsqueeze(0),\n",
    "    attention_mask = attention_mask.unsqueeze(0),\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=terminators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please perform Sentiment Classification task.\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\nno movement, no yuks, not much of anything.\\n\\nResponse:\\n<ans>negative</ans>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding the answer\n",
    "\n",
    "ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response:\\n<ans>negative</ans>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ans.find(\"Response:\\n\")\n",
    "ans[pos:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 1/15 [00:04<01:04,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 2/15 [00:09<00:59,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 3/15 [00:13<00:55,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 4/15 [00:18<00:50,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 5/15 [00:22<00:45,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 6/15 [00:27<00:41,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 7/15 [00:32<00:36,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 8/15 [00:36<00:32,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 9/15 [00:41<00:27,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 10/15 [00:45<00:22,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 11/15 [00:50<00:18,  4.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 12/15 [00:55<00:13,  4.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 13/15 [00:59<00:09,  4.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 14/15 [01:04<00:04,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 15/15 [01:05<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "model_generate_params = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"eos_token_id\": terminators\n",
    "}\n",
    "\n",
    "evaluator = TextClassificationEvaluator()\n",
    "metrics = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=sst2_ds,\n",
    "    batch_size=128,\n",
    "    model_generate_args = model_generate_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.5751775831182536, 'accuracy': 0.8594179022515102}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use your prompt instead of basic one\n",
    "\n",
    "my_prompt = \"You will be given movie reviews. Determine if the given review has negative or positive sentiment.\"\n",
    "\n",
    "prompted_sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=my_prompt,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/nfs/home/vzhuravlev/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 1/15 [00:05<01:10,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 2/15 [00:10<01:05,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 3/15 [00:15<01:03,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 4/15 [00:20<00:57,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 5/15 [00:25<00:51,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 6/15 [00:31<00:47,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 7/15 [00:36<00:42,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 8/15 [00:41<00:36,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 9/15 [00:46<00:31,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 10/15 [00:52<00:25,  5.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 11/15 [00:57<00:20,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 12/15 [01:02<00:15,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 13/15 [01:08<00:10,  5.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 14/15 [01:13<00:05,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 15/15 [01:14<00:00,  4.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6361686997429025, 'accuracy': 0.9505766062602965}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=prompted_sst2_ds,\n",
    "    batch_size=128,\n",
    "    model_generate_args = model_generate_params\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use generation dataset\n",
    "\n",
    "sds = SamsumDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n"
     ]
    }
   ],
   "source": [
    "print(len(sds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([712]) torch.Size([712]) torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, label = next(iter(sds))\n",
    "print(input_ids.shape, attention_mask.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.02k/7.02k [00:00<00:00, 16.5MB/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /nfs/home/vzhuravlev/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /nfs/home/vzhuravlev/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /nfs/home/vzhuravlev/nltk_data...\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/nfs/home/vzhuravlev/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 1/26 [00:42<17:45, 42.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 2/26 [01:22<16:19, 40.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 3/26 [01:59<15:06, 39.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 4/26 [02:44<15:13, 41.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 5/26 [03:25<14:25, 41.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 6/26 [04:04<13:29, 40.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 7/26 [04:46<13:01, 41.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 8/26 [05:28<12:23, 41.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 9/26 [06:02<11:04, 39.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 10/26 [06:54<11:25, 42.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 11/26 [07:36<10:42, 42.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 12/26 [08:16<09:46, 41.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 13/26 [09:03<09:24, 43.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 14/26 [09:43<08:29, 42.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 15/26 [10:20<07:27, 40.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 16/26 [11:10<07:15, 43.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 17/26 [12:19<07:40, 51.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 18/26 [12:54<06:10, 46.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 19/26 [13:39<05:20, 45.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 20/26 [14:29<04:42, 47.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 21/26 [15:16<03:55, 47.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 22/26 [15:56<03:00, 45.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 23/26 [16:35<02:09, 43.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 24/26 [17:28<01:32, 46.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 25/26 [18:16<00:46, 46.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 26/26 [18:39<00:00, 43.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.08601431946357946,\n",
       " 'rouge': np.float64(0.3092796788507406),\n",
       " 'meteor': np.float64(0.44558376449318987)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generate_params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"eos_token_id\": terminators\n",
    "}\n",
    "\n",
    "evaluator = GenerationEvaluator()\n",
    "metrics = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=sds,\n",
    "    batch_size=32,\n",
    "    model_generate_args = model_generate_params\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/nfs/home/vzhuravlev/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Multi-task dataset example\n",
    "\n",
    "ds = BBHDataset(\n",
    "    tokenizer,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "ds = ds.task('boolean_expressions')\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model_generate_params = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"eos_token_id\": terminators\n",
    "}\n",
    "\n",
    "evaluator = TextClassificationEvaluator()\n",
    "metrics = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=ds,\n",
    "    batch_size=128,\n",
    "    model_generate_args = model_generate_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.4848915748311411, 'accuracy': 0.6349206349206349}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
