{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T12:40:05.513807300Z",
     "start_time": "2025-03-07T12:39:57.970128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f08f04401d0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# This code enables using of \"src.data\" imports in vs code (when you're launching it directly from notebooks directory)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c1ff3c8ad154714bbe18220305cec01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading model weights\n",
    "qconf = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_name = \"AnatoliiPotapov/T-lite-instruct-0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=qconf,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T12:40:14.678148800Z",
     "start_time": "2025-03-07T12:40:05.512897100Z"
    }
   },
   "id": "5415d576e316a348"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'Please perform Sentiment Classification task\\n\\nAnswer using the label from [negative, positive].\\nGenerate the final answer bracketed with <ans> and </ans>.\\n\\nThe input:\\n<INPUT>\\n\\nResponse:\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.classification import SST2Dataset\n",
    "\n",
    "test_sst2_ds = SST2Dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=\"../../data/sst-2/test-00000-of-00001.parquet\",\n",
    "    config_path=\"../../data/\",\n",
    "    device=model.device\n",
    ")\n",
    "test_sst2_ds.prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T12:40:14.934921200Z",
     "start_time": "2025-03-07T12:40:14.695045200Z"
    }
   },
   "id": "58f752ae1ac148ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/nfs/home/edyagin/.virtualenvs/prompt_optimization/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 1/29 [00:02<01:20,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 2/29 [00:05<01:13,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 3/29 [00:08<01:09,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 4/29 [00:10<01:05,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 5/29 [00:13<01:02,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 6/29 [00:15<01:00,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 7/29 [00:18<00:57,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 8/29 [00:21<00:54,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 9/29 [00:23<00:51,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 10/29 [00:26<00:49,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 11/29 [00:28<00:46,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 12/29 [00:31<00:44,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 13/29 [00:34<00:41,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 14/29 [00:36<00:38,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 15/29 [00:39<00:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 16/29 [00:41<00:33,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 17/29 [00:44<00:31,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 18/29 [00:47<00:28,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 19/29 [00:49<00:26,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 20/29 [00:52<00:23,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 21/29 [00:54<00:20,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 22/29 [00:57<00:18,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 23/29 [01:00<00:15,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 24/29 [01:02<00:13,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 25/29 [01:05<00:10,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 26/29 [01:07<00:07,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 27/29 [01:10<00:05,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 28/29 [01:13<00:02,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 29/29 [01:14<00:00,  2.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.5565279674105245"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evaluation.evaluator import TextClassificationEvaluator\n",
    "\n",
    "# terminators were taken from hf model page (t-lite 0.1)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model_generate_params = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"eos_token_id\": terminators\n",
    "}\n",
    "\n",
    "evaluator = TextClassificationEvaluator()\n",
    "macro_f1 = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=test_sst2_ds,\n",
    "    batch_size=64,\n",
    "    model_generate_args = model_generate_params\n",
    ")\n",
    "macro_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T12:44:04.814428600Z",
     "start_time": "2025-03-07T12:42:49.159594Z"
    }
   },
   "id": "8dedbdaa35ed884"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T12:44:04.831064400Z",
     "start_time": "2025-03-07T12:44:04.794712900Z"
    }
   },
   "id": "436bcebd2084a9f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e68d89ce9ddf80d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'INSTRUCTION:\\nSummarize the following text\\n\\nINPUT:\\n<INPUT>\\n\\nRESPONSE:\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.generation import SamsumDataset\n",
    "\n",
    "gen_ds = SamsumDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=\"../../data/samsum/test-00000-of-00001.parquet\",\n",
    "    config_path='../../data',\n",
    "    device=model.device\n",
    ")\n",
    "gen_ds.prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T12:44:05.171842100Z",
     "start_time": "2025-03-07T12:44:04.830053700Z"
    }
   },
   "id": "8b1ba085730529a0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /nfs/home/edyagin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /nfs/home/edyagin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /nfs/home/edyagin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/nfs/home/edyagin/.virtualenvs/prompt_optimization/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 1/26 [00:51<21:21, 51.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 2/26 [01:35<18:45, 46.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 3/26 [02:07<15:27, 40.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 4/26 [03:03<17:02, 46.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 5/26 [03:43<15:24, 44.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 6/26 [04:22<14:10, 42.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 7/26 [05:03<13:14, 41.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 8/26 [05:43<12:23, 41.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 9/26 [06:21<11:27, 40.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 10/26 [07:03<10:52, 40.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 11/26 [07:37<09:39, 38.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 12/26 [08:16<09:02, 38.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 13/26 [08:51<08:09, 37.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 14/26 [09:32<07:43, 38.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 15/26 [10:12<07:10, 39.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 16/26 [10:56<06:44, 40.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 17/26 [11:50<06:41, 44.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 18/26 [12:19<05:18, 39.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 19/26 [12:54<04:30, 38.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 20/26 [13:46<04:15, 42.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 21/26 [14:46<03:58, 47.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 22/26 [15:32<03:08, 47.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 23/26 [16:14<02:16, 45.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 24/26 [17:08<01:36, 48.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 25/26 [17:53<00:47, 47.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 26/26 [18:19<00:00, 42.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'bleu': 0.0983179127962664,\n 'rouge': 0.3313492109838392,\n 'meteor': 0.4484997598087172}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evaluation.evaluator import GenerationEvaluator\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "model_generate_params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"eos_token_id\": terminators\n",
    "}\n",
    "\n",
    "evaluator = GenerationEvaluator()\n",
    "metrics = evaluator.evaluate(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    eval_ds=gen_ds,\n",
    "    batch_size=32,\n",
    "    model_generate_args = model_generate_params\n",
    ")\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T13:02:34.149031700Z",
     "start_time": "2025-03-07T12:44:05.186754600Z"
    }
   },
   "id": "c6424b0a6b1a0e4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
