{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e61688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-08 17:16:46,941] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-11-08 17:16:46,942] [INFO] [assistant.__init__] - Validating the system model\n",
      "[2025-11-08 17:16:46,942] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-08 17:16:46,942] [INFO] [assistant.__init__] - Validating the system model\n",
      "[2025-11-08 17:16:46,942] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-08 17:16:50,461] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-08 17:16:50,462] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-08 17:16:50,461] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-08 17:16:50,462] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-08 17:16:58,111] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 17:16:58,112] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 17:16:58,114] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-08 17:16:58,115] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-08 17:16:58,118] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-08 17:16:58,119] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 17:16:58,111] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 17:16:58,112] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 17:16:58,114] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-08 17:16:58,115] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-08 17:16:58,118] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-08 17:16:58,119] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 17:16:58,120] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-08 17:16:58,120] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-08 17:17:02,674] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 17:17:02,675] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Summarize the text provided.[/PROMPT_END]\n",
      "[2025-11-08 17:17:02,675] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 17:17:02,674] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 17:17:02,675] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Summarize the text provided.[/PROMPT_END]\n",
      "[2025-11-08 17:17:02,675] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 17:17:10,509] [DEBUG] [assistant.run] - Final prompt:\n",
      "[PROMPT_START]Summarize the text provided.[/PROMPT_END]\n",
      "[2025-11-08 17:17:10,511] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 17:17:10,512] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:17:10,514] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-08 17:17:10,509] [DEBUG] [assistant.run] - Final prompt:\n",
      "[PROMPT_START]Summarize the text provided.[/PROMPT_END]\n",
      "[2025-11-08 17:17:10,511] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 17:17:10,512] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:17:10,514] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-08 17:17:33,804] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:17:33,805] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[PROMPT_START]Summarize the text provided.[/PROMPT_END]\n",
      "[2025-11-08 17:17:33,804] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:17:33,805] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[PROMPT_START]Summarize the text provided.[/PROMPT_END]\n",
      "[2025-11-08 17:17:55,296] [INFO] [assistant.run] - Initial geval score: 0.5, final geval score: 0.5599999999999999\n",
      "[2025-11-08 17:17:55,297] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 17:17:55,296] [INFO] [assistant.run] - Initial geval score: 0.5, final geval score: 0.5599999999999999\n",
      "[2025-11-08 17:17:55,297] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 17:18:09,137] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 17:18:09,137] [INFO] [assistant.run] - The original prompt was open-ended and generic. We improved it by adding specific context, such as including a prompt tag 'Summarize the text provided', ensuring that the response will only contain a summary of the given text. This ensures the output is directly relevant to what the user needs without being too broad or detailed. Key takeaway: Adding clear instructions can prevent unnecessary information and make sure the answer is tailored to the specific request.\n",
      "[2025-11-08 17:18:09,138] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-08 17:18:09,139] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-08 17:18:09,137] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 17:18:09,137] [INFO] [assistant.run] - The original prompt was open-ended and generic. We improved it by adding specific context, such as including a prompt tag 'Summarize the text provided', ensuring that the response will only contain a summary of the given text. This ensures the output is directly relevant to what the user needs without being too broad or detailed. Key takeaway: Adding clear instructions can prevent unnecessary information and make sure the answer is tailored to the specific request.\n",
      "[2025-11-08 17:18:09,138] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-08 17:18:09,139] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-08 17:18:11,364] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 17:18:11,365] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 17:18:11,367] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-08 17:18:11,368] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-08 17:18:11,370] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-08 17:18:11,371] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 17:18:11,372] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-08 17:18:11,364] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 17:18:11,365] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 17:18:11,367] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-08 17:18:11,368] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-08 17:18:11,370] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-08 17:18:11,371] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 17:18:11,372] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-08 17:18:14,598] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 17:18:14,598] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[BEGIN_OF_TEXT]Summarize the text [END_OF_TEXT]\n",
      "[2025-11-08 17:18:14,599] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 17:18:14,598] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 17:18:14,598] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[BEGIN_OF_TEXT]Summarize the text [END_OF_TEXT]\n",
      "[2025-11-08 17:18:14,599] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 17:18:21,174] [DEBUG] [assistant.run] - Final prompt:\n",
      "[BEGIN_OF_TEXT]Summarize the text [END_OF_TEXT]\n",
      "[2025-11-08 17:18:21,175] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 17:18:21,176] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:18:21,177] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-08 17:18:21,174] [DEBUG] [assistant.run] - Final prompt:\n",
      "[BEGIN_OF_TEXT]Summarize the text [END_OF_TEXT]\n",
      "[2025-11-08 17:18:21,175] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 17:18:21,176] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:18:21,177] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-08 17:18:50,217] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:18:50,218] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[BEGIN_OF_TEXT]Summarize the text [END_OF_TEXT]\n",
      "[2025-11-08 17:18:50,217] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:18:50,218] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[BEGIN_OF_TEXT]Summarize the text [END_OF_TEXT]\n",
      "[2025-11-08 17:19:22,485] [INFO] [assistant.run] - Initial geval score: 0.4, final geval score: 0.5900000000000001\n",
      "[2025-11-08 17:19:22,486] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 17:19:22,485] [INFO] [assistant.run] - Initial geval score: 0.4, final geval score: 0.5900000000000001\n",
      "[2025-11-08 17:19:22,486] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 17:19:37,816] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 17:19:37,817] [INFO] [assistant.run] - The original prompt was general and vague, focusing on a summary without specific details. We improved it by adding context clues (BEGIN_OF_TEXT) and END_OF_TEXT markers to clearly delineate the text being summarized. Additionally, we clarified that the output should be concise (short), which is important for effective summarization. The enhanced version now guides the user in providing a brief yet comprehensive summary while adhering to specific guidelines.\n",
      "[2025-11-08 17:19:37,818] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-08 17:19:37,819] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-08 17:19:37,816] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 17:19:37,817] [INFO] [assistant.run] - The original prompt was general and vague, focusing on a summary without specific details. We improved it by adding context clues (BEGIN_OF_TEXT) and END_OF_TEXT markers to clearly delineate the text being summarized. Additionally, we clarified that the output should be concise (short), which is important for effective summarization. The enhanced version now guides the user in providing a brief yet comprehensive summary while adhering to specific guidelines.\n",
      "[2025-11-08 17:19:37,818] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-08 17:19:37,819] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-08 17:19:40,840] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 17:19:40,841] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 17:19:40,842] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-08 17:19:40,843] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-08 17:19:40,840] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 17:19:40,841] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 17:19:40,842] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-08 17:19:40,843] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-08 17:19:40,844] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-08 17:19:40,845] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 17:19:40,846] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "\n",
      "[2025-11-08 17:19:40,844] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-08 17:19:40,845] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 17:19:40,846] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "\n",
      "[2025-11-08 17:19:46,219] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 17:19:46,219] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[Prompt_START]Could you please provide a detailed explanation of the project's challenges, including its significance, current solutions or gaps (if any), relevant constraints, and objectives? Additionally, include information about key stakeholders involved and potential outcomes if these issues are not addressed. This comprehensive overview will help in creating effective problem statements for documentation, presentations, or other project-related communications.[Prompt_END]\n",
      "[2025-11-08 17:19:46,220] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 17:19:46,219] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 17:19:46,219] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[Prompt_START]Could you please provide a detailed explanation of the project's challenges, including its significance, current solutions or gaps (if any), relevant constraints, and objectives? Additionally, include information about key stakeholders involved and potential outcomes if these issues are not addressed. This comprehensive overview will help in creating effective problem statements for documentation, presentations, or other project-related communications.[Prompt_END]\n",
      "[2025-11-08 17:19:46,220] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 17:19:53,810] [DEBUG] [assistant.run] - Final prompt:\n",
      "[Prompt_START]Could you please provide a detailed explanation of the project's challenges, including its significance, current solutions or gaps (if any), relevant constraints, and objectives? Additionally, include information about key stakeholders involved and potential outcomes if these issues are not addressed. This comprehensive overview will help in creating effective problem statements for documentation, presentations, or other project-related communications.[Prompt_END]\n",
      "[2025-11-08 17:19:53,813] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 17:19:53,815] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:19:53,817] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "\n",
      "[2025-11-08 17:19:53,810] [DEBUG] [assistant.run] - Final prompt:\n",
      "[Prompt_START]Could you please provide a detailed explanation of the project's challenges, including its significance, current solutions or gaps (if any), relevant constraints, and objectives? Additionally, include information about key stakeholders involved and potential outcomes if these issues are not addressed. This comprehensive overview will help in creating effective problem statements for documentation, presentations, or other project-related communications.[Prompt_END]\n",
      "[2025-11-08 17:19:53,813] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 17:19:53,815] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:19:53,817] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "\n",
      "[2025-11-08 17:20:18,935] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:20:18,936] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[Prompt_START]Could you please provide a detailed explanation of the project's challenges, including its significance, current solutions or gaps (if any), relevant constraints, and objectives? Additionally, include information about key stakeholders involved and potential outcomes if these issues are not addressed. This comprehensive overview will help in creating effective problem statements for documentation, presentations, or other project-related communications.[Prompt_END]\n",
      "[2025-11-08 17:20:18,935] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-08 17:20:18,936] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[Prompt_START]Could you please provide a detailed explanation of the project's challenges, including its significance, current solutions or gaps (if any), relevant constraints, and objectives? Additionally, include information about key stakeholders involved and potential outcomes if these issues are not addressed. This comprehensive overview will help in creating effective problem statements for documentation, presentations, or other project-related communications.[Prompt_END]\n",
      "[2025-11-08 17:21:19,015] [INFO] [assistant.run] - Initial geval score: 0.1, final geval score: 0.29\n",
      "[2025-11-08 17:21:19,015] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 17:21:19,015] [INFO] [assistant.run] - Initial geval score: 0.1, final geval score: 0.29\n",
      "[2025-11-08 17:21:19,015] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 17:21:34,421] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 17:21:34,422] [INFO] [assistant.run] - Your initial prompt was too general without specific details. We improved it by providing clear instructions on the project's challenges, including its significance, solutions or gaps, constraints, and objectives. This ensures a focused response that meets your requirements better. The addition of key stakeholders and potential outcomes highlights the importance of addressing these issues effectively. Key takeaways: To get precise information, specify the project details thoroughly.\n",
      "[2025-11-08 17:21:34,421] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 17:21:34,422] [INFO] [assistant.run] - Your initial prompt was too general without specific details. We improved it by providing clear instructions on the project's challenges, including its significance, solutions or gaps, constraints, and objectives. This ensures a focused response that meets your requirements better. The addition of key stakeholders and potential outcomes highlights the importance of addressing these issues effectively. Key takeaways: To get precise information, specify the project details thoroughly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from coolprompt import PromptTuner\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "target_model = ChatOllama(model=\"qwen2.5:1.5b-instruct\")\n",
    "system_model = ChatOllama(model=\"qwen2.5:3b-instruct\")\n",
    "\n",
    "tuner = PromptTuner(target_model=target_model, system_model=system_model)\n",
    "\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:40]\n",
    "targets = samsum[\"train\"][\"summary\"][:40]\n",
    "\n",
    "result = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_criteria=\"relevance\",\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "result2 = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_criteria=[\"relevance\"],\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "custom = {\n",
    "    \"creativity\": \"\"\"You will be given a response to a question.\n",
    "    Rate the creativity on a scale from 1 to {metric_ceil}.\n",
    "    Return ONLY a single number.\n",
    "\n",
    "    Source: {request}\n",
    "    Response: {response}\n",
    "\n",
    "    Creativity score (number only):\"\"\"\n",
    "}\n",
    "\n",
    "result3 = tuner.run(\n",
    "    start_prompt=\"\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_criteria=[\"creativity\"],\n",
    "    geval_custom_templates=custom,\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacd5061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-08 12:16:00,756] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-11-08 12:16:02,816] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-11-08 12:16:05,406] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-08 12:16:05,407] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-08 12:16:05,407] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-11-08 12:16:05,408] [INFO] [assistant.run] - Dataset: 30 samples\n",
      "[2025-11-08 12:16:05,408] [INFO] [assistant.run] - Target: 30 samples\n",
      "[2025-11-08 12:16:05,409] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-08 12:16:05,409] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-08 12:16:08,273] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-08 12:16:08,273] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Summarize the following text:[PROMPT_END]\n",
      "[2025-11-08 12:16:08,274] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-08 12:16:14,780] [DEBUG] [assistant.run] - Final prompt:\n",
      "Summarize the following text:\n",
      "[2025-11-08 12:16:14,781] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-08 12:16:14,781] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 8 samples\n",
      "[2025-11-08 12:16:14,782] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-08 12:16:22,985] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 8 samples\n",
      "[2025-11-08 12:16:22,986] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the following text:\n",
      "[2025-11-08 12:16:35,486] [INFO] [assistant.run] - Initial meteor score: 0.11831869594904601, final meteor score: 0.06284947560519769\n",
      "[2025-11-08 12:16:35,487] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-08 12:16:46,181] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-08 12:16:46,182] [INFO] [assistant.run] - Your initial prompt was too general. We improved it by specifying that you need a summary of the given text, which reduces its scope and prevents an overly long response. This makes the output more focused and relevant to your needs. The new structure also aligns with best practices for summarizing texts, ensuring that your answer is precise and concise.\n"
     ]
    }
   ],
   "source": [
    "result2 = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"meteor\",\n",
    "    generate_num_samples=20,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c4270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-studio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
