{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31505f1",
   "metadata": {},
   "source": [
    "# Cost Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b12cd9",
   "metadata": {},
   "source": [
    "В данный момент один общий трекер для всех оптимизаторов, поэтому если хочется получить статистику по для каждого запуска отдельно, то нужно сбрасывать её"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5f65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asd480/anaconda3/envs/coolprompt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "from coolprompt.language_model import create_chat_model\n",
    "from coolprompt.assistant import PromptTuner\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584c1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-25 21:57:14,067] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-12-25 21:57:14,067] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n"
     ]
    }
   ],
   "source": [
    "model = create_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "tuner = PromptTuner(target_model=model, system_model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f05f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:10]\n",
    "targets = samsum[\"train\"][\"summary\"][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1eaad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stats: {'total_calls': 0, 'total_tokens': 0, 'prompt_tokens': 0, 'completion_tokens': 0, 'total_cost': 0.0, 'invoke_calls': 0, 'batch_calls': 0, 'batch_items': 0}\n"
     ]
    }
   ],
   "source": [
    "tuner.reset_stats()\n",
    "\n",
    "initial_stats = tuner.get_stats()\n",
    "print(\"Initial stats:\", initial_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e5147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-25 21:57:19,972] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/asd480/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-12-25 21:57:21,786] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-12-25 21:57:25,552] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-12-25 21:57:25,553] [INFO] [assistant.run] - Method: reflective, Task: generation\n",
      "[2025-12-25 21:57:25,553] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-12-25 21:57:25,554] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-12-25 21:57:25,554] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-12-25 21:57:25,554] [INFO] [run.reflectiveprompt] - Starting ReflectivePrompt optimization...\n",
      "[2025-12-25 21:57:25,555] [INFO] [evoluter._init_pop] - Initializing population...\n",
      "[2025-12-25 21:57:28,463] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:57:28,463] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:57:35,787] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:57:39,237] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:57:43,700] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:57:47,831] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:57:54,683] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:57:54,684] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:57:59,413] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:02,120] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:05,175] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:08,331] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:11,852] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.328685409745586\n",
      "[2025-12-25 21:58:14,953] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:58:14,954] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:20,161] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:24,514] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:28,562] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:31,101] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:34,009] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.4137945302418582\n",
      "[2025-12-25 21:58:34,010] [INFO] [evoluter._update_iter] - Iteration 0 finished...\n",
      "[2025-12-25 21:58:34,010] [INFO] [evoluter._update_iter] - Best score: 0.4137945302418582\n",
      "[2025-12-25 21:58:35,816] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:58:35,816] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:39,952] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:42,017] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:45,300] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:48,053] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:50,611] [INFO] [evoluter._update_elitist] - Iteration 1\n",
      "                Elitist score: 0.44624651570423934\n",
      "[2025-12-25 21:58:52,953] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:58:52,953] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:55,810] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:58:58,088] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:01,468] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:05,035] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:07,252] [INFO] [evoluter._update_elitist] - Iteration 1\n",
      "                Elitist score: 0.44624651570423934\n",
      "[2025-12-25 21:59:07,253] [INFO] [evoluter._update_iter] - Iteration 1 finished...\n",
      "[2025-12-25 21:59:07,253] [INFO] [evoluter._update_iter] - Best score: 0.44624651570423934\n",
      "[2025-12-25 21:59:09,642] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:59:09,643] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:13,344] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:16,490] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:21,143] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:25,747] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:31,064] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:59:31,065] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:34,034] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:37,470] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:40,343] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:42,661] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-12-25 21:59:45,781] [INFO] [evoluter._update_elitist] - Iteration 2\n",
      "                Elitist score: 0.44624651570423934\n",
      "[2025-12-25 21:59:45,782] [INFO] [evoluter._update_iter] - Iteration 2 finished...\n",
      "[2025-12-25 21:59:45,783] [INFO] [evoluter._update_iter] - Best score: 0.44624651570423934\n",
      "[2025-12-25 21:59:45,788] [INFO] [evoluter.evolution] - BEST TRAIN SCORE: 0.44624651570423934\n",
      "[2025-12-25 21:59:45,789] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-12-25 21:59:45,790] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-25 21:59:47,353] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-25 21:59:50,257] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-25 21:59:52,788] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-25 21:59:54,597] [INFO] [evoluter.evolution] - BEST VALIDATION SCORE: 0.47182940874881335\n",
      "[2025-12-25 21:59:54,598] [INFO] [run.reflectiveprompt] - ReflectivePrompt optimization completed\n",
      "[2025-12-25 21:59:54,598] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-12-25 21:59:55,908] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-12-25 21:59:55,909] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-25 21:59:57,686] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-12-25 21:59:59,553] [INFO] [assistant.run] - Initial meteor score: 0.4184736883608024, final meteor score: 0.48472614472434783\n",
      "[2025-12-25 21:59:59,554] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-12-25 22:00:03,642] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-12-25 22:00:03,643] [INFO] [assistant.run] - Your initial prompt, \"Summarize the dialogue,\" was quite vague and did not provide specific guidance on what aspects to focus on. In the final prompt, we improved clarity by specifying that you want to \"extract and summarize the key points and conclusions\" from the dialogue. This change encourages a more targeted response. Additionally, the emphasis on \"clarity and coherence\" directs the model to prioritize a well-structured summary, while the instruction to \"omit extraneous details\" helps to streamline the information, making it more digestible. This approach enhances the effectiveness of the output by ensuring it is both relevant and useful. A key lesson here is to always provide direction on the focus and desired characteristics of the summary to obtain a more precise and useful response.\n"
     ]
    }
   ],
   "source": [
    "final_prompt = tuner.run(\n",
    "    start_prompt=\"Summarize the dialogue\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"reflective\",\n",
    "    metric=\"meteor\",\n",
    "    population_size=5,\n",
    "    num_epochs=3,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85444145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost tracking\n",
      "total calls 59\n",
      "invoke calls 5\n",
      "batch calls 54\n",
      "batch items 312\n",
      "\n",
      "total tokens 99,890\n",
      "prompt tokens 73,723\n",
      "completion tokens 26,167\n",
      "\n",
      "total cost: $0.026759\n"
     ]
    }
   ],
   "source": [
    "stats = tuner.get_stats()\n",
    "\n",
    "if stats:\n",
    "    print(\"cost tracking\")\n",
    "    print(f\"total calls {stats['total_calls']}\")\n",
    "    print(f\"invoke calls {stats['invoke_calls']}\")\n",
    "    print(f\"batch calls {stats['batch_calls']}\")\n",
    "    print(f\"batch items {stats['batch_items']}\")\n",
    "    print()\n",
    "    print(f\"total tokens {stats['total_tokens']:,}\")\n",
    "    print(f\"prompt tokens {stats['prompt_tokens']:,}\")\n",
    "    print(f\"completion tokens {stats['completion_tokens']:,}\")\n",
    "    print()\n",
    "    print(f\"total cost: ${stats['total_cost']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7306a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial metric: 0.4185\n",
      "Final metric: 0.4847\n",
      "\n",
      "Initial prompt: Summarize the dialogue\n",
      "\n",
      "Final prompt: Please extract and summarize the key points and conclusions from the dialogue, focusing on clarity and coherence while omitting extraneous details to facilitate easier understanding and future reference.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial metric: {tuner.init_metric:.4f}\")\n",
    "print(f\"Final metric: {tuner.final_metric:.4f}\")\n",
    "print(f\"\\nInitial prompt: {tuner.init_prompt}\")\n",
    "print(f\"\\nFinal prompt: {tuner.final_prompt}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
