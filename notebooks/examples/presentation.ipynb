{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fdd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685e808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(tuner):\n",
    "    print(\"Final prompt:\", tuner.final_prompt)\n",
    "    print(\"Start prompt metric: \", tuner.init_metric)\n",
    "    print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb6438",
   "metadata": {},
   "source": [
    "# PromptTuner\n",
    "\n",
    "Точка входа - ассистент\n",
    "\n",
    "Он позволяет оптимизировать имеющийся промпт под определенную задачу\n",
    "\n",
    "Задачу можно задавать датасетом, подаваемым в функцию run, но некоторые оптимизаторы могут работать и без него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395e5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 17:01:58,154\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-30 17:02:00 config.py:1865] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-30 17:02:09 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 06-30 17:02:09 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='t-tech/T-lite-it-1.0', speculative_config=None, tokenizer='t-tech/T-lite-it-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=t-tech/T-lite-it-1.0, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 06-30 17:02:11 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-30 17:02:11 selector.py:144] Using XFormers backend.\n",
      "INFO 06-30 17:02:12 model_runner.py:1072] Starting to load model t-tech/T-lite-it-1.0...\n",
      "INFO 06-30 17:02:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20db19e0115a4d72b99b617d85fba796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 17:02:18 model_runner.py:1077] Loading model weights took 14.2426 GB\n",
      "INFO 06-30 17:02:24 worker.py:232] Memory profiling results: total_gpu_memory=31.74GiB initial_memory_usage=14.66GiB peak_torch_memory=18.59GiB memory_usage_post_profile=14.69GiB non_torch_memory=0.44GiB kv_cache_size=9.53GiB gpu_memory_utilization=0.90\n",
      "INFO 06-30 17:02:24 gpu_executor.py:113] # GPU blocks: 11154, # CPU blocks: 4681\n",
      "INFO 06-30 17:02:24 gpu_executor.py:117] Maximum concurrency for 32768 tokens per request: 5.45x\n",
      "INFO 06-30 17:02:28 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-30 17:02:28 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-30 17:02:55 model_runner.py:1518] Graph capturing finished in 27 secs, took 0.80 GiB\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "tuner = PromptTuner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ceab957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 174.64 toks/s, output: 39.82 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write an essay about autumn, focusing on the changes in nature, the emotions it evokes, and the cultural significance of the season. Use descriptive language and provide examples from literature, art, or personal experiences to support your points.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Запуск без датасета\n",
    "tuner.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a1649c",
   "metadata": {},
   "source": [
    "Ассистент в режиме оптимизации под датасет может решать одну из двух задач: классификацию или генерацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2095694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 349.26 toks/s, output: 37.87 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [01:42<00:00,  1.02s/it, est. speed input: 293.55 toks/s, output: 46.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:06<00:00, 16.66it/s, est. speed input: 5173.62 toks/s, output: 133.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Please provide a sentence and classify its sentiment as positive, negative, or neutral.\n",
      "Start prompt metric:  0.6364983164983165\n",
      "Final prompt metric:  0.8899889988998899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Классификация\n",
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "class_dataset = sst2['train']['sentence'][:100]\n",
    "class_targets = sst2['train']['label'][:100]\n",
    "\n",
    "tuner.run(\n",
    "    start_prompt=\"Classify sentence sentiment\",\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"f1\"\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6790e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sitkina-\n",
      "[nltk_data]     alena/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sitkina-\n",
      "[nltk_data]     alena/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sitkina-\n",
      "[nltk_data]     alena/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 326.70 toks/s, output: 37.90 toks/s]\n",
      "Processed prompts: 100%|██████████| 200/200 [00:18<00:00, 10.90it/s, est. speed input: 4161.02 toks/s, output: 272.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 200/200 [00:25<00:00,  7.82it/s, est. speed input: 3077.44 toks/s, output: 252.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Please provide a concise summary of the text, focusing on the main points and key details.\n",
      "Start prompt metric:  0.25465833610131716\n",
      "Final prompt metric:  0.3145034036877276\n"
     ]
    }
   ],
   "source": [
    "# Генерация\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "gen_dataset = samsum['train']['dialogue'][:200]\n",
    "gen_targets = samsum['train']['summary'][:200]\n",
    "\n",
    "tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"meteor\"\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f32b7",
   "metadata": {},
   "source": [
    "## Language model\n",
    "\n",
    "При инициализации юзер может подать уже имеющуюся LLM-ку, \n",
    "\n",
    "или инициализировать асситента без аргументов, и будет использована дефолтная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "# olama + gpt\n",
    "my_model = VLLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    dtype='float16',\n",
    ")\n",
    "\n",
    "tuner_with_custom_llm = PromptTuner(model=my_model)\n",
    "tuner_with_custom_llm.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bf28b",
   "metadata": {},
   "source": [
    "Также можно изменять конфиг дефолтной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coolprompt.language_model.llm import DefaultLLM\n",
    "\n",
    "changed_model = DefaultLLM.init(langchain_config={\n",
    "    'max_new_tokens': 1000,\n",
    "    \"temperature\": 0.0,\n",
    "})\n",
    "\n",
    "tuner_with_changed_llm = PromptTuner(model=changed_model)\n",
    "tuner_with_changed_llm.run(start_prompt=\"Write an essay about autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f8079",
   "metadata": {},
   "source": [
    "## Оптимизатор\n",
    "\n",
    "Фреймворк поддерживает несколько различных алгоритмов оптимизации промптов\n",
    "\n",
    "- HyPE\n",
    "- DistillPrompt\n",
    "- ReflectivePrompt\n",
    "\n",
    "Выбор алгоритма производится в параметрах метода run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a7661",
   "metadata": {},
   "source": [
    "### HyPE\n",
    "Мы уже видели его работу до этого - используя заранее заданную системную инструкцию просит модель оптимизировать поданный промпт. оптимизация происходит в одну итерацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12428e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 100.93 toks/s, output: 40.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:05<00:00, 17.27it/s, est. speed input: 5188.66 toks/s, output: 138.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [00:07<00:00, 13.72it/s, est. speed input: 5274.07 toks/s, output: 109.74 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Perform Sentiment Classification task by following these steps: 1. Identify the text to be classified. 2. Determine the sentiment categories (e.g., positive, negative, neutral). 3. Analyze the text for sentiment indicators (e.g., positive words, negative words, emoticons). 4. Assign the text to the appropriate sentiment category based on the analysis. 5. Evaluate the accuracy of the classification using a test dataset.\n",
      "Start prompt metric:  0.93\n",
      "Final prompt metric:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task=\"classification\",\n",
    "    method=\"hype\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"accuracy\"\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7468c",
   "metadata": {},
   "source": [
    "### DistillPrompt\n",
    "\n",
    "Метод автопромптинга, основанный на последовательной дистилляции и агрегации знаний LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d1a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-30 17:25:20,276] - Starting DistillPrompt optimization...\n",
      "Processed prompts: 100%|██████████| 75/75 [03:23<00:00,  2.71s/it, est. speed input: 17.87 toks/s, output: 698.34 toks/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][2025-06-30 17:28:43,857] - Starting round 0\n",
      "Processed prompts: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s, est. speed input: 146.88 toks/s, output: 77.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [04:07<00:00,  3.31s/it, est. speed input: 15.58 toks/s, output: 676.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [02:47<00:00,  2.24s/it, est. speed input: 35.98 toks/s, output: 550.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [00:03<00:00, 22.87it/s, est. speed input: 2161.06 toks/s, output: 223.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [01:39<00:00,  1.32s/it, est. speed input: 118.49 toks/s, output: 46.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [01:44<00:00, 26.18s/it, est. speed input: 10.68 toks/s, output: 77.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-30 17:43:36 scheduler.py:1481] Sequence group 1905 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 75/75 [04:28<00:00,  3.59s/it, est. speed input: 18.26 toks/s, output: 673.49 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [14:22<00:00, 11.50s/it, est. speed input: 351.61 toks/s, output: 334.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [00:07<00:00,  9.57it/s, est. speed input: 1037.94 toks/s, output: 154.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-30 18:01:58 scheduler.py:1481] Sequence group 2124 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 75/75 [05:39<00:00,  4.53s/it, est. speed input: 21.32 toks/s, output: 716.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it, est. speed input: 437.40 toks/s, output: 56.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [01:58<00:00,  1.59s/it, est. speed input: 40.68 toks/s, output: 281.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [07:52<00:00,  6.30s/it, est. speed input: 64.36 toks/s, output: 634.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [00:07<00:00,  9.64it/s, est. speed input: 1045.37 toks/s, output: 155.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [01:37<00:00,  1.30s/it, est. speed input: 47.91 toks/s, output: 47.84 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it, est. speed input: 371.78 toks/s, output: 37.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [00:05<00:00, 13.45it/s, est. speed input: 1257.42 toks/s, output: 203.89 toks/s]\n",
      "Processed prompts: 100%|██████████| 3/3 [00:03<00:00,  1.11s/it, est. speed input: 61.25 toks/s, output: 96.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [02:09<00:00,  1.73s/it, est. speed input: 99.68 toks/s, output: 345.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [02:19<00:00,  1.86s/it, est. speed input: 75.05 toks/s, output: 466.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 75/75 [02:52<00:00,  2.30s/it, est. speed input: 58.79 toks/s, output: 629.13 toks/s]\n",
      "[2025-06-30 18:23:10,480] - Best candidate score in round 0: 0.6440351510774046\n",
      "[2025-06-30 18:23:10,481] - Best candidate prompt: Classify text sentiment as positive, negative, or neutral using the train dataset to ensure accurate identification.\n",
      "100%|██████████| 1/1 [54:26<00:00, 3266.63s/it]\n",
      "Processed prompts: 100%|██████████| 25/25 [01:43<00:00,  4.15s/it, est. speed input: 14.07 toks/s, output: 78.94 toks/s]\n",
      "[2025-06-30 18:24:54,300] - Final best prompt score on validation: 0.5714285714285714\n",
      "[2025-06-30 18:24:54,301] - Final best prompt: Classify text sentiment as positive, negative, or neutral using the train dataset to ensure accurate identification.\n",
      "Processed prompts: 100%|██████████| 100/100 [04:50<00:00,  2.90s/it, est. speed input: 16.36 toks/s, output: 731.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [01:54<00:00,  1.14s/it, est. speed input: 53.88 toks/s, output: 217.19 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Classify text sentiment as positive, negative, or neutral using the train dataset to ensure accurate identification.\n",
      "Start prompt metric:  0.3254835996635828\n",
      "Final prompt metric:  0.5608629721532948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task='classification',\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    method='distill',\n",
    "    use_cache=True,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dad7ef",
   "metadata": {},
   "source": [
    "### ReflectivePrompt\n",
    "\n",
    "Метод автопромптинга на основе эволюционных алгоритмов, использующий подход рефлексивной эволюции для более точного и расширенного поиска оптимальных промптов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58afa175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-30 18:32:34,777] - Initializing population...\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it, est. speed input: 49.37 toks/s, output: 40.11 toks/s]\n",
      "[2025-06-30 18:32:36,406] - Evaluating population...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Evaluator.evaluate() missing 1 required positional argument: 'template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPerform Sentiment Classification task.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflective\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment classification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m print_results(tuner)\n",
      "File \u001b[0;32m~/CoolPrompt/coolprompt/assistant.py:159\u001b[0m, in \u001b[0;36mPromptTuner.run\u001b[0;34m(self, start_prompt, task, dataset, target, method, metric, problem_description, validation_size, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain dataset is not defined for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReflectivePrompt optimization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m     dataset_split \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    157\u001b[0m         dataset, target, test_size\u001b[38;5;241m=\u001b[39mvalidation_size\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m     final_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mreflectiveprompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/CoolPrompt/coolprompt/optimizer/reflective_prompt/run.py:63\u001b[0m, in \u001b[0;36mreflectiveprompt\u001b[0;34m(model, dataset_split, evaluator, task, problem_description, initial_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     48\u001b[0m evoluter \u001b[38;5;241m=\u001b[39m ReflectiveEvoluter(\n\u001b[1;32m     49\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     50\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mevoluter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_prompt\n",
      "File \u001b[0;32m~/CoolPrompt/coolprompt/optimizer/reflective_prompt/evoluter.py:577\u001b[0m, in \u001b[0;36mReflectiveEvoluter.evolution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevolution\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provides evolution operation.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    Selection -> Short-term reflection -> Long-term reflection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m        str: best evoluted prompt\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m     population \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_pop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_population(\n\u001b[1;32m    579\u001b[0m         population,\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_output_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_population.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs:\n",
      "File \u001b[0;32m~/CoolPrompt/coolprompt/optimizer/reflective_prompt/evoluter.py:218\u001b[0m, in \u001b[0;36mReflectiveEvoluter._init_pop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m prompts \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(answer)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    215\u001b[0m initial_population \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    216\u001b[0m     Prompt(prompt, origin\u001b[38;5;241m=\u001b[39mPromptOrigin\u001b[38;5;241m.\u001b[39mAPE) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts\n\u001b[1;32m    217\u001b[0m ]\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_population\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m initial_population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reranking(initial_population)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m initial_population\n",
      "File \u001b[0;32m~/CoolPrompt/coolprompt/optimizer/reflective_prompt/evoluter.py:181\u001b[0m, in \u001b[0;36mReflectiveEvoluter._evaluation\u001b[0;34m(self, population, split)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating population...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m population:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CoolPrompt/coolprompt/optimizer/reflective_prompt/evoluter.py:158\u001b[0m, in \u001b[0;36mReflectiveEvoluter._evaluate\u001b[0;34m(self, prompt, split)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     dataset, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_dataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_targets\n\u001b[0;32m--> 158\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m prompt\u001b[38;5;241m.\u001b[39mset_score(score)\n",
      "\u001b[0;31mTypeError\u001b[0m: Evaluator.evaluate() missing 1 required positional argument: 'template'"
     ]
    }
   ],
   "source": [
    "tuner.run(\n",
    "    start_prompt=\"Perform Sentiment Classification task.\",\n",
    "    task='classification',\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    method='reflective',\n",
    "    problem_description='sentiment classification',\n",
    "    use_cache=False,\n",
    "    population_size=4,\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print_results(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8d567",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Если вы укажете датасет для оптимизации, фреймворк позволяет измерять метрики, получаемые при использовании определенного промпта\n",
    "\n",
    "На данный момент поддерживаются\n",
    "- accuracy и f1 для классификации\n",
    "- meteor, rouge и bleu для генерации\n",
    "\n",
    "Метрики находятся в публичных полях ассистента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start prompt metric: \", tuner.init_metric)\n",
    "print(\"Final prompt metric: \", tuner.final_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
