{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c0b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968dd629-4357-4929-9af9-ccf2a7023a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 15:27:50 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-29 15:27:50 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 05-29 15:27:50 [cuda.py:409] Detected different devices in the system: NVIDIA GeForce RTX 2080 Ti, NVIDIA TITAN RTX. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n",
      "WARNING 05-29 15:27:53 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 05-29 15:28:02 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-29 15:28:02 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 05-29 15:28:02 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='t-tech/T-lite-it-1.0', speculative_config=None, tokenizer='t-tech/T-lite-it-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=t-tech/T-lite-it-1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-29 15:28:04 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 05-29 15:28:04 [cuda.py:289] Using XFormers backend.\n",
      "INFO 05-29 15:28:05 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-29 15:28:05 [model_runner.py:1108] Starting to load model t-tech/T-lite-it-1.0...\n",
      "INFO 05-29 15:28:05 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e3133bb9b949dc9346ec36c57c6e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 15:28:14 [loader.py:458] Loading weights took 8.38 seconds\n",
      "INFO 05-29 15:28:14 [model_runner.py:1140] Model loading took 14.2427 GiB and 9.304607 seconds\n",
      "INFO 05-29 15:28:20 [worker.py:287] Memory profiling takes 5.20 seconds\n",
      "INFO 05-29 15:28:20 [worker.py:287] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.27GiB\n",
      "INFO 05-29 15:28:20 [worker.py:287] model weights take 14.24GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 2.62GiB.\n",
      "INFO 05-29 15:28:20 [executor_base.py:112] # cuda blocks: 3068, # CPU blocks: 4681\n",
      "INFO 05-29 15:28:20 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 1.50x\n",
      "INFO 05-29 15:28:25 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1975b0dfe2450698628103094dcbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 15:28:43 [model_runner.py:1592] Graph capturing finished in 18 secs, took 0.21 GiB\n",
      "INFO 05-29 15:28:43 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 28.93 seconds\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "tuner = PromptTuner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d53dd2f-8596-4ca3-afe8-212c043680ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"sst2\")\n",
    "n_instances = 100\n",
    "dataset = sst2['train']['sentence'][:n_instances]\n",
    "targets = sst2['train']['label'][:n_instances]\n",
    "\n",
    "start_prompt = '''\n",
    "Please perform Sentiment Classification task.\n",
    "\n",
    "Answer using the label from [0, 1], where 0 stands for negative, 1 stands for positive.\n",
    "Generate the final answer bracketed with <ans> and </ans>.\n",
    "\n",
    "The input:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792dae57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ed99bf19c14beeada55cb0058e7253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                      | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e500ba73ba54a5eab5ef772bebbcb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                    | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 15:52:58 [scheduler.py:1768] Sequence group 288 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f251085d55a426f94d150137ac7270b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                    | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-29 16:00:06 [scheduler.py:1768] Sequence group 388 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    }
   ],
   "source": [
    "final_prompt = tuner.run(start_prompt=start_prompt,\n",
    "                         dataset=dataset,\n",
    "                         target=targets,\n",
    "                         task='classification',\n",
    "                         metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa563efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Perform Sentiment Classification task. Use the label [0, 1] where 0 represents negative and 1 represents positive. Generate the final answer in the format <ans> [0/1] </ans>. \n",
      "INITIAL METRIC: 0.41\n",
      "FINAL METRIC: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"PROMPT:\", final_prompt)\n",
    "print(\"INITIAL METRIC:\", tuner.init_metric)\n",
    "print(\"FINAL METRIC:\", tuner.final_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122665b-fc42-4ce3-af3c-5e5b5d0f6e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
