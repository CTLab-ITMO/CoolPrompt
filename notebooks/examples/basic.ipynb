{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a12ce0-63ac-48d3-8b6c-307862598f75",
   "metadata": {},
   "source": [
    "# Basic tutorial of CoolPrompt\n",
    "*Actual for 1.1.0+ version*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0929c-beb9-48be-9d39-387801bcf0df",
   "metadata": {},
   "source": [
    "## Quick Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9089fb7f-a327-4c3f-87a5-0cccc5396586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "\n",
    "prompt_tuner = PromptTuner()\n",
    "\n",
    "prompt_tuner.run('Write an essay about autumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d489c9-65a1-47d1-bb71-d6a03bf614ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert writer and seasonal observer tasked with composing a rich, well-structured, and vividly descriptive essay on the theme of autumn. Your goal is to create a coherent, engaging, and insightful piece that captures the essence of the season through sensory detail, natural observation, and cultural reflection.\n",
      "\n",
      "Begin by describing the key physical and environmental characteristics of autumn: the transformation of foliage, the gradual cooling of temperatures, the crispness of the air, and the quiet rhythm of nature as leaves fall. Use specific, evocative language to convey sight, sound, smell, and touch—such as the rustle of dry leaves, the golden light filtering through trees, or the earthy scent of damp soil after rain.\n",
      "\n",
      "Integrate cultural and human elements of autumn, including harvest festivals (e.g., Thanksgiving, Halloween, harvest fairs), traditional foods (e.g., apples, pumpkins, squash), and seasonal rituals that reflect humanity’s connection to nature. Reflect on the emotional tone of autumn—its melancholy, nostalgia, abundance, and transition—offering thoughtful insights into how people perceive and respond to the season’s changes.\n",
      "\n",
      "Structure your essay with a clear narrative arc: open with a vivid scene or sensory moment, develop through observations and reflections, and conclude with a broader philosophical or emotional takeaway about the value of seasonal change, impermanence, and balance in life.\n",
      "\n",
      "Write in clear, natural prose that is accessible and immersive. Avoid generic statements; instead, ground your writing in specific details and personal or universal reflections. Ensure the essay is comprehensive, thoughtful, and inspiring—offering readers not just a description of autumn, but a deeper appreciation for its beauty and significance in both the natural world and human experience.\n",
      "\n",
      "Maintain a consistent tone that is reflective yet lively, poetic yet grounded. Do not include any code, technical jargon, or irrelevant content.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854dd0b-0e49-46f9-b881-367e446afa05",
   "metadata": {},
   "source": [
    "### What happened step-by-step\n",
    "1. We initialized a CoolPrompt Tuner with default large language model: qwen3-4B-instruct\n",
    "2. We input a start prompt that should be modified\n",
    "3. CoolPrompt auto-detected preferred task and metric for prompt optimization and evaluation\n",
    "4. CoolPrompt generated automatically the dataset with target labels and split into train and test samples\n",
    "5. The default prompt optimizer HyPE suggested the optimized prompt\n",
    "6. CoolPrompt outputs the results:\n",
    "   - Metric scores of optimization efficiency\n",
    "   - The final prompt\n",
    "   - The interpretation of prompt optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42992843-51c8-4d39-9b3b-98935d6764e8",
   "metadata": {},
   "source": [
    "### That is a simple approach how to optimize prompts. Let's try to configure CoolPrompt Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe16ef-f038-44f8-a080-a07816042060",
   "metadata": {},
   "source": [
    "## Setup a CoolPrompt Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec42bb-400c-4705-922c-661e67761f36",
   "metadata": {},
   "source": [
    "### 1. LLM Choice\n",
    "The framework is a model-agnostic, you can use proprietary, open-source or custom LLMs with different interfaces via LangChain compatibility. \n",
    "\n",
    "#### Check [the full list of provider interfaces](https://python.langchain.com/docs/integrations/llms/).\n",
    "\n",
    "Default LLM in CoolPrompt is Qwen/Qwen3-4B-Instruct-2507 and it a same can be defined as:\n",
    "\n",
    "```python\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 4000,\n",
    "        \"temperature\": 0.01,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "    }\n",
    ")\n",
    "target_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=target_model)\n",
    "```\n",
    "\n",
    "\n",
    "You can use other popular interfaces as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22194f-1ae6-4ebd-8df5-5adc9419fb8d",
   "metadata": {},
   "source": [
    "#### Ollama\n",
    "For rapid experiments with low resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5481eb-d2f8-46fb-97e9-a857443f5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before the run command serve the model with ollama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "my_model = OllamaLLM(\n",
    "    model=\"qwen2.5-coder:32b\"\n",
    ")\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288eb4dc-5335-4d36-a36d-c5353ad59093",
   "metadata": {},
   "source": [
    "#### VLLM\n",
    "As a production ready solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c87892-bdfc-472c-82b3-d850036187d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "my_model = VLLM(\n",
    "    model=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    trust_remote_code=True,\n",
    "    dtype='bfloat16',\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91b7b6-227f-43b6-b390-16954fc0b729",
   "metadata": {},
   "source": [
    "#### ChatOpenAI\n",
    "For OpenAI and OpenAI compatible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88cbcf-e003-4f0c-ac15-6f52fb5e8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "my_model = ChatOpenAI(\n",
    "    model=\"paste model_name\",\n",
    "    base_url=\"paste base_url\",\n",
    "    openai_api_key=\"paste api_key\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1419b38-296b-4a62-b943-5034bb5c22e6",
   "metadata": {},
   "source": [
    "### 2. System Model\n",
    "Argument `system_model` defines a core in a **PromptAssistant** module.\n",
    "\n",
    "**PromptAssistant** is responsible for:\n",
    "- Definition automatically a task type and a metric for the specific start prompt.\n",
    "- Generates a syntetic dataset and target labels (when the real data is not provided).\n",
    "- Provides an optimization feedback.\n",
    "\n",
    "By default a `system_model` is a defined the same as a `target_model`. It could be set with a different llm in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63f20e3e-45a6-4943-896b-52ca891bcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "target_model = VLLM(\n",
    "    model=\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    ")\n",
    "system_model = VLLM(\n",
    "    model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(\n",
    "    target_model=target_model,\n",
    "    system_model=system_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e308fa-c9fd-46b3-bfd2-8d2db922a86b",
   "metadata": {},
   "source": [
    "#### IMPORTANT NOTE: `system_model` needs to be a confident instructional llm that can generate a structual output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445763b-db76-445f-99b5-48c7fe558455",
   "metadata": {},
   "source": [
    "### 3. Task and metrics\n",
    "CoolPrompt supports 2 task types and a set of metrics: \n",
    "- `classification` - a common classification\n",
    "    - `accuracy`\n",
    "    - `f1` (f1-macro)\n",
    "- `generation` - a general new text generation\n",
    "    - `bleu`\n",
    "    - `rouge`\n",
    "    - `meteor`\n",
    "    - `bertscore`\n",
    "    - `geval` (experimental, one metric: *textual accuracy*)\n",
    "\n",
    "For each run you can define as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efad17-c57a-4fd9-b5c4-bb8911b47898",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Classify a sentence sentiment\",\n",
    "    task=\"classification\",\n",
    "    metric=\"f1\",\n",
    ")\n",
    "\n",
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    metric=\"rouge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c49d5-5dcb-4c3e-93ec-228672efea11",
   "metadata": {},
   "source": [
    "### 4. Setup a dataset configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b86bf",
   "metadata": {},
   "source": [
    "#### 4.1 Input a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e1654ba-7185-40fb-a827-c707ca4cb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:13:30,157] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-10-12 12:13:30,158] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "my_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=\"key\",\n",
    "    temperature=0,\n",
    "    max_tokens=3500,\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6df954db-0077-499a-a6c4-4e937aa26de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:12:49,368] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-10-12 12:12:50,423] [INFO] [evaluator.__init__] - Evaluator successfully initialized with f1 metric\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:12:51,334] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:12:51,335] [INFO] [assistant.run] - Method: hype, Task: classification\n",
      "[2025-10-12 12:12:51,336] [INFO] [assistant.run] - Metric: f1, Validation size: 0.25\n",
      "[2025-10-12 12:12:51,337] [INFO] [assistant.run] - Dataset: 30 samples\n",
      "[2025-10-12 12:12:51,338] [INFO] [assistant.run] - Target: 30 samples\n",
      "[2025-10-12 12:12:51,338] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-12 12:12:52,655] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-12 12:12:52,656] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:12:53,423] [INFO] [assistant.run] - Evaluating on given dataset for classification task...\n",
      "[2025-10-12 12:12:53,424] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 8 samples\n",
      "[2025-10-12 12:12:54,042] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 8 samples\n",
      "[2025-10-12 12:12:54,825] [INFO] [assistant.run] - Initial f1 score: 0.2727272727272727, final f1 score: 0.75\n",
      "[2025-10-12 12:12:54,826] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:12:57,434] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:12:57,435] [INFO] [assistant.run] - Your original prompt was vague and lacked specificity. We refined it by specifying the task to train a machine learning model for sentiment classification, focusing on positive or negative sentiments. By mentioning the need for a labeled dataset and emphasizing the goal of accurate prediction for new sentences, the prompt becomes more actionable and goal-oriented. This clarity helps the AI understand the task better and generate more relevant responses. Key advice: Always provide clear task instructions, dataset requirements, and the desired outcome to guide the AI effectively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train a machine learning model to classify the sentiment of sentences as either positive or negative. Use a dataset of labeled sentences where each sentence is associated with its sentiment label. Aim to create a classifier that can accurately predict the sentiment of new, unseen sentences based on the patterns and features in the training data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "class_dataset = sst2['train']['sentence'][:30]\n",
    "class_targets = sst2['train']['label'][:30]\n",
    "\n",
    "prompt_tuner.run(\n",
    "    start_prompt=\"Classify sentence sentiment positive or negative\",\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"f1\"\n",
    ")\n",
    "\n",
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "623382a1-f3e7-4b2f-90aa-e5f15c44b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:11:45,024] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:11:46,060] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:11:47,011] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:11:47,012] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-10-12 12:11:47,013] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:11:47,014] [INFO] [assistant.run] - Dataset: 30 samples\n",
      "[2025-10-12 12:11:47,015] [INFO] [assistant.run] - Target: 30 samples\n",
      "[2025-10-12 12:11:47,015] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-12 12:11:47,624] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-12 12:11:47,624] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:11:48,583] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:11:48,583] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 8 samples\n",
      "[2025-10-12 12:11:49,781] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 8 samples\n",
      "[2025-10-12 12:11:50,728] [INFO] [assistant.run] - Initial meteor score: 0.3321594171032163, final meteor score: 0.3886049104693431\n",
      "[2025-10-12 12:11:50,729] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:11:53,192] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:11:53,192] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by providing a clear directive to summarize a text by extracting crucial details and presenting them concisely. This specificity helps the AI focus on the core task of summarization. Additionally, we included guidance on how to achieve the summary, ensuring a more targeted and effective response. Key advice: Always aim for clarity, specificity, and guidance on the desired output format to optimize the AI's performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given a text, create a concise summary by extracting the most important information and presenting it in a condensed form.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "gen_dataset = samsum['train']['dialogue'][:30]\n",
    "gen_targets = samsum['train']['summary'][:30]\n",
    "\n",
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    ")\n",
    "\n",
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760366e5-f58d-4f99-8064-41d456489d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afaa6afc",
   "metadata": {},
   "source": [
    "#### 4.2 Define a split strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6dd49",
   "metadata": {},
   "source": [
    "`validation_size` - a ratio for train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fe92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"bertscore\",\n",
    "    validation_size=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df29c42",
   "metadata": {},
   "source": [
    "`train_as_test` - assign a test set as a train, `validation_size` will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692913ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"bertscore\",\n",
    "    train_as_test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5f720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3abca5d",
   "metadata": {},
   "source": [
    "### 5. Choosing an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9544c",
   "metadata": {},
   "source": [
    "#### HyPE\n",
    "\n",
    "Provides the prompt refactoring workflow: the initial prompt is injected into special predetermined by our researches query template. \n",
    "\n",
    "This optimizer requires only one query to the LLM, so it surely can be used as a fast and simple tool to make your prompt better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56451584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:01:33,328] [INFO] [detector.generate] - Detecting the task by query\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:01:33,849] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-12 12:01:33,850] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:01:34,882] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-12 12:01:34,883] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:01:35,845] [INFO] [generator.generate] - Generated problem description: The task is to create a system that can automatically generate a concise summary of a given text. This system should be able to analyze the content of the text and extract the most important information to present a brief and informative summary to the user.\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:01:40,477] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:01:40,478] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-10-12 12:01:40,479] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:01:40,480] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-12 12:01:40,480] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-12 12:01:40,481] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-12 12:01:41,277] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-12 12:01:41,277] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:01:42,219] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:01:42,220] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:01:45,968] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:01:46,933] [INFO] [assistant.run] - Initial meteor score: 0.2852323515034904, final meteor score: 0.3452339044721156\n",
      "[2025-10-12 12:01:46,934] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:01:48,854] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:01:48,855] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by adding a clear directive to summarize a text and extract the most important information. This helps the AI focus on the key details and deliver a concise yet informative summary. By emphasizing the need for brevity, the final prompt ensures that the output is not overly long. Remember, providing clear instructions on the task and desired outcome is crucial for generating accurate and relevant responses.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given a text, create a concise summary by extracting the most important information. Ensure the summary is brief yet informative for the user.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    method='hype'\n",
    ")\n",
    "\n",
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aee2da",
   "metadata": {},
   "source": [
    "#### ReflectivePrompt\n",
    "\n",
    "This method is based on the idea of Reflective Evolution and text-based gradients. It implements short-term and long-term reflections to provide some clarifications and make crossover and mutation operations more precise and effective.\n",
    "\n",
    "ReflectivePrompt has 2 arguments:\n",
    "- population_size (default: 10)\n",
    "- num_epochs (default: 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b8b3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:02:56,733] [INFO] [detector.generate] - Detecting the task by query\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:02:57,194] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-12 12:02:57,194] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:02:58,202] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-12 12:02:58,203] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:02:59,371] [INFO] [generator.generate] - Generated problem description: The task is to create a system that can automatically generate a concise summary of a given text. This system should be able to extract the most important information from the text and present it in a condensed form, making it easier for users to quickly grasp the main points of the original text.\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:03:03,336] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:03:03,337] [INFO] [assistant.run] - Method: reflective, Task: generation\n",
      "[2025-10-12 12:03:03,337] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:03:03,338] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-12 12:03:03,338] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-12 12:03:03,339] [INFO] [run.reflectiveprompt] - Starting ReflectivePrompt optimization...\n",
      "[2025-10-12 12:03:03,339] [INFO] [evoluter._init_pop] - Initializing population...\n",
      "[2025-10-12 12:03:04,408] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:04,409] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:05,124] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:06,537] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:07,382] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:08,355] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:09,291] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:10,319] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:11,168] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:12,157] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:13,414] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:15,917] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:15,918] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:16,655] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:17,592] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:18,382] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:19,526] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:20,201] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:20,839] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:21,543] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:22,352] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:22,892] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:23,572] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.543967441814112\n",
      "[2025-10-12 12:03:25,057] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:25,058] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:25,698] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:26,327] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:26,946] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:27,616] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:28,310] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:28,956] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:29,592] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:30,232] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:31,286] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:32,011] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.563435528218003\n",
      "[2025-10-12 12:03:32,012] [INFO] [evoluter._update_iter] - Iteration 0 finished...\n",
      "[2025-10-12 12:03:32,013] [INFO] [evoluter._update_iter] - Best score: 0.563435528218003\n",
      "[2025-10-12 12:03:33,757] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:33,758] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:34,367] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:35,023] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:35,994] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:36,953] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:37,687] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:38,381] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:39,071] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:39,771] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:40,762] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:43,250] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:43,250] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:44,096] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:45,083] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:45,772] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:46,587] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:47,431] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:48,573] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:49,871] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:50,652] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:51,552] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:52,627] [INFO] [evoluter._update_elitist] - Iteration 1\n",
      "                Elitist score: 0.563435528218003\n",
      "[2025-10-12 12:03:52,629] [INFO] [evoluter._update_iter] - Iteration 1 finished...\n",
      "[2025-10-12 12:03:52,629] [INFO] [evoluter._update_iter] - Best score: 0.563435528218003\n",
      "[2025-10-12 12:03:52,645] [INFO] [evoluter.evolution] - BEST TRAIN SCORE: 0.563435528218003\n",
      "[2025-10-12 12:03:52,645] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:52,646] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:53,620] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:54,310] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:55,023] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:55,678] [INFO] [evoluter.evolution] - BEST VALIDATION SCORE: 0.6041594951233505\n",
      "[2025-10-12 12:03:55,679] [INFO] [run.reflectiveprompt] - ReflectivePrompt optimization completed\n",
      "[2025-10-12 12:03:55,680] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:03:56,588] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:03:56,588] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:57,426] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:58,036] [INFO] [assistant.run] - Initial meteor score: 0.14212962962962963, final meteor score: 0.4555904715543269\n",
      "[2025-10-12 12:03:58,037] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:04:00,551] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:04:00,551] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by providing a clear objective: developing an automatic text summarization system. This specificity helps guide the AI model towards a focused task. Additionally, we included details about what the summary should contain - main points and key details - which sets clear expectations for the output. By emphasizing the need for a concise and informative summary, the final prompt ensures that the AI-generated summary will be both brief and informative. Key advice: Always define the task clearly, specify the desired content, and emphasize the importance of conciseness in the prompt.\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    method='reflective',\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16a66d",
   "metadata": {},
   "source": [
    "#### DistillPrompt\n",
    "\n",
    "This method is based on different prompt-transformation methods that will use LLM to effectively examine the search area and find the best prompt variations for given task.\n",
    "\n",
    "DistillPrompt has 1 argument:\n",
    "- num_epochs (default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b5f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:08:01,668] [INFO] [detector.generate] - Detecting the task by query\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:08:02,166] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-12 12:08:02,166] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:08:03,729] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-12 12:08:03,730] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:08:04,511] [INFO] [generator.generate] - Generated problem description: The task is to create a system that can automatically generate a concise summary of a given text. This system should be able to analyze the content of the text and extract the most important information to provide a brief and informative summary.\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:08:07,378] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:08:07,379] [INFO] [assistant.run] - Method: distill, Task: generation\n",
      "[2025-10-12 12:08:07,380] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:08:07,380] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-12 12:08:07,381] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-12 12:08:07,383] [INFO] [distiller.distillation] - Starting DistillPrompt optimization...\n",
      "[2025-10-12 12:08:07,384] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "  0%|          | 0/2 [00:00<?, ?it/s][2025-10-12 12:08:08,505] [INFO] [distiller.distillation] - Starting round 0\n",
      "[2025-10-12 12:08:09,738] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:10,525] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:17,415] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:18,350] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:20,577] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:21,512] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:22,530] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:23,418] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:25,860] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:27,099] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:28,261] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:29,138] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:30,740] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:32,588] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:33,541] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:34,414] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:35,071] [INFO] [distiller.distillation] - Best candidate score in round 0: 0.7306964952565531\n",
      " 50%|█████     | 1/2 [00:26<00:26, 26.58s/it][2025-10-12 12:08:35,084] [INFO] [distiller.distillation] - Starting round 1\n",
      "[2025-10-12 12:08:35,758] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:36,552] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:37,718] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:38,690] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:40,331] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:41,467] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:42,277] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:43,006] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:45,406] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:46,293] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:46,950] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:48,002] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:49,226] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:51,114] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:52,525] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:53,760] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:55,507] [INFO] [distiller.distillation] - Best candidate score in round 1: 0.7718590066640569\n",
      "100%|██████████| 2/2 [00:47<00:00, 23.50s/it]\n",
      "[2025-10-12 12:08:55,513] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:08:56,062] [INFO] [distiller.distillation] - Final best prompt score on validation: 0.753456871541753\n",
      "[2025-10-12 12:08:56,064] [INFO] [distiller.distillation] - DistillPrompt optimization completed\n",
      "[2025-10-12 12:08:56,065] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:08:57,080] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:08:57,081] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:08:57,885] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:08:58,639] [INFO] [assistant.run] - Initial meteor score: 0.2138408105858377, final meteor score: 0.7006453459110112\n",
      "[2025-10-12 12:08:58,640] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:09:01,004] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:09:01,005] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by adding clarity and focus on the task by specifying to summarize text by identifying main ideas and key points concisely. This helps the AI understand the exact requirements and deliver a more targeted response. The revised prompt emphasizes the importance of conciseness for clarity and coherence, which guides the AI to provide a more structured and coherent summary. Key advice: Always aim for specificity in your prompts to guide the AI effectively.\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    method='distill',\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed121ec-45cb-4645-b3d5-df859db9f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2197bc",
   "metadata": {},
   "source": [
    "## Framework Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f964fdb",
   "metadata": {},
   "source": [
    "#### Metrics before-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3637e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization: 0.2138408105858377\n",
      "After optimization: 0.7006453459110112\n"
     ]
    }
   ],
   "source": [
    "print('Before optimization:', prompt_tuner.init_metric)\n",
    "print('After optimization:', prompt_tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542aab1d",
   "metadata": {},
   "source": [
    "#### Prompts before-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9572715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization: Summarize the text\n",
      "After optimization: Summarize text by identifying main ideas and key points concisely for clarity and coherence.\n"
     ]
    }
   ],
   "source": [
    "print('Before optimization:', prompt_tuner.init_prompt)\n",
    "print('After optimization:', prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4767aff",
   "metadata": {},
   "source": [
    "#### Generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49d7d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A new study shows that regular exercise can improve mental health and reduce the risk of depression.', 'The latest research indicates that eating a balanced diet can lead to better overall health and well-being.', 'Scientists have discovered a new treatment for a common type of cancer that shows promising results in clinical trials.', 'A recent report highlights the importance of sleep for cognitive function and overall productivity.', 'Experts suggest that practicing mindfulness meditation can reduce stress and improve mental clarity.', 'A study found that spending time in nature can have a positive impact on mental health and well-being.', 'Researchers have developed a new technology that could revolutionize renewable energy production.', 'A report highlights the importance of regular physical activity in maintaining a healthy lifestyle and preventing chronic diseases.', \"Scientists have identified a gene that may play a role in the development of Alzheimer's disease.\", 'Recent studies suggest that social connections and relationships are crucial for overall happiness and well-being.']\n",
      "['Regular exercise can boost mental health and lower depression risk.', 'A balanced diet is essential for good health and well-being.', 'New cancer treatment shows promise in trials.', 'Adequate sleep is crucial for cognitive function and productivity.', 'Mindfulness meditation can lower stress and enhance mental clarity.', 'Nature exposure benefits mental health and well-being.', 'New technology could transform renewable energy production.', 'Regular physical activity is key to a healthy lifestyle and disease prevention.', \"Gene linked to Alzheimer's disease development identified.\", 'Social connections are vital for happiness and well-being.']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_tuner.synthetic_dataset)\n",
    "print(prompt_tuner.synthetic_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3cfe3",
   "metadata": {},
   "source": [
    "#### Optimization feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00e1581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback: Your original prompt was quite vague and lacked specificity. We enhanced it by adding clarity and focus on the task by specifying to summarize text by identifying main ideas and key points concisely. This helps the AI understand the exact requirements and deliver a more targeted response. The revised prompt emphasizes the importance of conciseness for clarity and coherence, which guides the AI to provide a more structured and coherent summary. Key advice: Always aim for specificity in your prompts to guide the AI effectively.\n"
     ]
    }
   ],
   "source": [
    "print('Feedback:', prompt_tuner.assistant_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18405ba",
   "metadata": {},
   "source": [
    "# LLM as judge mertic #\n",
    "\n",
    "\n",
    "There are 4 available templates for llm as judge metric:\n",
    "\n",
    "- Accuracy: The answer is factually correct and contains no mistakes. It uses the right terms and numbers and does not introduce wrong information.\n",
    "\n",
    "- Coherence: The answer is well structured and easy to follow. \n",
    "Ideas are in a logical order and there are no contradictions.\n",
    "\n",
    "- Fluency: The answer is written in natural, correct language. \n",
    "Grammar, vocabulary and phrasing are clear and do not make the text hard to read.\n",
    "\n",
    "- Relevance: The answer directly responds to the request and stays on topic. \n",
    "It does not add unnecessary or unrelated details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ceea6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-19 16:52:25,911] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-11-19 16:52:25,911] [INFO] [assistant.__init__] - Validating the system model\n",
      "[2025-11-19 16:52:25,912] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-19 16:52:54,452] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-19 16:52:54,453] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-19 16:53:04,941] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-19 16:53:04,942] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-19 16:53:04,943] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-19 16:53:04,943] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-19 16:53:04,943] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-19 16:53:04,944] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-19 16:53:04,944] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-19 16:53:10,766] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-19 16:53:10,767] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]You are required to provide a concise summary of the provided text. Begin by reading through the entire passage carefully, and then identify the most important points or ideas within it. After comprehending the main themes and key information, summarize each section concisely without repeating the same information multiple times. Your final output should be a brief yet comprehensive version that captures the essence of the original text while omitting unnecessary details. Please ensure that all relevant content is retained in your summary.[PROMPT_END]\n",
      "[2025-11-19 16:53:10,768] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-19 16:53:25,346] [DEBUG] [assistant.run] - Final prompt:\n",
      "You are required to provide a concise summary of the provided text. Begin by reading through the entire passage carefully, and then identify the most important points or ideas within it. After comprehending the main themes and key information, summarize each section concisely without repeating the same information multiple times. Your final output should be a brief yet comprehensive version that captures the essence of the original text while omitting unnecessary details. Please ensure that all relevant content is retained in your summary.\n",
      "[2025-11-19 16:53:25,347] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-19 16:53:25,348] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-19 16:53:25,349] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-19 16:53:54,922] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-19 16:53:54,923] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "You are required to provide a concise summary of the provided text. Begin by reading through the entire passage carefully, and then identify the most important points or ideas within it. After comprehending the main themes and key information, summarize each section concisely without repeating the same information multiple times. Your final output should be a brief yet comprehensive version that captures the essence of the original text while omitting unnecessary details. Please ensure that all relevant content is retained in your summary.\n",
      "[2025-11-19 16:54:29,172] [INFO] [assistant.run] - Initial llm_as_judge score: 0.63, final llm_as_judge score: 0.7100000000000001\n",
      "[2025-11-19 16:54:29,173] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-19 16:55:00,873] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-19 16:55:00,873] [INFO] [assistant.run] - Your initial prompt was broad and open-ended, allowing for general summaries. By refining it, we've focused on providing a concise yet comprehensive summary of the text. We've added specific guidelines such as reading through carefully, identifying important points, summarizing concisely without repetition, ensuring relevant content is retained, and specifying that all key information should be included in your response. This improvement aligns with best practices for effective prompt engineering.\n",
      "[2025-11-19 16:55:00,874] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-19 16:55:00,876] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-19 16:55:08,980] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-19 16:55:08,980] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-19 16:55:08,981] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-19 16:55:08,982] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-19 16:55:08,982] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-19 16:55:08,983] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-19 16:55:08,983] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-19 16:55:14,982] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-19 16:55:14,983] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[Prompt_START]Summarize this passage while retaining key points and maintaining readability:[PROMPT_END]\n",
      "[2025-11-19 16:55:14,983] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-19 16:55:21,384] [DEBUG] [assistant.run] - Final prompt:\n",
      "[Prompt_START]Summarize this passage while retaining key points and maintaining readability:[PROMPT_END]\n",
      "[2025-11-19 16:55:21,385] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-19 16:55:21,385] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-19 16:55:21,386] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-19 16:55:57,619] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-19 16:55:57,620] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[Prompt_START]Summarize this passage while retaining key points and maintaining readability:[PROMPT_END]\n",
      "[2025-11-19 16:56:38,558] [INFO] [assistant.run] - Initial llm_as_judge score: 0.405, final llm_as_judge score: 0.645\n",
      "[2025-11-19 16:56:38,559] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-19 16:56:53,072] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-19 16:56:53,072] [INFO] [assistant.run] - Your initial prompt was clear but lacked specific context. By adding a structure that includes 'key points' and 'readability,' the prompt is more detailed and guides the response effectively. This ensures that the summary retains essential information while being easy to understand, making it more suitable for readers.\n",
      "[2025-11-19 16:56:53,073] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-19 16:56:53,074] [INFO] [evaluator.__init__] - Evaluator successfully initialized with llm_as_judge metric\n",
      "[2025-11-19 16:56:56,077] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-19 16:56:56,078] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-19 16:56:56,079] [INFO] [assistant.run] - Metric: llm_as_judge, Validation size: 0.25\n",
      "[2025-11-19 16:56:56,079] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-19 16:56:56,080] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-19 16:56:56,080] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-19 16:56:56,081] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "\n",
      "[2025-11-19 16:57:03,091] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-19 16:57:03,091] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]\n",
      "Please provide detailed information on defining your project scope, including:\n",
      "1. What specific challenges and objectives need to be addressed?\n",
      "2. Who are the key stakeholders involved in this project?\n",
      "3. What potential solutions have been considered or researched?\n",
      "4. How do you expect the outcomes of these efforts will impact the overall project goals?\n",
      "5. Are there any particular requirements or guidelines that must be adhered to during development?\n",
      "6. Any specific details about the software, hardware, materials, or processes required for this project?\n",
      "\n",
      "Also, outline your approach and methodology, including:\n",
      "1. What steps are you planning to take in addressing these challenges?\n",
      "2. How do you intend to measure progress towards achieving your objectives?\n",
      "3. Are there any risks you anticipate, and what mitigation strategies will be implemented?\n",
      "\n",
      "Lastly, describe the expected timeline for each phase of your project, ensuring all deadlines are met.\n",
      "PROMPT_END]\n",
      "[2025-11-19 16:57:03,092] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-19 16:57:11,548] [DEBUG] [assistant.run] - Final prompt:\n",
      "[PROMPT_START]\n",
      "Please provide detailed information on defining your project scope, including:\n",
      "1. What specific challenges and objectives need to be addressed?\n",
      "2. Who are the key stakeholders involved in this project?\n",
      "3. What potential solutions have been considered or researched?\n",
      "4. How do you expect the outcomes of these efforts will impact the overall project goals?\n",
      "5. Are there any particular requirements or guidelines that must be adhered to during development?\n",
      "6. Any specific details about the software, hardware, materials, or processes required for this project?\n",
      "\n",
      "Also, outline your approach and methodology, including:\n",
      "1. What steps are you planning to take in addressing these challenges?\n",
      "2. How do you intend to measure progress towards achieving your objectives?\n",
      "3. Are there any risks you anticipate, and what mitigation strategies will be implemented?\n",
      "\n",
      "Lastly, describe the expected timeline for each phase of your project, ensuring all deadlines are met.\n",
      "PROMPT_END]\n",
      "[2025-11-19 16:57:11,549] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-19 16:57:11,549] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-19 16:57:11,550] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "\n",
      "[2025-11-19 16:57:37,250] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-19 16:57:37,251] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "[PROMPT_START]\n",
      "Please provide detailed information on defining your project scope, including:\n",
      "1. What specific challenges and objectives need to be addressed?\n",
      "2. Who are the key stakeholders involved in this project?\n",
      "3. What potential solutions have been considered or researched?\n",
      "4. How do you expect the outcomes of these efforts will impact the overall project goals?\n",
      "5. Are there any particular requirements or guidelines that must be adhered to during development?\n",
      "6. Any specific details about the software, hardware, materials, or processes required for this project?\n",
      "\n",
      "Also, outline your approach and methodology, including:\n",
      "1. What steps are you planning to take in addressing these challenges?\n",
      "2. How do you intend to measure progress towards achieving your objectives?\n",
      "3. Are there any risks you anticipate, and what mitigation strategies will be implemented?\n",
      "\n",
      "Lastly, describe the expected timeline for each phase of your project, ensuring all deadlines are met.\n",
      "PROMPT_END]\n",
      "[2025-11-19 16:58:42,018] [INFO] [assistant.run] - Initial llm_as_judge score: 0.13, final llm_as_judge score: 0.22000000000000003\n",
      "[2025-11-19 16:58:42,019] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-19 16:58:55,672] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-19 16:58:55,672] [INFO] [assistant.run] - Your initial prompt was broad and non-specific. We improved it by focusing on the key elements that define a project scope—challenges, objectives, stakeholders, solutions considered, impact assessment, requirements adherence, methodology outline, and timeline detailing. This makes the final prompt more precise and easier to answer.\n",
      "\n",
      "Key takeaways: To get clear and comprehensive answers, focus on specific aspects like challenges, objectives, stakeholder involvement, potential solutions, expected outcomes, adherence to guidelines, methodology, measurement of progress, identified risks, mitigation strategies, and project timeline.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from coolprompt import PromptTuner\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "target_model = ChatOllama(model=\"qwen2.5:1.5b-instruct\")\n",
    "system_model = ChatOllama(model=\"qwen2.5:3b-instruct\")\n",
    "\n",
    "tuner = PromptTuner(target_model=target_model, system_model=system_model)\n",
    "\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:40]\n",
    "targets = samsum[\"train\"][\"summary\"][:40]\n",
    "\n",
    "\n",
    "result = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"llm_as_judge\",\n",
    "    llm_as_judge_criteria=\"relevance\",\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# You can use several criteria, and the final score will be the mean of them.\n",
    "\n",
    "result2 = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"llm_as_judge\",\n",
    "    llm_as_judge_criteria=[\"relevance\", \"coherence\"],\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# You can use custom criteria in llm as judge\n",
    "\n",
    "custom = {\n",
    "    \"creativity\": \"\"\"You will be given a response to a question.\n",
    "    Rate the creativity on a scale from 1 to {metric_ceil}.\n",
    "    Return ONLY a single number.\n",
    "\n",
    "    Source: {request}\n",
    "    Response: {response}\n",
    "\n",
    "    Creativity score (number only):\"\"\"\n",
    "}\n",
    "\n",
    "result3 = tuner.run(\n",
    "    start_prompt=\"\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"llm_as_judge\",\n",
    "    llm_as_judge_criteria=[\"creativity\"],\n",
    "    llm_as_judge_custom_templates=custom,\n",
    "    generate_num_samples=20,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9469623",
   "metadata": {},
   "source": [
    "# Deepeval GEval metric #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f4dcb",
   "metadata": {},
   "source": [
    "GEval is an LLM-as-a-judge metric that checks whether a generated answer matches the expected output according to natural-language instructions. \n",
    "\n",
    "You can pass either a single `geval_criteria` string (as in the example below) or a list of explicit `evaluation_steps`, and optionally select which fields the judge sees via `evaluation_params` (default INPUT, ACTUAL_OUTPUT, EXPECTED_OUTPUT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7159f",
   "metadata": {},
   "source": [
    "You can use Ollama model or ChatOpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6fdc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 01:01:13,640] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-11-21 01:01:13,641] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-21 01:01:13,641] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 01:01:19,574] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-21 01:01:19,575] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-21 01:01:19,575] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "[2025-11-21 01:01:20,566] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-21 01:01:20,567] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-21 01:01:20,568] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-21 01:01:20,569] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-21 01:01:20,569] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-21 01:01:20,570] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-21 01:01:20,570] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-21 01:01:20,566] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-21 01:01:20,567] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-21 01:01:20,568] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-21 01:01:20,569] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-21 01:01:20,569] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-21 01:01:20,570] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-21 01:01:20,570] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-21 01:01:22,251] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-21 01:01:22,251] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.[PROMPT_END]\n",
      "[2025-11-21 01:01:22,252] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-21 01:01:22,251] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-21 01:01:22,251] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.[PROMPT_END]\n",
      "[2025-11-21 01:01:22,252] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-21 01:01:24,714] [DEBUG] [assistant.run] - Final prompt:\n",
      "Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.\n",
      "[2025-11-21 01:01:24,715] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-21 01:01:24,716] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-21 01:01:24,716] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-21 01:01:24,714] [DEBUG] [assistant.run] - Final prompt:\n",
      "Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.\n",
      "[2025-11-21 01:01:24,715] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-21 01:01:24,716] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-21 01:01:24,716] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-21 01:01:57,789] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-21 01:01:57,791] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.\n",
      "[2025-11-21 01:01:57,789] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-21 01:01:57,791] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asd480/anaconda3/envs/label-studio/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x74c960248b00> is already entered\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asd480/anaconda3/envs/label-studio/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x74c960248b00> is already entered\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asd480/anaconda3/envs/label-studio/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x74c960248b00> is already entered\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asd480/anaconda3/envs/label-studio/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x74c960248b00> is already entered\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-137' coro=<_async_in_context.<locals>.run_in_context() done, defined at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-138' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "/home/asd480/anaconda3/envs/label-studio/lib/python3.12/selectors.py:74: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n",
      "  raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-138' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-139' coro=<_async_in_context.<locals>.run_in_context() done, defined at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-140' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-140' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-137' coro=<_async_in_context.<locals>.run_in_context() done, defined at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-138' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "/home/asd480/anaconda3/envs/label-studio/lib/python3.12/selectors.py:74: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n",
      "  raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-138' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-139' coro=<_async_in_context.<locals>.run_in_context() done, defined at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-140' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-140' coro=<Kernel.shell_main() running at /home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:590> cb=[Task.__wakeup()]>\n",
      "[2025-11-21 01:02:36,094] [INFO] [assistant.run] - Initial geval score: 0.44000000000000006, final geval score: 0.52\n",
      "[2025-11-21 01:02:36,094] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-21 01:02:36,094] [INFO] [assistant.run] - Initial geval score: 0.44000000000000006, final geval score: 0.52\n",
      "[2025-11-21 01:02:36,094] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-21 01:02:45,890] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-21 01:02:45,891] [INFO] [assistant.run] - 你的初始提示是开放的，询问需要一般摘要但未明确指定需要什么文本或是否包括代码片段。我们改进了它，要求提供特定的详细信息，确保总结捕捉到关键信息且简洁。这确保了你得到的是完全符合你需要的回复。\n",
      "[2025-11-21 01:02:45,890] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-21 01:02:45,891] [INFO] [assistant.run] - 你的初始提示是开放的，询问需要一般摘要但未明确指定需要什么文本或是否包括代码片段。我们改进了它，要求提供特定的详细信息，确保总结捕捉到关键信息且简洁。这确保了你得到的是完全符合你需要的回复。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GEval score: 0.44000000000000006\n",
      "Final GEval score: 0.52\n",
      "Final prompt Please provide the text that needs summarization, including any code snippets or specific details you would like included in the summary. I will then generate a concise summary capturing the key points and essential information from the provided content.\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "from langchain_ollama import ChatOllama\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = ChatOllama(model=\"qwen2.5:1.5b-instruct\")\n",
    "\n",
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "start_prompt = \"Summarize the text\"\n",
    "\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:40]\n",
    "targets = samsum[\"train\"][\"summary\"][:40]\n",
    "\n",
    "geval_criteria = (\n",
    "    \"Evaluate whether the assistant's summary is an accurate, concise, and coherent \"\n",
    "    \"paraphrase of the original text. The summary should capture all key facts and \"\n",
    "    \"important events from the expected output, avoid adding new information that is \"\n",
    "    \"not supported by the original text, and omit minor irrelevant details. \"\n",
    ")\n",
    "res = tuner.run(\n",
    "    start_prompt=start_prompt,\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_criteria=geval_criteria,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Initial GEval score:\", tuner.init_metric)\n",
    "print(\"Final GEval score:\", tuner.final_metric)\n",
    "print(\"Final prompt\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa600ac6",
   "metadata": {},
   "source": [
    "Using evaluation steps example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295bd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 19:06:05,048] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-11-21 19:06:05,049] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-11-21 19:06:05,050] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-11-21 19:06:05,050] [INFO] [evaluator.__init__] - Evaluator successfully initialized with geval metric\n",
      "/home/asd480/autoprompt/CoolPrompt/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1963: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-11-21 19:06:07,421] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-11-21 19:06:07,422] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-11-21 19:06:07,422] [INFO] [assistant.run] - Metric: geval, Validation size: 0.25\n",
      "[2025-11-21 19:06:07,423] [INFO] [assistant.run] - Dataset: 40 samples\n",
      "[2025-11-21 19:06:07,423] [INFO] [assistant.run] - Target: 40 samples\n",
      "[2025-11-21 19:06:07,424] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-11-21 19:06:07,424] [DEBUG] [hype.hype_optimizer] - Start prompt:\n",
      "Summarize the text\n",
      "[2025-11-21 19:06:08,848] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-11-21 19:06:08,849] [DEBUG] [hype.hype_optimizer] - Raw HyPE output:\n",
      "[PROMPT_START]\n",
      "Given a text, create a concise summary that captures the main points effectively. Ensure that the summary is brief yet informative, highlighting the key information from the original text.\n",
      "[PROMPT_END]\n",
      "[2025-11-21 19:06:08,850] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-11-21 19:06:10,488] [DEBUG] [assistant.run] - Final prompt:\n",
      "\n",
      "Given a text, create a concise summary that captures the main points effectively. Ensure that the summary is brief yet informative, highlighting the key information from the original text.\n",
      "\n",
      "[2025-11-21 19:06:10,488] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-11-21 19:06:10,489] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-21 19:06:10,489] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "Summarize the text\n",
      "[2025-11-21 19:07:14,052] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 10 samples\n",
      "[2025-11-21 19:07:14,053] [DEBUG] [evaluator.evaluate] - Prompt to evaluate:\n",
      "\n",
      "Given a text, create a concise summary that captures the main points effectively. Ensure that the summary is brief yet informative, highlighting the key information from the original text.\n",
      "\n",
      "[2025-11-21 19:07:35,992] [INFO] [assistant.run] - Initial geval score: 0.24, final geval score: 0.27999999999999997\n",
      "[2025-11-21 19:07:35,992] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-11-21 19:07:38,889] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-11-21 19:07:38,890] [INFO] [assistant.run] - Your original prompt was quite general and lacked specificity. We enhanced it by adding a clear directive to summarize a text, which provides a focused goal for the AI. The revised prompt emphasizes the need for a concise yet informative summary, ensuring that the main points are effectively captured. By requesting the summary to be brief and informative, you guide the AI to extract only the essential information from the text. Key advice: Always aim for clarity, specificity, and a balance between brevity and informativeness in your prompts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GEval score: 0.24\n",
      "Final GEval score: 0.27999999999999997\n",
      "Final prompt: \n",
      "Given a text, create a concise summary that captures the main points effectively. Ensure that the summary is brief yet informative, highlighting the key information from the original text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import load_dataset\n",
    "from coolprompt.assistant import PromptTuner\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "dataset = samsum[\"train\"][\"dialogue\"][:40]\n",
    "targets = samsum[\"train\"][\"summary\"][:40]\n",
    "\n",
    "geval_steps = [\n",
    "    \"1. Compare the assistant's summary with the reference and list the key facts each one mentions.\",\n",
    "    \"2. Check whether the assistant introduces any unsupported information or misses essential events.\",\n",
    "    \"3. Decide if the assistant's summary is concise and coherent while covering all required details.\",\n",
    "]\n",
    "\n",
    "tuner = PromptTuner(target_model=model, system_model=model)\n",
    "\n",
    "final_prompt = tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=dataset,\n",
    "    target=targets,\n",
    "    method=\"hype\",\n",
    "    metric=\"geval\",\n",
    "    geval_evaluation_steps=geval_steps,\n",
    "    verbose=2,\n",
    " )\n",
    "\n",
    "print(\"Initial GEval score:\", tuner.init_metric)\n",
    "print(\"Final GEval score:\", tuner.final_metric)\n",
    "print(\"Final prompt:\", final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
