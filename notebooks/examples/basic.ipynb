{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a12ce0-63ac-48d3-8b6c-307862598f75",
   "metadata": {},
   "source": [
    "# Basic tutorial of CoolPrompt\n",
    "*Actual for 1.1.0+ version*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0929c-beb9-48be-9d39-387801bcf0df",
   "metadata": {},
   "source": [
    "## Quick Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9089fb7f-a327-4c3f-87a5-0cccc5396586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-15 11:10:04,534] [INFO] [llm.init] - Initializing default model: Qwen/Qwen3-4B-Instruct-2507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56ac3bbb18c4a7d895de0bfccf314e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "[2025-10-15 11:10:15,132] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-10-15 11:10:15,133] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n",
      "[2025-10-15 11:10:15,135] [INFO] [detector.generate] - Detecting the task by query\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n",
      "[2025-10-15 11:10:16,263] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-15 11:10:16,264] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-15 11:10:17,316] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-15 11:10:17,316] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "[2025-10-15 11:10:20,101] [INFO] [generator.generate] - Generated problem description: The user needs a concise summary of an essay that discusses the theme, characteristics, and cultural or natural significance of autumn. The task involves extracting the main ideas, key observations, and central arguments from the essay and presenting them in a clear, brief, and coherent manner without adding external information or altering the original content.\n",
      "[2025-10-15 11:11:28,037] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-15 11:11:28,040] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-10-15 11:11:28,040] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-15 11:11:28,041] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-15 11:11:28,041] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-15 11:11:28,042] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-15 11:11:39,740] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-15 11:11:39,742] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-15 11:11:40,611] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-15 11:11:40,612] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-15 11:11:51,835] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "[2025-10-15 11:12:03,418] [INFO] [assistant.run] - Initial meteor score: 0.6168142031214708, final meteor score: 0.6286828396593116\n",
      "[2025-10-15 11:12:03,419] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-15 11:12:20,873] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-15 11:12:20,875] [INFO] [assistant.run] - Your original prompt was too general and lacked specificity. We improved it by adding the role of \"artificial summarizer\", which helps the model focus on the text compression task. Clear steps were introduced for a structured approach: defining the central theme, extracting key features, analyzing cultural aspects, and synthesizing into a single paragraph. This ensures logical and sequential work on the text. A strict output format was also added — only the summarized text without explanations — eliminating unnecessary information and making the result immediately ready for use. The key element is the requirement to maintain neutrality and avoid interpretations, ensuring accuracy and fidelity to the original. Important advice: to get an accurate and structured response, always define the role, break down the task into steps, specify the format, and prohibit external interpretations.\n"
     ]
    }
   ],
   "source": [
    "from coolprompt.assistant import PromptTuner\n",
    "\n",
    "prompt_tuner = PromptTuner()\n",
    "\n",
    "prompt_tuner.run('Summarize an essay about autumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d489c9-65a1-47d1-bb71-d6a03bf614ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a skilled summarizer tasked with distilling the essential ideas of an essay about autumn into a concise, coherent, and accurate summary. The essay explores the theme, characteristics, and cultural or natural significance of autumn. Your goal is to extract the main arguments, key observations, and central themes without introducing external information or altering the original content.\n",
      "\n",
      "Follow these steps:\n",
      "1. Identify the central theme of the essay (e.g., the season's natural changes, emotional tone, cultural representations).\n",
      "2. Extract key characteristics of autumn described in the essay (e.g., falling leaves, harvest, temperature changes, animal behavior).\n",
      "3. Highlight any cultural, historical, or societal references to autumn (e.g., festivals, traditions, literature).\n",
      "4. Synthesize these elements into a single, clear, and brief paragraph that reflects the essay’s core message.\n",
      "5. Ensure the summary is neutral, factual, and directly derived from the text—do not interpret, speculate, or add commentary.\n",
      "\n",
      "Output only the summary. Do not include explanations, meta-comments, or additional context. Maintain the original tone and focus of the essay.\n",
      "\n",
      "Example format:\n",
      "\"Autumn is portrayed as a season of transition and reflection, marked by the changing colors of leaves, the harvest season, and widespread cultural traditions such as Halloween and Thanksgiving. The essay emphasizes its natural cycles and emotional resonance, linking it to themes of impermanence and renewal.\"\n",
      "\n",
      "If the essay content is provided, apply this process to it.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854dd0b-0e49-46f9-b881-367e446afa05",
   "metadata": {},
   "source": [
    "### What happened step-by-step\n",
    "1. We initialized a CoolPrompt Tuner with default large language model: qwen3-4B-instruct\n",
    "2. We input a start prompt that should be modified\n",
    "3. CoolPrompt auto-detected preferred task and metric for prompt optimization and evaluation\n",
    "4. CoolPrompt generated automatically the dataset with target labels and split into train and test samples\n",
    "5. The default prompt optimizer HyPE suggested the optimized prompt\n",
    "6. CoolPrompt outputs the results:\n",
    "   - Metric scores of optimization efficiency\n",
    "   - The final prompt\n",
    "   - The interpretation of prompt optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42992843-51c8-4d39-9b3b-98935d6764e8",
   "metadata": {},
   "source": [
    "### That is a simple approach how to optimize prompts. Let's try to configure CoolPrompt Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe16ef-f038-44f8-a080-a07816042060",
   "metadata": {},
   "source": [
    "## Setup a CoolPrompt Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec42bb-400c-4705-922c-661e67761f36",
   "metadata": {},
   "source": [
    "### 1. LLM Choice\n",
    "The framework is a model-agnostic, you can use proprietary, open-source or custom LLMs with different interfaces via LangChain compatibility. \n",
    "\n",
    "#### Check [the full list of provider interfaces](https://python.langchain.com/docs/integrations/llms/).\n",
    "\n",
    "Default LLM in CoolPrompt is Qwen/Qwen3-4B-Instruct-2507 and it a same can be defined as:\n",
    "\n",
    "```python\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 4000,\n",
    "        \"temperature\": 0.01,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "    }\n",
    ")\n",
    "target_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=target_model)\n",
    "```\n",
    "\n",
    "\n",
    "You can use other popular interfaces as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22194f-1ae6-4ebd-8df5-5adc9419fb8d",
   "metadata": {},
   "source": [
    "#### Ollama\n",
    "For rapid experiments with low resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5481eb-d2f8-46fb-97e9-a857443f5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before the run command serve the model with ollama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "my_model = OllamaLLM(\n",
    "    model=\"qwen2.5-coder:32b\"\n",
    ")\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288eb4dc-5335-4d36-a36d-c5353ad59093",
   "metadata": {},
   "source": [
    "#### VLLM\n",
    "As a production ready solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c87892-bdfc-472c-82b3-d850036187d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "my_model = VLLM(\n",
    "    model=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    trust_remote_code=True,\n",
    "    dtype='bfloat16',\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91b7b6-227f-43b6-b390-16954fc0b729",
   "metadata": {},
   "source": [
    "#### ChatOpenAI\n",
    "For OpenAI and OpenAI compatible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88cbcf-e003-4f0c-ac15-6f52fb5e8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "my_model = ChatOpenAI(\n",
    "    model=\"paste model_name\",\n",
    "    base_url=\"paste base_url\",\n",
    "    openai_api_key=\"paste api_key\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1419b38-296b-4a62-b943-5034bb5c22e6",
   "metadata": {},
   "source": [
    "### 2. System Model\n",
    "Argument `system_model` defines a core in a **PromptAssistant** module.\n",
    "\n",
    "**PromptAssistant** is responsible for:\n",
    "- Definition automatically a task type and a metric for the specific start prompt.\n",
    "- Generates a syntetic dataset and target labels (when the real data is not provided).\n",
    "- Provides an optimization feedback.\n",
    "\n",
    "By default a `system_model` is a defined the same as a `target_model`. It could be set with a different llm in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63f20e3e-45a6-4943-896b-52ca891bcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "target_model = VLLM(\n",
    "    model=\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    ")\n",
    "system_model = VLLM(\n",
    "    model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(\n",
    "    target_model=target_model,\n",
    "    system_model=system_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e308fa-c9fd-46b3-bfd2-8d2db922a86b",
   "metadata": {},
   "source": [
    "#### IMPORTANT NOTE: `system_model` needs to be a confident instructional llm that can generate a structual output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445763b-db76-445f-99b5-48c7fe558455",
   "metadata": {},
   "source": [
    "### 3. Task and metrics\n",
    "CoolPrompt supports 2 task types and a set of metrics: \n",
    "- `classification` - a common classification\n",
    "    - `accuracy`\n",
    "    - `f1` (f1-macro)\n",
    "- `generation` - a general new text generation\n",
    "    - `bleu`\n",
    "    - `rouge`\n",
    "    - `meteor`\n",
    "    - `bertscore`\n",
    "    - `geval` (experimental, one metric: *textual accuracy*)\n",
    "\n",
    "For each run you can define as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efad17-c57a-4fd9-b5c4-bb8911b47898",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Classify a sentence sentiment\",\n",
    "    task=\"classification\",\n",
    "    metric=\"f1\",\n",
    ")\n",
    "\n",
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    metric=\"rouge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c49d5-5dcb-4c3e-93ec-228672efea11",
   "metadata": {},
   "source": [
    "### 4. Setup a dataset configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b86bf",
   "metadata": {},
   "source": [
    "#### 4.1 Input a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e1654ba-7185-40fb-a827-c707ca4cb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:13:30,157] [INFO] [assistant.__init__] - Validating the target model\n",
      "[2025-10-12 12:13:30,158] [INFO] [assistant.__init__] - PromptTuner successfully initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "my_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=\"key\",\n",
    "    temperature=0,\n",
    "    max_tokens=3500,\n",
    ")\n",
    "\n",
    "prompt_tuner = PromptTuner(target_model=my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6df954db-0077-499a-a6c4-4e937aa26de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:12:49,368] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[2025-10-12 12:12:50,423] [INFO] [evaluator.__init__] - Evaluator successfully initialized with f1 metric\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:12:51,334] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:12:51,335] [INFO] [assistant.run] - Method: hype, Task: classification\n",
      "[2025-10-12 12:12:51,336] [INFO] [assistant.run] - Metric: f1, Validation size: 0.25\n",
      "[2025-10-12 12:12:51,337] [INFO] [assistant.run] - Dataset: 30 samples\n",
      "[2025-10-12 12:12:51,338] [INFO] [assistant.run] - Target: 30 samples\n",
      "[2025-10-12 12:12:51,338] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-12 12:12:52,655] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-12 12:12:52,656] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:12:53,423] [INFO] [assistant.run] - Evaluating on given dataset for classification task...\n",
      "[2025-10-12 12:12:53,424] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 8 samples\n",
      "[2025-10-12 12:12:54,042] [INFO] [evaluator.evaluate] - Evaluating prompt for classification task on 8 samples\n",
      "[2025-10-12 12:12:54,825] [INFO] [assistant.run] - Initial f1 score: 0.2727272727272727, final f1 score: 0.75\n",
      "[2025-10-12 12:12:54,826] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:12:57,434] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:12:57,435] [INFO] [assistant.run] - Your original prompt was vague and lacked specificity. We refined it by specifying the task to train a machine learning model for sentiment classification, focusing on positive or negative sentiments. By mentioning the need for a labeled dataset and emphasizing the goal of accurate prediction for new sentences, the prompt becomes more actionable and goal-oriented. This clarity helps the AI understand the task better and generate more relevant responses. Key advice: Always provide clear task instructions, dataset requirements, and the desired outcome to guide the AI effectively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train a machine learning model to classify the sentiment of sentences as either positive or negative. Use a dataset of labeled sentences where each sentence is associated with its sentiment label. Aim to create a classifier that can accurately predict the sentiment of new, unseen sentences based on the patterns and features in the training data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "class_dataset = sst2['train']['sentence'][:30]\n",
    "class_targets = sst2['train']['label'][:30]\n",
    "\n",
    "prompt_tuner.run(\n",
    "    start_prompt=\"Classify sentence sentiment positive or negative\",\n",
    "    task=\"classification\",\n",
    "    dataset=class_dataset,\n",
    "    target=class_targets,\n",
    "    metric=\"f1\"\n",
    ")\n",
    "\n",
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "623382a1-f3e7-4b2f-90aa-e5f15c44b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:11:45,024] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:11:46,060] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:11:47,011] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:11:47,012] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-10-12 12:11:47,013] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:11:47,014] [INFO] [assistant.run] - Dataset: 30 samples\n",
      "[2025-10-12 12:11:47,015] [INFO] [assistant.run] - Target: 30 samples\n",
      "[2025-10-12 12:11:47,015] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-12 12:11:47,624] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-12 12:11:47,624] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:11:48,583] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:11:48,583] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 8 samples\n",
      "[2025-10-12 12:11:49,781] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 8 samples\n",
      "[2025-10-12 12:11:50,728] [INFO] [assistant.run] - Initial meteor score: 0.3321594171032163, final meteor score: 0.3886049104693431\n",
      "[2025-10-12 12:11:50,729] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:11:53,192] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:11:53,192] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by providing a clear directive to summarize a text by extracting crucial details and presenting them concisely. This specificity helps the AI focus on the core task of summarization. Additionally, we included guidance on how to achieve the summary, ensuring a more targeted and effective response. Key advice: Always aim for clarity, specificity, and guidance on the desired output format to optimize the AI's performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given a text, create a concise summary by extracting the most important information and presenting it in a condensed form.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samsum = load_dataset(\"knkarthick/samsum\")\n",
    "gen_dataset = samsum['train']['dialogue'][:30]\n",
    "gen_targets = samsum['train']['summary'][:30]\n",
    "\n",
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    ")\n",
    "\n",
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760366e5-f58d-4f99-8064-41d456489d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afaa6afc",
   "metadata": {},
   "source": [
    "#### 4.2 Define a split strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6dd49",
   "metadata": {},
   "source": [
    "`validation_size` - a ratio for train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fe92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"bertscore\",\n",
    "    validation_size=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df29c42",
   "metadata": {},
   "source": [
    "`train_as_test` - assign a test set as a train, `validation_size` will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692913ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    task=\"generation\",\n",
    "    dataset=gen_dataset,\n",
    "    target=gen_targets,\n",
    "    metric=\"bertscore\",\n",
    "    train_as_test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5f720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3abca5d",
   "metadata": {},
   "source": [
    "### 5. Choosing an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9544c",
   "metadata": {},
   "source": [
    "#### HyPE\n",
    "\n",
    "Provides the prompt refactoring workflow: the initial prompt is injected into special predetermined by our researches query template. \n",
    "\n",
    "This optimizer requires only one query to the LLM, so it surely can be used as a fast and simple tool to make your prompt better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56451584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:01:33,328] [INFO] [detector.generate] - Detecting the task by query\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:01:33,849] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-12 12:01:33,850] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:01:34,882] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-12 12:01:34,883] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:01:35,845] [INFO] [generator.generate] - Generated problem description: The task is to create a system that can automatically generate a concise summary of a given text. This system should be able to analyze the content of the text and extract the most important information to present a brief and informative summary to the user.\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:01:40,477] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:01:40,478] [INFO] [assistant.run] - Method: hype, Task: generation\n",
      "[2025-10-12 12:01:40,479] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:01:40,480] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-12 12:01:40,480] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-12 12:01:40,481] [INFO] [hype.hype_optimizer] - Running HyPE optimization...\n",
      "[2025-10-12 12:01:41,277] [INFO] [hype.hype_optimizer] - HyPE optimization completed\n",
      "[2025-10-12 12:01:41,277] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:01:42,219] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:01:42,220] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:01:45,968] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:01:46,933] [INFO] [assistant.run] - Initial meteor score: 0.2852323515034904, final meteor score: 0.3452339044721156\n",
      "[2025-10-12 12:01:46,934] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:01:48,854] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:01:48,855] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by adding a clear directive to summarize a text and extract the most important information. This helps the AI focus on the key details and deliver a concise yet informative summary. By emphasizing the need for brevity, the final prompt ensures that the output is not overly long. Remember, providing clear instructions on the task and desired outcome is crucial for generating accurate and relevant responses.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given a text, create a concise summary by extracting the most important information. Ensure the summary is brief yet informative for the user.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    method='hype'\n",
    ")\n",
    "\n",
    "print(prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aee2da",
   "metadata": {},
   "source": [
    "#### ReflectivePrompt\n",
    "\n",
    "This method is based on the idea of Reflective Evolution and text-based gradients. It implements short-term and long-term reflections to provide some clarifications and make crossover and mutation operations more precise and effective.\n",
    "\n",
    "ReflectivePrompt has 2 arguments:\n",
    "- population_size (default: 10)\n",
    "- num_epochs (default: 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b8b3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:02:56,733] [INFO] [detector.generate] - Detecting the task by query\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:02:57,194] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-12 12:02:57,194] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:02:58,202] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-12 12:02:58,203] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:02:59,371] [INFO] [generator.generate] - Generated problem description: The task is to create a system that can automatically generate a concise summary of a given text. This system should be able to extract the most important information from the text and present it in a condensed form, making it easier for users to quickly grasp the main points of the original text.\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:03:03,336] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:03:03,337] [INFO] [assistant.run] - Method: reflective, Task: generation\n",
      "[2025-10-12 12:03:03,337] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:03:03,338] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-12 12:03:03,338] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-12 12:03:03,339] [INFO] [run.reflectiveprompt] - Starting ReflectivePrompt optimization...\n",
      "[2025-10-12 12:03:03,339] [INFO] [evoluter._init_pop] - Initializing population...\n",
      "[2025-10-12 12:03:04,408] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:04,409] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:05,124] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:06,537] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:07,382] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:08,355] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:09,291] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:10,319] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:11,168] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:12,157] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:13,414] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:15,917] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:15,918] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:16,655] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:17,592] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:18,382] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:19,526] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:20,201] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:20,839] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:21,543] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:22,352] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:22,892] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:23,572] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.543967441814112\n",
      "[2025-10-12 12:03:25,057] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:25,058] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:25,698] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:26,327] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:26,946] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:27,616] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:28,310] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:28,956] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:29,592] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:30,232] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:31,286] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:32,011] [INFO] [evoluter._update_elitist] - Iteration 0\n",
      "                Elitist score: 0.563435528218003\n",
      "[2025-10-12 12:03:32,012] [INFO] [evoluter._update_iter] - Iteration 0 finished...\n",
      "[2025-10-12 12:03:32,013] [INFO] [evoluter._update_iter] - Best score: 0.563435528218003\n",
      "[2025-10-12 12:03:33,757] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:33,758] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:34,367] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:35,023] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:35,994] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:36,953] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:37,687] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:38,381] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:39,071] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:39,771] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:40,762] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:43,250] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:43,250] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:44,096] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:45,083] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:45,772] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:46,587] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:47,431] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:48,573] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:49,871] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:50,652] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:51,552] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:03:52,627] [INFO] [evoluter._update_elitist] - Iteration 1\n",
      "                Elitist score: 0.563435528218003\n",
      "[2025-10-12 12:03:52,629] [INFO] [evoluter._update_iter] - Iteration 1 finished...\n",
      "[2025-10-12 12:03:52,629] [INFO] [evoluter._update_iter] - Best score: 0.563435528218003\n",
      "[2025-10-12 12:03:52,645] [INFO] [evoluter.evolution] - BEST TRAIN SCORE: 0.563435528218003\n",
      "[2025-10-12 12:03:52,645] [INFO] [evoluter._evaluation] - Evaluating population...\n",
      "[2025-10-12 12:03:52,646] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:53,620] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:54,310] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:55,023] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:55,678] [INFO] [evoluter.evolution] - BEST VALIDATION SCORE: 0.6041594951233505\n",
      "[2025-10-12 12:03:55,679] [INFO] [run.reflectiveprompt] - ReflectivePrompt optimization completed\n",
      "[2025-10-12 12:03:55,680] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:03:56,588] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:03:56,588] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:57,426] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:03:58,036] [INFO] [assistant.run] - Initial meteor score: 0.14212962962962963, final meteor score: 0.4555904715543269\n",
      "[2025-10-12 12:03:58,037] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:04:00,551] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:04:00,551] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by providing a clear objective: developing an automatic text summarization system. This specificity helps guide the AI model towards a focused task. Additionally, we included details about what the summary should contain - main points and key details - which sets clear expectations for the output. By emphasizing the need for a concise and informative summary, the final prompt ensures that the AI-generated summary will be both brief and informative. Key advice: Always define the task clearly, specify the desired content, and emphasize the importance of conciseness in the prompt.\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    method='reflective',\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16a66d",
   "metadata": {},
   "source": [
    "#### DistillPrompt\n",
    "\n",
    "This method is based on different prompt-transformation methods that will use LLM to effectively examine the search area and find the best prompt variations for given task.\n",
    "\n",
    "DistillPrompt has 1 argument:\n",
    "- num_epochs (default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b5f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-12 12:08:01,668] [INFO] [detector.generate] - Detecting the task by query\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:08:02,166] [INFO] [detector.generate] - Task defined as generation\n",
      "[2025-10-12 12:08:02,166] [INFO] [assistant.run] - Validating args for PromptTuner running\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[2025-10-12 12:08:03,729] [INFO] [evaluator.__init__] - Evaluator successfully initialized with meteor metric\n",
      "[2025-10-12 12:08:03,730] [INFO] [generator.generate] - Problem description was not provided, so it will be generated automatically\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:08:04,511] [INFO] [generator.generate] - Generated problem description: The task is to create a system that can automatically generate a concise summary of a given text. This system should be able to analyze the content of the text and extract the most important information to provide a brief and informative summary.\n",
      "/venv/main/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1949: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "[2025-10-12 12:08:07,378] [INFO] [assistant.run] - === Starting Prompt Optimization ===\n",
      "[2025-10-12 12:08:07,379] [INFO] [assistant.run] - Method: distill, Task: generation\n",
      "[2025-10-12 12:08:07,380] [INFO] [assistant.run] - Metric: meteor, Validation size: 0.25\n",
      "[2025-10-12 12:08:07,380] [INFO] [assistant.run] - Dataset: 10 samples\n",
      "[2025-10-12 12:08:07,381] [INFO] [assistant.run] - Target: 10 samples\n",
      "[2025-10-12 12:08:07,383] [INFO] [distiller.distillation] - Starting DistillPrompt optimization...\n",
      "[2025-10-12 12:08:07,384] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "  0%|          | 0/2 [00:00<?, ?it/s][2025-10-12 12:08:08,505] [INFO] [distiller.distillation] - Starting round 0\n",
      "[2025-10-12 12:08:09,738] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:10,525] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:17,415] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:18,350] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:20,577] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:21,512] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:22,530] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:23,418] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:25,860] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:27,099] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:28,261] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:29,138] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:30,740] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:32,588] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:33,541] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:34,414] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:35,071] [INFO] [distiller.distillation] - Best candidate score in round 0: 0.7306964952565531\n",
      " 50%|█████     | 1/2 [00:26<00:26, 26.58s/it][2025-10-12 12:08:35,084] [INFO] [distiller.distillation] - Starting round 1\n",
      "[2025-10-12 12:08:35,758] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:36,552] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:37,718] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:38,690] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:40,331] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:41,467] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:42,277] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:43,006] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:45,406] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:46,293] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:46,950] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:48,002] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:49,226] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:51,114] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:52,525] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:53,760] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 7 samples\n",
      "[2025-10-12 12:08:55,507] [INFO] [distiller.distillation] - Best candidate score in round 1: 0.7718590066640569\n",
      "100%|██████████| 2/2 [00:47<00:00, 23.50s/it]\n",
      "[2025-10-12 12:08:55,513] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:08:56,062] [INFO] [distiller.distillation] - Final best prompt score on validation: 0.753456871541753\n",
      "[2025-10-12 12:08:56,064] [INFO] [distiller.distillation] - DistillPrompt optimization completed\n",
      "[2025-10-12 12:08:56,065] [INFO] [assistant.run] - Running the prompt format checking...\n",
      "[2025-10-12 12:08:57,080] [INFO] [assistant.run] - Evaluating on given dataset for generation task...\n",
      "[2025-10-12 12:08:57,081] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:08:57,885] [INFO] [evaluator.evaluate] - Evaluating prompt for generation task on 3 samples\n",
      "[2025-10-12 12:08:58,639] [INFO] [assistant.run] - Initial meteor score: 0.2138408105858377, final meteor score: 0.7006453459110112\n",
      "[2025-10-12 12:08:58,640] [INFO] [assistant.run] - === Prompt Optimization Completed ===\n",
      "[2025-10-12 12:09:01,004] [INFO] [assistant.run] - === Assistant's feedback ===\n",
      "[2025-10-12 12:09:01,005] [INFO] [assistant.run] - Your original prompt was quite vague and lacked specificity. We enhanced it by adding clarity and focus on the task by specifying to summarize text by identifying main ideas and key points concisely. This helps the AI understand the exact requirements and deliver a more targeted response. The revised prompt emphasizes the importance of conciseness for clarity and coherence, which guides the AI to provide a more structured and coherent summary. Key advice: Always aim for specificity in your prompts to guide the AI effectively.\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner.run(\n",
    "    start_prompt=\"Summarize the text\",\n",
    "    method='distill',\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed121ec-45cb-4645-b3d5-df859db9f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2197bc",
   "metadata": {},
   "source": [
    "## Framework Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f964fdb",
   "metadata": {},
   "source": [
    "#### Metrics before-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3637e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization: 0.2138408105858377\n",
      "After optimization: 0.7006453459110112\n"
     ]
    }
   ],
   "source": [
    "print('Before optimization:', prompt_tuner.init_metric)\n",
    "print('After optimization:', prompt_tuner.final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542aab1d",
   "metadata": {},
   "source": [
    "#### Prompts before-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9572715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization: Summarize the text\n",
      "After optimization: Summarize text by identifying main ideas and key points concisely for clarity and coherence.\n"
     ]
    }
   ],
   "source": [
    "print('Before optimization:', prompt_tuner.init_prompt)\n",
    "print('After optimization:', prompt_tuner.final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4767aff",
   "metadata": {},
   "source": [
    "#### Generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49d7d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A new study shows that regular exercise can improve mental health and reduce the risk of depression.', 'The latest research indicates that eating a balanced diet can lead to better overall health and well-being.', 'Scientists have discovered a new treatment for a common type of cancer that shows promising results in clinical trials.', 'A recent report highlights the importance of sleep for cognitive function and overall productivity.', 'Experts suggest that practicing mindfulness meditation can reduce stress and improve mental clarity.', 'A study found that spending time in nature can have a positive impact on mental health and well-being.', 'Researchers have developed a new technology that could revolutionize renewable energy production.', 'A report highlights the importance of regular physical activity in maintaining a healthy lifestyle and preventing chronic diseases.', \"Scientists have identified a gene that may play a role in the development of Alzheimer's disease.\", 'Recent studies suggest that social connections and relationships are crucial for overall happiness and well-being.']\n",
      "['Regular exercise can boost mental health and lower depression risk.', 'A balanced diet is essential for good health and well-being.', 'New cancer treatment shows promise in trials.', 'Adequate sleep is crucial for cognitive function and productivity.', 'Mindfulness meditation can lower stress and enhance mental clarity.', 'Nature exposure benefits mental health and well-being.', 'New technology could transform renewable energy production.', 'Regular physical activity is key to a healthy lifestyle and disease prevention.', \"Gene linked to Alzheimer's disease development identified.\", 'Social connections are vital for happiness and well-being.']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_tuner.synthetic_dataset)\n",
    "print(prompt_tuner.synthetic_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3cfe3",
   "metadata": {},
   "source": [
    "#### Optimization feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00e1581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback: Your original prompt was quite vague and lacked specificity. We enhanced it by adding clarity and focus on the task by specifying to summarize text by identifying main ideas and key points concisely. This helps the AI understand the exact requirements and deliver a more targeted response. The revised prompt emphasizes the importance of conciseness for clarity and coherence, which guides the AI to provide a more structured and coherent summary. Key advice: Always aim for specificity in your prompts to guide the AI effectively.\n"
     ]
    }
   ],
   "source": [
    "print('Feedback:', prompt_tuner.assistant_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ba635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
