# Distill Prompt

## Описание

Решение состоит из 5 генерации на каждой эпохе:

1. Генерация комбинаций (улучшений) исходного промпа с его скором на тренировочной выборке
2. Дистилляция примеров из трейна в промпт
3. Сжатие (можно считать повторной дистилляцией, чтобы ещё сильнее 'выделить' идею из примеров)
4. Аггрегация
5. Генерация синонимов к агрегации

Затем выбирается лучший кандидат по скору на тренировочной выборке, он становится исходным промптов для следующей эпохи

## Запуск

**Из-за особенностей ядер triton запуск скрипта возможен только на ОС Linux.**

1. Создаем окружение
```shell
bash ./scripts/setup_env.sh
```
2. Скачиваем датасеты
```shell
bash ./scripts/load_data.sh
```
3. Запуск метода на всех датасетах в бенчмарке с моделью T-lite-instruct-0.1, логи по каждому из дататесов будут писаться в .logs/
```python
 PYTHONPATH=$PYTHONPATH:../../../ .venv/bin/python3 main.py --meta-dir logs/ --model AnatoliiPotapov/T-lite-instruct-0.1
```
4. (Опционально) Удалить данные датасетов:
```shell
rm -rf ~/autoprompting_data/
```