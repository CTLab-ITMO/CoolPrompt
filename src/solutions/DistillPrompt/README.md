# Distill Prompt

## Описание

Решение состоит из 5 генерации на каждой эпохе:

1. Генерация комбинаций (улучшений) исходного промпа с его скором на тренировочной выборке
2. Дистилляция примеров из трейна в промпт
3. Сжатие (можно считать повторной дистилляцией, чтобы ещё сильнее 'выделить' идею из примеров)
4. Аггрегация
5. Генерация синонимов к агрегации

Затем выбирается лучший кандидат по скору на тренировочной выборке, он становится исходным промптов для следующей эпохи

## Запуск

**Из-за особенностей ядер triton запуск скрипта возможен только на ОС Linux.**

1. Заходим в директорию CoolPrompt
2. Создаем окружение
```shell
bash ./src/solutions/DistillPrompt/scripts/setup_env.sh
```
3. Скачиваем датасеты
```shell
bash ./src/solutions/DistillPrompt/scripts/load_data.sh
```
4. Запуск метода на всех датасетах в бенчмарке с моделью T-lite-instruct-0.1, логи по каждому из дататесов будут писаться в .logs/
```python
 PYTHONPATH=$PYTHONPATH:. ./.venv/bin/python3 ./src/solutions/DistillPrompt/main.py --meta-dir ./logs/ --model AnatoliiPotapov/T-lite-instruct-0.1
```
5. (Опционально) Удалить данные датасетов:
```shell
rm -rf ~/autoprompting_data/
```